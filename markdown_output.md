# Meta's MILS System: How it Works

Meta AI has introduced a novel system called MILS (Multimodal Iterative LLM Solver) designed to enhance Large Language Models (LLMs) with the ability to process and understand multimedia data, including images, videos, and audio [1, 2, 3].  A key advantage of MILS is that it achieves this multimodal capability without requiring specialized training or modifications to the underlying LLM [1, 2, 3].

The MILS system operates through an iterative optimization cycle, employing two core components: a Generator and a Scorer [1, 2]. The Generator is an LLM that proposes potential solutions for multimodal tasks, such as generating image captions, creating video descriptions, or designing stylized image prompts [2, 3].  The Scorer, typically a pre-trained multimodal model, evaluates these proposed solutions based on their relevance, coherence, and alignment with the input multimedia data [2, 3].  This evaluation provides feedback to the Generator, enabling it to refine its solutions iteratively until a satisfactory output is achieved [1].

MILS is a training-free approach, meaning it leverages the pre-existing capabilities of LLMs and pre-trained multimodal models without requiring additional fine-tuning for specific multimodal tasks [2, 3, 1]. This allows for zero-shot generalization across different modalities, including text, images, videos, and audio [2].  Furthermore, MILS is implemented as a gradient-free optimization method, which means it does not require tuning the parameters of the pre-trained models [2].

The versatility of MILS is evident in its wide range of potential applications [2, 1]. For instance, in image captioning, MILS can utilize Llama 3.1 8B as the GENERATOR and CLIP-based models as the SCORER to iteratively refine captions, producing highly accurate and descriptive results [2]. MILS can also be applied to various other multimodal tasks such as video description, stylized image generation, and potentially audio processing [2].  It is also task-agnostic, adaptable to diverse multimodal tasks without needing task-specific training [3].  The system's ability to convert different data types into readable text also allows for the integration of information from multiple sources, enhancing its multimodal understanding capabilities [1].

# References

[1] [the-decoder.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUBnsYuSKLpksFbXJWX9AqEhCEZDYokvxC84aSxW2JFM385METqZ5Cs1Boa1TUXOEhrbuHMmRRBcMG_SkpeqD05bSJEiOaDgQrzEAOmHx_LMvFd_bW3qBfLx5s_IGDpjOLyMfU81FnI_SNlWwkm50rbKWLWN-7zToc3SqrohNFKUsCv8A0K8mpZHKGh4aO090vjSZI6VyIyA4n5aiZut-kwc5TUPgqmyd4_pien9AQAOhg==)  
[2] [marktechpost.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUBnsYtYOg8fJ85uMEb4-1kSnHav-WKvxTTyxtLSu3w-AhNepJsZf4zTqcXiUbwJFBD1JnVgflFxKEmSEhzUmrEqGBqqg-w-THxhqfRLaRpzKIOJ1T3fBN0bMd-AKp1U9xpHPf4hgS60iF3fFForbdcWGDGMJSnZ9xpmnPXO40AIP-hT-sgz4yUjDEGMtusWGjVYK6YMEflhDZ7BxK0KITUpT0Q9Nvl6QYp_WJwrC-Z0rdRDW_cTRG6rQHSYb3eCe02zLQkPRB3iK3AxwtKHCYBu4u56DU1T3zyW)  
[3] [medium.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUBnsYsiJeKjBnPoVZ_rDQicNnqb1Qmv4AP0kgncl1_jGGA8kT7JKtkl52pZ6-BHHtyBIAyouQE8LzjgmidKPKfOUQQPnZgV755I6GkrQiL7OcXPK2L-h1UNpksOxnlFxhrtMG102ls_EvRlJwbfRwOiNteYspxDv7PmbMKft_Z5ESJ55Bm11fM2SbeEfrXgT7hXLvywWDXZA-8m2CbLX9P7aS5hePiCadTXw0OAwqkwnsFajZ-etx_1Fe2fJH6ctYwkdBKE8iOTk9pu27K6uTeNPEY=)  