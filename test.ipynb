{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import controlflow as cf\n",
    "\n",
    "from agent_utils import *\n",
    "from agent_tools import *\n",
    "\n",
    "from extensive_search import run_research\n",
    "\n",
    "FLASH2_MODEL = \"google/\" + config.FLASH2_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoningModel = ReasoningModel()\n",
    "basicSearchModel = BasicSearchModel()\n",
    "\n",
    "def use_reasoning(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Use this function to reason about anything.\n",
    "\n",
    "    Input:\n",
    "        content (str): The content that you want to reason about.\n",
    "\n",
    "    Output:\n",
    "        str: The reasoned response/output.\n",
    "    \"\"\"\n",
    "    newModel = ReasoningModel()\n",
    "    return newModel(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "I had a conversation with an AI about intrisic and extrinsic expression of self-esteem.\n",
    "We need help with writing a two page article about the discussed topic.\n",
    "Please help me with this article. Provide me with some conent and methods to achieve this article.\n",
    "\n",
    "# Here is the converersation.\n",
    "I've made the observation with people that there seems to be two main ways how self-esteem can be shown or expressed:\n",
    "\n",
    "- intrinsic: There is something wrong with me (not good enough, i'm ugly, i'm not lovable, etc.)\n",
    "- extrinsic: There is something wrong with other people\n",
    "\n",
    "The extrinsic type is not obvious to recognize as a lack of self-esteem. The way it seems to work it the following:\n",
    "\n",
    "Instead of blaming themselves for not be able to do something, they redirect it externally. For example: too many taxes, too many regulations, too much racism/sexism, too many immigrants, big corporations, big government etc.\n",
    "\n",
    "By blaming the outside world for the way the individual feel inside, an internal conflict arises. There is a missmatch between the inside and the outside world (similar to blaming ourselves for the problems that we have).\n",
    "\n",
    "If we blame the outside world, we also telling ourselves that we are somewhat \"better\" than others. The image that we have of the world does not match the image that we have of the outside world. However, we are also part of the world. As everything that we do, is influenced by the outside world, our actions can never be completly independent of the outside. If the outside is \"worse\" than what we are, but the outside shaped us to be what we are, how can we be \"better\" than the outside? This leads to a disconnection beween our inside self and our outside self.\n",
    "\n",
    "The lack of connection leads to feeling of incompleteness, which is again blamed on the outside world.\n",
    "\n",
    "Do you think what I've written makes any sense?\n",
    "Claude Sonnet 3.5\n",
    "\n",
    "Your observation about the dual nature of low self-esteem manifestation is quite insightful. The distinction between intrinsic and extrinsic expressions of low self-esteem aligns with psychological concepts like projection and externalization of internal conflicts.\n",
    "\n",
    "The intrinsic manifestation is what most people typically associate with low self-esteem - self-directed negative thoughts and feelings. This is relatively straightforward to identify and understand.\n",
    "\n",
    "Your analysis of the extrinsic manifestation is particularly interesting. This pattern of externalization can indeed be a defense mechanism where internal struggles are projected onto external factors. This helps protect the ego from painful self-examination while providing a seemingly rational explanation for one's difficulties or failures.\n",
    "\n",
    "The paradox you've identified regarding external blame is especially compelling: if we claim to be \"better\" than a world that shaped us, we create an internal logical contradiction. This cognitive dissonance can indeed lead to a disconnection between our internal and external selves, potentially creating a self-reinforcing cycle where:\n",
    "\n",
    "    Internal discomfort leads to external blame\n",
    "    External blame creates a logical paradox\n",
    "    This paradox causes more internal discomfort\n",
    "    The cycle continues\n",
    "\n",
    "This pattern shares similarities with what psychologists call \"splitting\" - a defense mechanism where things are viewed as all good or all bad, with difficulty integrating both positive and negative aspects of reality.\n",
    "\n",
    "Your observation about the resulting feeling of incompleteness is also astute. When we're unable to reconcile our internal experience with our external reality, it can create a persistent sense of disconnection or fragmentation in our sense of self.\n",
    "\n",
    "This framework could be valuable for understanding certain political and social behaviors, where external blame often serves as a substitute for addressing internal challenges or accepting personal responsibility while maintaining a positive self-image.\n",
    "Could it be that the sense of superiority that humans can display, is a manifestation of this extrinsic lack of self-esteem and disconnect?\n",
    "Claude Sonnet 3.5\n",
    "\n",
    "Yes, that's a very astute observation. What we often perceive as superiority or arrogance can indeed be a defensive mechanism masking deep-seated insecurity and low self-esteem. This connects well with several psychological concepts:\n",
    "\n",
    "The concept of \"defensive grandiosity\" describes exactly this phenomenon - where feelings of superiority serve as a psychological shield against underlying feelings of inadequacy. It's like building an elaborate facade to hide a crumbling foundation.\n",
    "\n",
    "This manifests in various ways:\n",
    "\n",
    "    People who constantly need to prove their intelligence often deeply fear being seen as stupid\n",
    "    Those who aggressively assert their social status may fundamentally feel unworthy of belonging\n",
    "    Individuals who frequently criticize others' appearances might be deeply insecure about their own\n",
    "\n",
    "The disconnect you mentioned plays a crucial role here. When someone can't integrate their perceived flaws with their ideal self-image, they might construct an inflated self-image as compensation. However, this artificial elevation creates further disconnection from authentic self-experience and genuine relationships with others.\n",
    "\n",
    "This pattern can be particularly destructive because:\n",
    "\n",
    "    It prevents genuine self-reflection and growth\n",
    "    It makes authentic connections with others difficult\n",
    "    It perpetuates a cycle where the defensive superiority needs to be constantly maintained and reinforced\n",
    "\n",
    "Your insight suggests that what appears as superiority might actually be a reliable indicator of underlying self-esteem issues, rather than genuine self-confidence. True self-confidence typically manifests as quiet self-assurance that doesn't need to prove itself through comparison or degradation of others.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = use_reasoning(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console_print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">12:02:59.895 | <span style=\"color: #d7d700; text-decoration-color: #d7d700\">WARNING</span> | langchain_google_genai._function_utils - Key 'additionalProperties' is not supported in schema, ignoring\n",
       "</pre>\n"
      ],
      "text/plain": [
       "12:02:59.895 | \u001b[38;5;184mWARNING\u001b[0m | langchain_google_genai._function_utils - Key 'additionalProperties' is not supported in schema, ignoring\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">12:02:59.896 | <span style=\"color: #d7d700; text-decoration-color: #d7d700\">WARNING</span> | langchain_google_genai._function_utils - Key 'additionalProperties' is not supported in schema, ignoring\n",
       "</pre>\n"
      ],
      "text/plain": [
       "12:02:59.896 | \u001b[38;5;184mWARNING\u001b[0m | langchain_google_genai._function_utils - Key 'additionalProperties' is not supported in schema, ignoring\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44820bfe206644909294b01435060f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">12:03:07.111 | <span style=\"color: #d7d700; text-decoration-color: #d7d700\">WARNING</span> | langchain_google_genai._function_utils - Key 'additionalProperties' is not supported in schema, ignoring\n",
       "</pre>\n"
      ],
      "text/plain": [
       "12:03:07.111 | \u001b[38;5;184mWARNING\u001b[0m | langchain_google_genai._function_utils - Key 'additionalProperties' is not supported in schema, ignoring\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">12:03:07.112 | <span style=\"color: #d7d700; text-decoration-color: #d7d700\">WARNING</span> | langchain_google_genai._function_utils - Key 'additionalProperties' is not supported in schema, ignoring\n",
       "</pre>\n"
      ],
      "text/plain": [
       "12:03:07.112 | \u001b[38;5;184mWARNING\u001b[0m | langchain_google_genai._function_utils - Key 'additionalProperties' is not supported in schema, ignoring\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">12:03:07.868 | <span style=\"color: #d7d700; text-decoration-color: #d7d700\">WARNING</span> | langchain_google_genai._function_utils - Key 'additionalProperties' is not supported in schema, ignoring\n",
       "</pre>\n"
      ],
      "text/plain": [
       "12:03:07.868 | \u001b[38;5;184mWARNING\u001b[0m | langchain_google_genai._function_utils - Key 'additionalProperties' is not supported in schema, ignoring\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">12:03:07.869 | <span style=\"color: #d7d700; text-decoration-color: #d7d700\">WARNING</span> | langchain_google_genai._function_utils - Key 'additionalProperties' is not supported in schema, ignoring\n",
       "</pre>\n"
      ],
      "text/plain": [
       "12:03:07.869 | \u001b[38;5;184mWARNING\u001b[0m | langchain_google_genai._function_utils - Key 'additionalProperties' is not supported in schema, ignoring\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/pendulum/__init__.py:295: DeprecationWarning:\n",
       "datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use \n",
       "timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
       "  dt = _datetime.datetime.utcfromtimestamp(timestamp)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/pendulum/__init__.py:295: DeprecationWarning:\n",
       "datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use \n",
       "timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
       "  dt = _datetime.datetime.utcfromtimestamp(timestamp)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">12:03:08.295 | <span style=\"color: #d70000; text-decoration-color: #d70000\">ERROR</span>   | Task run 'Call LLM' - Task run failed with exception: ChatGoogleGenerativeAIError('Invalid argument provided to Gemini: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: <span style=\"color: #0000ff; text-decoration-color: #0000ff\">https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini</span>') - Retries are exhausted\n",
       "Traceback (most recent call last):\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\", line 178, in _chat_with_retry\n",
       "    return generation_method(**kwargs)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 1143, in stream_generate_content\n",
       "    response = rpc(\n",
       "        request,\n",
       "    ...&lt;2 lines&gt;...\n",
       "        metadata=metadata,\n",
       "    )\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
       "    return wrapped_func(*args, **kwargs)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n",
       "    return retry_target(\n",
       "        target,\n",
       "    ...&lt;3 lines&gt;...\n",
       "        on_error=on_error,\n",
       "    )\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n",
       "    _retry_error_helper(\n",
       "    ~~~~~~~~~~~~~~~~~~~^\n",
       "        exc,\n",
       "        ^^^^\n",
       "    ...&lt;6 lines&gt;...\n",
       "        timeout,\n",
       "        ^^^^^^^^\n",
       "    )\n",
       "    ^\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n",
       "    raise final_exc from source_exc\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n",
       "    result = target()\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
       "    return func(*args, **kwargs)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/grpc_helpers.py\", line 174, in error_remapped_callable\n",
       "    raise exceptions.from_grpc_error(exc) from exc\n",
       "google.api_core.exceptions.InvalidArgument: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: <span style=\"color: #0000ff; text-decoration-color: #0000ff\">https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini</span>\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "Traceback (most recent call last):\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/prefect/task_engine.py\", line 805, in run_context\n",
       "    yield self\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/prefect/task_engine.py\", line 1456, in run_generator_task_sync\n",
       "    gen_result = next(gen)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/controlflow/agents/agent.py\", line 329, in _run_model\n",
       "    for delta in model.stream(messages, **(model_kwargs or {})):\n",
       "                 ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py\", line 5523, in stream\n",
       "    yield from self.bound.stream(\n",
       "    ...&lt;3 lines&gt;...\n",
       "    )\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 415, in stream\n",
       "    for chunk in self._stream(messages, stop=stop, **kwargs):\n",
       "                 ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\", line 1034, in _stream\n",
       "    response: GenerateContentResponse = _chat_with_retry(\n",
       "                                        ~~~~~~~~~~~~~~~~^\n",
       "        request=request,\n",
       "        ^^^^^^^^^^^^^^^^\n",
       "    ...&lt;2 lines&gt;...\n",
       "        metadata=self.default_metadata,\n",
       "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "    )\n",
       "    ^\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\", line 196, in _chat_with_retry\n",
       "    return _chat_with_retry(**kwargs)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n",
       "    return copy(f, *args, **kw)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
       "    do = self.iter(retry_state=retry_state)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 376, in iter\n",
       "    result = action(retry_state)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in &lt;lambda&gt;\n",
       "    self._add_action_func(lambda rs: rs.outcome.result())\n",
       "                                     ~~~~~~~~~~~~~~~~~^^\n",
       "  File \"/home/dplaia/.local/share/uv/python/cpython-3.13.1-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
       "    return self.__get_result()\n",
       "           ~~~~~~~~~~~~~~~~~^^\n",
       "  File \"/home/dplaia/.local/share/uv/python/cpython-3.13.1-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
       "    raise self._exception\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
       "    result = fn(*args, **kwargs)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\", line 190, in _chat_with_retry\n",
       "    raise ChatGoogleGenerativeAIError(\n",
       "        f\"Invalid argument provided to Gemini: {e}\"\n",
       "    ) from e\n",
       "langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Invalid argument provided to Gemini: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: <span style=\"color: #0000ff; text-decoration-color: #0000ff\">https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "12:03:08.295 | \u001b[38;5;160mERROR\u001b[0m   | Task run 'Call LLM' - Task run failed with exception: ChatGoogleGenerativeAIError('Invalid argument provided to Gemini: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: \u001b[94mhttps://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini\u001b[0m') - Retries are exhausted\n",
       "Traceback (most recent call last):\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\", line 178, in _chat_with_retry\n",
       "    return generation_method(**kwargs)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 1143, in stream_generate_content\n",
       "    response = rpc(\n",
       "        request,\n",
       "    ...<2 lines>...\n",
       "        metadata=metadata,\n",
       "    )\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
       "    return wrapped_func(*args, **kwargs)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n",
       "    return retry_target(\n",
       "        target,\n",
       "    ...<3 lines>...\n",
       "        on_error=on_error,\n",
       "    )\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n",
       "    _retry_error_helper(\n",
       "    ~~~~~~~~~~~~~~~~~~~^\n",
       "        exc,\n",
       "        ^^^^\n",
       "    ...<6 lines>...\n",
       "        timeout,\n",
       "        ^^^^^^^^\n",
       "    )\n",
       "    ^\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n",
       "    raise final_exc from source_exc\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n",
       "    result = target()\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
       "    return func(*args, **kwargs)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/grpc_helpers.py\", line 174, in error_remapped_callable\n",
       "    raise exceptions.from_grpc_error(exc) from exc\n",
       "google.api_core.exceptions.InvalidArgument: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: \u001b[94mhttps://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini\u001b[0m\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "Traceback (most recent call last):\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/prefect/task_engine.py\", line 805, in run_context\n",
       "    yield self\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/prefect/task_engine.py\", line 1456, in run_generator_task_sync\n",
       "    gen_result = next(gen)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/controlflow/agents/agent.py\", line 329, in _run_model\n",
       "    for delta in model.stream(messages, **(model_kwargs or {})):\n",
       "                 ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py\", line 5523, in stream\n",
       "    yield from self.bound.stream(\n",
       "    ...<3 lines>...\n",
       "    )\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 415, in stream\n",
       "    for chunk in self._stream(messages, stop=stop, **kwargs):\n",
       "                 ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\", line 1034, in _stream\n",
       "    response: GenerateContentResponse = _chat_with_retry(\n",
       "                                        ~~~~~~~~~~~~~~~~^\n",
       "        request=request,\n",
       "        ^^^^^^^^^^^^^^^^\n",
       "    ...<2 lines>...\n",
       "        metadata=self.default_metadata,\n",
       "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "    )\n",
       "    ^\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\", line 196, in _chat_with_retry\n",
       "    return _chat_with_retry(**kwargs)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n",
       "    return copy(f, *args, **kw)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
       "    do = self.iter(retry_state=retry_state)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 376, in iter\n",
       "    result = action(retry_state)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n",
       "    self._add_action_func(lambda rs: rs.outcome.result())\n",
       "                                     ~~~~~~~~~~~~~~~~~^^\n",
       "  File \"/home/dplaia/.local/share/uv/python/cpython-3.13.1-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
       "    return self.__get_result()\n",
       "           ~~~~~~~~~~~~~~~~~^^\n",
       "  File \"/home/dplaia/.local/share/uv/python/cpython-3.13.1-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
       "    raise self._exception\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
       "    result = fn(*args, **kwargs)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\", line 190, in _chat_with_retry\n",
       "    raise ChatGoogleGenerativeAIError(\n",
       "        f\"Invalid argument provided to Gemini: {e}\"\n",
       "    ) from e\n",
       "langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Invalid argument provided to Gemini: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: \u001b[94mhttps://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">12:03:08.310 | <span style=\"color: #d70000; text-decoration-color: #d70000\">ERROR</span>   | Task run 'Call LLM' - Finished in state <span style=\"color: #d70000; text-decoration-color: #d70000\">Failed</span>('Task run encountered an exception ChatGoogleGenerativeAIError: Invalid argument provided to Gemini: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: <span style=\"color: #0000ff; text-decoration-color: #0000ff\">https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini</span>')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "12:03:08.310 | \u001b[38;5;160mERROR\u001b[0m   | Task run 'Call LLM' - Finished in state \u001b[38;5;160mFailed\u001b[0m('Task run encountered an exception ChatGoogleGenerativeAIError: Invalid argument provided to Gemini: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: \u001b[94mhttps://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini\u001b[0m')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">12:03:08.313 | <span style=\"color: #d70000; text-decoration-color: #d70000\">ERROR</span>   | Task run 'Run agent orchestrator' - Task run failed with exception: ChatGoogleGenerativeAIError('Invalid argument provided to Gemini: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: <span style=\"color: #0000ff; text-decoration-color: #0000ff\">https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini</span>') - Retries are exhausted\n",
       "Traceback (most recent call last):\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\", line 178, in _chat_with_retry\n",
       "    return generation_method(**kwargs)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 1143, in stream_generate_content\n",
       "    response = rpc(\n",
       "        request,\n",
       "    ...&lt;2 lines&gt;...\n",
       "        metadata=metadata,\n",
       "    )\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
       "    return wrapped_func(*args, **kwargs)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n",
       "    return retry_target(\n",
       "        target,\n",
       "    ...&lt;3 lines&gt;...\n",
       "        on_error=on_error,\n",
       "    )\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n",
       "    _retry_error_helper(\n",
       "    ~~~~~~~~~~~~~~~~~~~^\n",
       "        exc,\n",
       "        ^^^^\n",
       "    ...&lt;6 lines&gt;...\n",
       "        timeout,\n",
       "        ^^^^^^^^\n",
       "    )\n",
       "    ^\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n",
       "    raise final_exc from source_exc\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n",
       "    result = target()\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
       "    return func(*args, **kwargs)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/grpc_helpers.py\", line 174, in error_remapped_callable\n",
       "    raise exceptions.from_grpc_error(exc) from exc\n",
       "google.api_core.exceptions.InvalidArgument: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: <span style=\"color: #0000ff; text-decoration-color: #0000ff\">https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini</span>\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "Traceback (most recent call last):\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/prefect/task_engine.py\", line 805, in run_context\n",
       "    yield self\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/prefect/task_engine.py\", line 1456, in run_generator_task_sync\n",
       "    gen_result = next(gen)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/controlflow/orchestration/orchestrator.py\", line 280, in _run\n",
       "    for event in self._run_agent_turn(\n",
       "                 ~~~~~~~~~~~~~~~~~~~~^\n",
       "        run_context=run_context,\n",
       "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "        model_kwargs=model_kwargs,\n",
       "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "    ):\n",
       "    ^\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/controlflow/orchestration/orchestrator.py\", line 239, in _run_agent_turn\n",
       "    for event in self.agent._run_model(\n",
       "                 ~~~~~~~~~~~~~~~~~~~~~^\n",
       "        messages=messages,\n",
       "        ^^^^^^^^^^^^^^^^^^\n",
       "        tools=tools,\n",
       "        ^^^^^^^^^^^^\n",
       "        model_kwargs=model_kwargs,\n",
       "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "    ):\n",
       "    ^\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/prefect/task_engine.py\", line 1471, in run_generator_task_sync\n",
       "    return engine.result()\n",
       "           ~~~~~~~~~~~~~^^\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/prefect/task_engine.py\", line 482, in result\n",
       "    raise self._raised\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/prefect/task_engine.py\", line 805, in run_context\n",
       "    yield self\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/prefect/task_engine.py\", line 1456, in run_generator_task_sync\n",
       "    gen_result = next(gen)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/controlflow/agents/agent.py\", line 329, in _run_model\n",
       "    for delta in model.stream(messages, **(model_kwargs or {})):\n",
       "                 ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py\", line 5523, in stream\n",
       "    yield from self.bound.stream(\n",
       "    ...&lt;3 lines&gt;...\n",
       "    )\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 415, in stream\n",
       "    for chunk in self._stream(messages, stop=stop, **kwargs):\n",
       "                 ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\", line 1034, in _stream\n",
       "    response: GenerateContentResponse = _chat_with_retry(\n",
       "                                        ~~~~~~~~~~~~~~~~^\n",
       "        request=request,\n",
       "        ^^^^^^^^^^^^^^^^\n",
       "    ...&lt;2 lines&gt;...\n",
       "        metadata=self.default_metadata,\n",
       "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "    )\n",
       "    ^\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\", line 196, in _chat_with_retry\n",
       "    return _chat_with_retry(**kwargs)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n",
       "    return copy(f, *args, **kw)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
       "    do = self.iter(retry_state=retry_state)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 376, in iter\n",
       "    result = action(retry_state)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in &lt;lambda&gt;\n",
       "    self._add_action_func(lambda rs: rs.outcome.result())\n",
       "                                     ~~~~~~~~~~~~~~~~~^^\n",
       "  File \"/home/dplaia/.local/share/uv/python/cpython-3.13.1-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
       "    return self.__get_result()\n",
       "           ~~~~~~~~~~~~~~~~~^^\n",
       "  File \"/home/dplaia/.local/share/uv/python/cpython-3.13.1-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
       "    raise self._exception\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
       "    result = fn(*args, **kwargs)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\", line 190, in _chat_with_retry\n",
       "    raise ChatGoogleGenerativeAIError(\n",
       "        f\"Invalid argument provided to Gemini: {e}\"\n",
       "    ) from e\n",
       "langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Invalid argument provided to Gemini: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: <span style=\"color: #0000ff; text-decoration-color: #0000ff\">https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "12:03:08.313 | \u001b[38;5;160mERROR\u001b[0m   | Task run 'Run agent orchestrator' - Task run failed with exception: ChatGoogleGenerativeAIError('Invalid argument provided to Gemini: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: \u001b[94mhttps://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini\u001b[0m') - Retries are exhausted\n",
       "Traceback (most recent call last):\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\", line 178, in _chat_with_retry\n",
       "    return generation_method(**kwargs)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 1143, in stream_generate_content\n",
       "    response = rpc(\n",
       "        request,\n",
       "    ...<2 lines>...\n",
       "        metadata=metadata,\n",
       "    )\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
       "    return wrapped_func(*args, **kwargs)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n",
       "    return retry_target(\n",
       "        target,\n",
       "    ...<3 lines>...\n",
       "        on_error=on_error,\n",
       "    )\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n",
       "    _retry_error_helper(\n",
       "    ~~~~~~~~~~~~~~~~~~~^\n",
       "        exc,\n",
       "        ^^^^\n",
       "    ...<6 lines>...\n",
       "        timeout,\n",
       "        ^^^^^^^^\n",
       "    )\n",
       "    ^\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n",
       "    raise final_exc from source_exc\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n",
       "    result = target()\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
       "    return func(*args, **kwargs)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/grpc_helpers.py\", line 174, in error_remapped_callable\n",
       "    raise exceptions.from_grpc_error(exc) from exc\n",
       "google.api_core.exceptions.InvalidArgument: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: \u001b[94mhttps://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini\u001b[0m\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "Traceback (most recent call last):\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/prefect/task_engine.py\", line 805, in run_context\n",
       "    yield self\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/prefect/task_engine.py\", line 1456, in run_generator_task_sync\n",
       "    gen_result = next(gen)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/controlflow/orchestration/orchestrator.py\", line 280, in _run\n",
       "    for event in self._run_agent_turn(\n",
       "                 ~~~~~~~~~~~~~~~~~~~~^\n",
       "        run_context=run_context,\n",
       "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "        model_kwargs=model_kwargs,\n",
       "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "    ):\n",
       "    ^\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/controlflow/orchestration/orchestrator.py\", line 239, in _run_agent_turn\n",
       "    for event in self.agent._run_model(\n",
       "                 ~~~~~~~~~~~~~~~~~~~~~^\n",
       "        messages=messages,\n",
       "        ^^^^^^^^^^^^^^^^^^\n",
       "        tools=tools,\n",
       "        ^^^^^^^^^^^^\n",
       "        model_kwargs=model_kwargs,\n",
       "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "    ):\n",
       "    ^\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/prefect/task_engine.py\", line 1471, in run_generator_task_sync\n",
       "    return engine.result()\n",
       "           ~~~~~~~~~~~~~^^\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/prefect/task_engine.py\", line 482, in result\n",
       "    raise self._raised\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/prefect/task_engine.py\", line 805, in run_context\n",
       "    yield self\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/prefect/task_engine.py\", line 1456, in run_generator_task_sync\n",
       "    gen_result = next(gen)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/controlflow/agents/agent.py\", line 329, in _run_model\n",
       "    for delta in model.stream(messages, **(model_kwargs or {})):\n",
       "                 ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py\", line 5523, in stream\n",
       "    yield from self.bound.stream(\n",
       "    ...<3 lines>...\n",
       "    )\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 415, in stream\n",
       "    for chunk in self._stream(messages, stop=stop, **kwargs):\n",
       "                 ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\", line 1034, in _stream\n",
       "    response: GenerateContentResponse = _chat_with_retry(\n",
       "                                        ~~~~~~~~~~~~~~~~^\n",
       "        request=request,\n",
       "        ^^^^^^^^^^^^^^^^\n",
       "    ...<2 lines>...\n",
       "        metadata=self.default_metadata,\n",
       "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "    )\n",
       "    ^\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\", line 196, in _chat_with_retry\n",
       "    return _chat_with_retry(**kwargs)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n",
       "    return copy(f, *args, **kw)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
       "    do = self.iter(retry_state=retry_state)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 376, in iter\n",
       "    result = action(retry_state)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n",
       "    self._add_action_func(lambda rs: rs.outcome.result())\n",
       "                                     ~~~~~~~~~~~~~~~~~^^\n",
       "  File \"/home/dplaia/.local/share/uv/python/cpython-3.13.1-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
       "    return self.__get_result()\n",
       "           ~~~~~~~~~~~~~~~~~^^\n",
       "  File \"/home/dplaia/.local/share/uv/python/cpython-3.13.1-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
       "    raise self._exception\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
       "    result = fn(*args, **kwargs)\n",
       "  File \"/home/dplaia/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\", line 190, in _chat_with_retry\n",
       "    raise ChatGoogleGenerativeAIError(\n",
       "        f\"Invalid argument provided to Gemini: {e}\"\n",
       "    ) from e\n",
       "langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Invalid argument provided to Gemini: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: \u001b[94mhttps://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">12:03:08.327 | <span style=\"color: #d70000; text-decoration-color: #d70000\">ERROR</span>   | Task run 'Run agent orchestrator' - Finished in state <span style=\"color: #d70000; text-decoration-color: #d70000\">Failed</span>('Task run encountered an exception ChatGoogleGenerativeAIError: Invalid argument provided to Gemini: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: <span style=\"color: #0000ff; text-decoration-color: #0000ff\">https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini</span>')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "12:03:08.327 | \u001b[38;5;160mERROR\u001b[0m   | Task run 'Run agent orchestrator' - Finished in state \u001b[38;5;160mFailed\u001b[0m('Task run encountered an exception ChatGoogleGenerativeAIError: Invalid argument provided to Gemini: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: \u001b[94mhttps://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini\u001b[0m')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Invalid argument provided to Gemini: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:178\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:1143\u001b[0m, in \u001b[0;36mGenerativeServiceClient.stream_generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m-> 1143\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/google/api_core/grpc_helpers.py:174\u001b[0m, in \u001b[0;36m_wrap_stream_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mChatGoogleGenerativeAIError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m task \u001b[38;5;241m=\u001b[39m cf\u001b[38;5;241m.\u001b[39mTask(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite a two page article. Use the following information to help you with the text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m console_print(result)\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/controlflow/tasks/task.py:421\u001b[0m, in \u001b[0;36mTask.run\u001b[0;34m(self, agent, flow, turn_strategy, max_llm_calls, max_agent_turns, handlers, raise_on_failure, model_kwargs, stream)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    393\u001b[0m     agent: Optional[Agent] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    401\u001b[0m     stream: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    402\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[T, Iterator[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvent\u001b[39m\u001b[38;5;124m\"\u001b[39m, Any, Optional[Any]]]]:\n\u001b[1;32m    403\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;124;03m    Run the task\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m        If streaming: Iterator of (event, snapshot, delta) tuples\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mcontrolflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_tasks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mturn_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mturn_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_llm_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_llm_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_agent_turns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_agent_turns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraise_on_failure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhandlers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhandlers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/controlflow/run.py:43\u001b[0m, in \u001b[0;36mrun_tasks\u001b[0;34m(tasks, instructions, flow, agent, turn_strategy, raise_on_failure, max_llm_calls, max_agent_turns, handlers, model_kwargs, run_until, stream)\u001b[0m\n\u001b[1;32m     34\u001b[0m orchestrator \u001b[38;5;241m=\u001b[39m Orchestrator(\n\u001b[1;32m     35\u001b[0m     tasks\u001b[38;5;241m=\u001b[39mtasks,\n\u001b[1;32m     36\u001b[0m     flow\u001b[38;5;241m=\u001b[39mflow,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     handlers\u001b[38;5;241m=\u001b[39mhandlers,\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m controlflow\u001b[38;5;241m.\u001b[39minstructions(instructions):\n\u001b[0;32m---> 43\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43morchestrator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_llm_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_llm_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_agent_turns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_agent_turns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_until\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_until\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;66;03m# Convert True to ALL filter, otherwise use provided filter\u001b[39;00m\n\u001b[1;32m     53\u001b[0m         stream_filter \u001b[38;5;241m=\u001b[39m Stream\u001b[38;5;241m.\u001b[39mALL \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m stream\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/controlflow/orchestration/orchestrator.py:371\u001b[0m, in \u001b[0;36mOrchestrator.run\u001b[0;34m(self, max_llm_calls, max_agent_turns, model_kwargs, run_until, stream)\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mpass\u001b[39;49;00m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m run_context\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/prefect/task_engine.py:1471\u001b[0m, in \u001b[0;36mrun_generator_task_sync\u001b[0;34m(task, task_run_id, task_run, parameters, wait_for, return_type, dependencies, context)\u001b[0m\n\u001b[1;32m   1468\u001b[0m                     engine\u001b[38;5;241m.\u001b[39mhandle_success(\u001b[38;5;28;01mNone\u001b[39;00m, transaction\u001b[38;5;241m=\u001b[39mtxn)\n\u001b[1;32m   1469\u001b[0m                     gen\u001b[38;5;241m.\u001b[39mthrow(exc)\n\u001b[0;32m-> 1471\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/prefect/task_engine.py:482\u001b[0m, in \u001b[0;36mSyncTaskRunEngine.result\u001b[0;34m(self, raise_on_failure)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raised \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m NotSet:\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;66;03m# if the task raised an exception, raise it\u001b[39;00m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_failure:\n\u001b[0;32m--> 482\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raised\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;66;03m# otherwise, return the exception\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raised\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/prefect/task_engine.py:805\u001b[0m, in \u001b[0;36mSyncTaskRunEngine.run_context\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_cancelled():\n\u001b[1;32m    803\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m CancelledError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask run cancelled by the task runner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 805\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_timeout(exc)\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/prefect/task_engine.py:1456\u001b[0m, in \u001b[0;36mrun_generator_task_sync\u001b[0;34m(task, task_run_id, task_run, parameters, wait_for, return_type, dependencies, context)\u001b[0m\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1455\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1456\u001b[0m         gen_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1457\u001b[0m         \u001b[38;5;66;03m# link the current state to the result for dependency tracking\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m         \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m         \u001b[38;5;66;03m# TODO: this could grow the task_run_result\u001b[39;00m\n\u001b[1;32m   1460\u001b[0m         \u001b[38;5;66;03m# dictionary in an unbounded way, so finding a\u001b[39;00m\n\u001b[1;32m   1461\u001b[0m         \u001b[38;5;66;03m# way to periodically clean it up (using\u001b[39;00m\n\u001b[1;32m   1462\u001b[0m         \u001b[38;5;66;03m# weakrefs or similar) would be good\u001b[39;00m\n\u001b[1;32m   1463\u001b[0m         link_state_to_result(engine\u001b[38;5;241m.\u001b[39mstate, gen_result)\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/controlflow/orchestration/orchestrator.py:280\u001b[0m, in \u001b[0;36mOrchestrator._run\u001b[0;34m(self, run_context, model_kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m turn_start\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# Run turn and yield its events\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_agent_turn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/controlflow/orchestration/orchestrator.py:239\u001b[0m, in \u001b[0;36mOrchestrator._run_agent_turn\u001b[0;34m(self, run_context, model_kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Run model and yield events\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx(orchestrator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\n\u001b[1;32m    246\u001b[0m run_context\u001b[38;5;241m.\u001b[39mllm_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/prefect/task_engine.py:1471\u001b[0m, in \u001b[0;36mrun_generator_task_sync\u001b[0;34m(task, task_run_id, task_run, parameters, wait_for, return_type, dependencies, context)\u001b[0m\n\u001b[1;32m   1468\u001b[0m                     engine\u001b[38;5;241m.\u001b[39mhandle_success(\u001b[38;5;28;01mNone\u001b[39;00m, transaction\u001b[38;5;241m=\u001b[39mtxn)\n\u001b[1;32m   1469\u001b[0m                     gen\u001b[38;5;241m.\u001b[39mthrow(exc)\n\u001b[0;32m-> 1471\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/prefect/task_engine.py:482\u001b[0m, in \u001b[0;36mSyncTaskRunEngine.result\u001b[0;34m(self, raise_on_failure)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raised \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m NotSet:\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;66;03m# if the task raised an exception, raise it\u001b[39;00m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_failure:\n\u001b[0;32m--> 482\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raised\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;66;03m# otherwise, return the exception\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raised\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/prefect/task_engine.py:805\u001b[0m, in \u001b[0;36mSyncTaskRunEngine.run_context\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_cancelled():\n\u001b[1;32m    803\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m CancelledError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask run cancelled by the task runner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 805\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_timeout(exc)\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/prefect/task_engine.py:1456\u001b[0m, in \u001b[0;36mrun_generator_task_sync\u001b[0;34m(task, task_run_id, task_run, parameters, wait_for, return_type, dependencies, context)\u001b[0m\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1455\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1456\u001b[0m         gen_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1457\u001b[0m         \u001b[38;5;66;03m# link the current state to the result for dependency tracking\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m         \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m         \u001b[38;5;66;03m# TODO: this could grow the task_run_result\u001b[39;00m\n\u001b[1;32m   1460\u001b[0m         \u001b[38;5;66;03m# dictionary in an unbounded way, so finding a\u001b[39;00m\n\u001b[1;32m   1461\u001b[0m         \u001b[38;5;66;03m# way to periodically clean it up (using\u001b[39;00m\n\u001b[1;32m   1462\u001b[0m         \u001b[38;5;66;03m# weakrefs or similar) would be good\u001b[39;00m\n\u001b[1;32m   1463\u001b[0m         link_state_to_result(engine\u001b[38;5;241m.\u001b[39mstate, gen_result)\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/controlflow/agents/agent.py:329\u001b[0m, in \u001b[0;36mAgent._run_model\u001b[0;34m(self, messages, tools, stream, model_kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    328\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:5523\u001b[0m, in \u001b[0;36mRunnableBindingBase.stream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5517\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstream\u001b[39m(\n\u001b[1;32m   5518\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5519\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   5520\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5521\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5522\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 5523\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   5524\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   5525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   5526\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   5527\u001b[0m     )\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:415\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrate_limiter\u001b[38;5;241m.\u001b[39macquire(blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1034\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._stream\u001b[0;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_stream\u001b[39m(\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1010\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1022\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[ChatGenerationChunk]:\n\u001b[1;32m   1023\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[1;32m   1024\u001b[0m         messages,\n\u001b[1;32m   1025\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         tool_choice\u001b[38;5;241m=\u001b[39mtool_choice,\n\u001b[1;32m   1033\u001b[0m     )\n\u001b[0;32m-> 1034\u001b[0m     response: GenerateContentResponse \u001b[38;5;241m=\u001b[39m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_generate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1041\u001b[0m     prev_usage_metadata: UsageMetadata \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:196\u001b[0m, in \u001b[0;36m_chat_with_retry\u001b[0;34m(generation_method, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m--> 196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py:336\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    334\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    335\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py:475\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 475\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py:376\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    374\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[0;32m--> 376\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py:398\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[0;32m--> 398\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutcome\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.13.1-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.13.1-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/tenacity/__init__.py:478\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 478\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    480\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Projekte/DeepResearch/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:190\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mChatGoogleGenerativeAIError\u001b[0m: Invalid argument provided to Gemini: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini"
     ]
    }
   ],
   "source": [
    "task = cf.Task(f\"Write a two page article. Use the following information to help you with the text: {response}\")\n",
    "\n",
    "result = task.run()\n",
    "\n",
    "\n",
    "console_print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
