{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import controlflow as cf\n",
    "\n",
    "from agent_utils import *\n",
    "from agent_tools import *\n",
    "\n",
    "from extensive_search import run_research\n",
    "\n",
    "FLASH2_MODEL = \"google/\" + config.FLASH2_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "We need an detailed and extensive overview of existing systems for running pretrained model (e.g., transformer models) up to 100 million parameters (inference only).\n",
    "To accomplish this, we plan to do a deep research by processing many webpages, PDFs, acticles etc. using LLM APIs (e.g., Google Gemini, GPT4o etc.).\n",
    "\n",
    "We use an existing Python agent framework (e.g., PydanticAI, LangChain, Controlflow) to build the system.\n",
    "\n",
    "We have access to different types of search tools:\n",
    "- Google Search API (general search with time range search)\n",
    "- Google Schoolar API (search papers)\n",
    "- Google News API (search news articles)\n",
    "- Could be expanded in the future.\n",
    "\n",
    "For this task, we plan to build our own deep research agent system that uses LLMs to process a workflow.\n",
    "\n",
    "The workflow has these basic working steps or tasks:\n",
    "- Based on a search query (user input) generate a research plan/instruction and suitable search queries for the search tools.\n",
    "- Use search APIs for generated search queries to collect information.\n",
    "- Use a reasoning model (LLM + thinking) to select suitable URLs to extract information from.\n",
    "- Use web crawlers to comvert HTML or PDF content to markdown text for the LLMs.\n",
    "- For the first markdown document use an LLM to rate the relevance and quality of the document.\n",
    "- For the first document also extract a summary of all relevant and important information.\n",
    "- Save the document with rating and contectualized summary.\n",
    "- For the second markdown document use the contectualized summary from the previous document, to genrate a rating and a new contextualized summary.\n",
    "- Repeat the earlier steps for each downloaded document.\n",
    "- Stop when a cerain document budget is reached.\n",
    "- Use the original user query and the compressed/accumulated summary to create a report based on the generated research plan/instruction.\n",
    "\n",
    "What do you think about this approach? How would you improve it?\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoningModel = ReasoningModel()\n",
    "basicSearchModel = BasicSearchModel()\n",
    "model = GeminiModel(config.LITE2_MODEL)\n",
    "cleanupModel = BasicAgent(model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await crawl4ai_website_async(\"https://the-decoder.com/anthropic-ceo-dario-amodei-urges-faster-eu-response-to-ai-challenges/\")\n",
    "console_print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
