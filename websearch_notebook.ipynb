{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from os.path import join, exists\n",
    "from os import listdir, makedirs\n",
    "from datetime import datetime\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from openai import OpenAI\n",
    "from openai import AsyncOpenAI\n",
    "import requests\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "from crawl4ai import *\n",
    "from pydantic_ai import Agent, RunContext\n",
    "from pydantic_ai.models.gemini import GeminiModel\n",
    "from dataclasses import dataclass\n",
    "from rich import print as rprint\n",
    "import asyncio\n",
    "import nest_asyncio \n",
    "# Add this line to allow nested event loops\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent \"HTTP/1.1 200 OK\"\n",
      "/home/dplaia/Projekte/DeepResearchAgent/.venv/lib/python3.13/site-packages/httpx/_content.py:204: DeprecationWarning: Use 'content=<...>' to upload raw bytes/text content.\n",
      "  warnings.warn(message, DeprecationWarning)\n",
      "INFO:httpx:HTTP Request: GET https://paperswithcode.com/api/v1/search/?items_per_page=5&q=test-time%20compute%20and%20training \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://google.serper.dev/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://google.serper.dev/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://google.serper.dev/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://google.serper.dev/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://google.serper.dev/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://google.serper.dev/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT].... → Crawl4AI 0.4.247\n",
      "[FETCH]... ↓ https://www.forwardfuture.ai/p/the-magic-of-prolon... | Status: True | Time: 0.00s\n",
      "[COMPLETE] ● https://www.forwardfuture.ai/p/the-magic-of-prolon... | Status: True | Total: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT].... → Crawl4AI 0.4.247\n",
      "[FETCH]... ↓ https://huggingface.co/spaces/HuggingFaceH4/blogpo... | Status: True | Time: 0.00s\n",
      "[COMPLETE] ● https://huggingface.co/spaces/HuggingFaceH4/blogpo... | Status: True | Total: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT].... → Crawl4AI 0.4.247\n",
      "[FETCH]... ↓ https://blog.ml.cmu.edu/2025/01/08/optimizing-llm-... | Status: True | Time: 0.00s\n",
      "[COMPLETE] ● https://blog.ml.cmu.edu/2025/01/08/optimizing-llm-... | Status: True | Total: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT].... → Crawl4AI 0.4.247\n",
      "[FETCH]... ↓ https://www.ikangai.com/test-time-training-a-break... | Status: True | Time: 1.53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dplaia/Projekte/DeepResearchAgent/.venv/lib/python3.13/site-packages/bs4/builder/_lxml.py:124: DeprecationWarning: The 'strip_cdata' option of HTMLParser() has never done anything and will eventually be removed.\n",
      "  parser = parser(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRAPE].. ◆ Processed https://www.ikangai.com/test-time-training-a-break... | Time: 36ms\n",
      "[COMPLETE] ● https://www.ikangai.com/test-time-training-a-break... | Status: True | Total: 1.59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://google.serper.dev/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent \"HTTP/1.1 200 OK\"\n",
      "/home/dplaia/Projekte/DeepResearchAgent/.venv/lib/python3.13/site-packages/httpx/_content.py:204: DeprecationWarning: Use 'content=<...>' to upload raw bytes/text content.\n",
      "  warnings.warn(message, DeprecationWarning)\n",
      "INFO:httpx:HTTP Request: POST https://google.serper.dev/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://google.serper.dev/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://google.serper.dev/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://paperswithcode.com/api/v1/search/?items_per_page=5&q=Scaling%20LLM%20Test-Time%20Compute%20Optimally \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed: 'NoneType' object is not subscriptable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://google.serper.dev/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://google.serper.dev/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "ename": "UnexpectedModelBehavior",
     "evalue": "Unexpected response from gemini 429, body:\n{\n  \"error\": {\n    \"code\": 429,\n    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n    \"status\": \"RESOURCE_EXHAUSTED\"\n  }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedModelBehavior\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 46\u001b[0m\n\u001b[1;32m     38\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(\n\u001b[1;32m     39\u001b[0m     model,\n\u001b[1;32m     40\u001b[0m     result_type\u001b[38;5;241m=\u001b[39mResponse,\n\u001b[1;32m     41\u001b[0m     system_prompt\u001b[38;5;241m=\u001b[39msystem_prompt,\n\u001b[1;32m     42\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools_list\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     45\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFind me papers about test-time compute and training.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 46\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mrun(query)\n",
      "File \u001b[0;32m~/Projekte/DeepResearchAgent/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py:300\u001b[0m, in \u001b[0;36mAgent.run\u001b[0;34m(self, user_prompt, message_history, model, deps, model_settings, usage_limits, usage, result_type, infer_name)\u001b[0m\n\u001b[1;32m    297\u001b[0m     agent_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_model(run_context, result_schema)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _logfire\u001b[38;5;241m.\u001b[39mspan(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel request\u001b[39m\u001b[38;5;124m'\u001b[39m, run_step\u001b[38;5;241m=\u001b[39mrun_context\u001b[38;5;241m.\u001b[39mrun_step) \u001b[38;5;28;01mas\u001b[39;00m model_req_span:\n\u001b[0;32m--> 300\u001b[0m     model_response, request_usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m agent_model\u001b[38;5;241m.\u001b[39mrequest(messages, model_settings)\n\u001b[1;32m    301\u001b[0m     model_req_span\u001b[38;5;241m.\u001b[39mset_attribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m, model_response)\n\u001b[1;32m    302\u001b[0m     model_req_span\u001b[38;5;241m.\u001b[39mset_attribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m'\u001b[39m, request_usage)\n",
      "File \u001b[0;32m~/Projekte/DeepResearchAgent/.venv/lib/python3.13/site-packages/pydantic_ai/models/gemini.py:174\u001b[0m, in \u001b[0;36mGeminiAgentModel.request\u001b[0;34m(self, messages, model_settings)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m, messages: \u001b[38;5;28mlist\u001b[39m[ModelMessage], model_settings: ModelSettings \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    173\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ModelResponse, usage\u001b[38;5;241m.\u001b[39mUsage]:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(messages, \u001b[38;5;28;01mFalse\u001b[39;00m, model_settings) \u001b[38;5;28;01mas\u001b[39;00m http_response:\n\u001b[1;32m    175\u001b[0m         response \u001b[38;5;241m=\u001b[39m _gemini_response_ta\u001b[38;5;241m.\u001b[39mvalidate_json(\u001b[38;5;28;01mawait\u001b[39;00m http_response\u001b[38;5;241m.\u001b[39maread())\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(response), _metadata_as_usage(response)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.13.1-linux-x86_64-gnu/lib/python3.13/contextlib.py:214\u001b[0m, in \u001b[0;36m_AsyncGeneratorContextManager.__aenter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m anext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projekte/DeepResearchAgent/.venv/lib/python3.13/site-packages/pydantic_ai/models/gemini.py:229\u001b[0m, in \u001b[0;36mGeminiAgentModel._make_request\u001b[0;34m(self, messages, streamed, model_settings)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m r\u001b[38;5;241m.\u001b[39maread()\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedModelBehavior(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnexpected response from gemini \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, r\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m r\n",
      "\u001b[0;31mUnexpectedModelBehavior\u001b[0m: Unexpected response from gemini 429, body:\n{\n  \"error\": {\n    \"code\": 429,\n    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n    \"status\": \"RESOURCE_EXHAUSTED\"\n  }\n}"
     ]
    }
   ],
   "source": [
    "from WebSearchAgent import *\n",
    "\n",
    "class Response(BaseModel):\n",
    "    text_response: str = Field(description=\"The text response of the agent.\")\n",
    "    additional_notes: str = Field(description=\"Additional notes or observations (optional).\")\n",
    "    tools_used: list[str] = Field(description=\"List all tools that you have used (e.g. Google Search, Papers With Code, etc.)\")\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a web search agent. Choose the search and crawling tools that available to you.\n",
    "Write a report of the search results. Write it down as a form of a news article/blog post.\n",
    "\n",
    "Use the webcrawler for links that you find in the search results but only if it seems useful.\n",
    "\n",
    "Number of function calling are limited:\n",
    "\n",
    "For news you can use:\n",
    "- google_general_search (up to 3 calls)\n",
    "- google_news_search  (up to 3 calls)\n",
    "\n",
    "For scientific papers:\n",
    "- google_scholar_search (up to 3 calls)\n",
    "- papers_with_code_search  (up to 3 calls)\n",
    "\n",
    "Web crawling:\n",
    "- crawl_website_async (up to 3 calls in total).\n",
    "\n",
    "If possible add citations. e.g. [1].\n",
    "\n",
    "At the end of the document:\n",
    "[1] www.example.com\n",
    "[2] etc.\n",
    "\n",
    "The output/formatting should be MarkDown. \n",
    "\"\"\"\n",
    "tools_list = [google_general_search, google_scholar_search, papers_with_code_search, crawl_website_async, google_news_search] #perplexity_search,\n",
    "\n",
    "# Create agent with selected tools\n",
    "agent = Agent(\n",
    "    model,\n",
    "    result_type=Response,\n",
    "    system_prompt=system_prompt,\n",
    "    tools=tools_list\n",
    ")\n",
    "\n",
    "query = \"Find me papers about test-time compute and training.\"\n",
    "result = await agent.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Here's a summary of the information found on test-time compute and training:                                       \n",
       "\n",
       "<span style=\"font-weight: bold\">Test-Time Compute (TTC)</span>:                                                                                           \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>It is a method that uses additional computational resources during the inference phase to improve model         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>performance. It's like giving the model more time to \"think\" about its answers, allowing it to engage in more   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>complex reasoning.                                                                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>It involves dynamic retrieval of learned patterns, focusing on adapting to specific inputs during application,  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>rather than relying solely on pre-trained parameters.                                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>TTC can involve strategies like iterative self-refinement or verifier-guided search.                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>It is often used with Chain of Thought (CoT) to make models reason step-by-step, which is similar to system 2   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>thinking in humans.                                                                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>It has been shown that scaling test-time compute can be more effective than scaling model size for certain      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>problems.                                                                                                       \n",
       "\n",
       "<span style=\"font-weight: bold\">Test-Time Training (TTT):</span>                                                                                          \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>It is a technique where the model continues to learn during inference by updating its parameters based on new   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>inputs. It's like a musician learning a new instrument during a concert.                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>TTT is more computationally intensive than TTC because the model actively updates its parameters during         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>inference.                                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>It is particularly useful when models need to adapt to new situations or data during usage.                     \n",
       "\n",
       "<span style=\"font-weight: bold\">Relation between TTC and TTT:</span>                                                                                      \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>TTC is like a musician playing their practiced notes while TTT is like a musician learning a new instrument in  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>the middle of a concert. The former is fast and efficient, whereas the latter is more flexible and adaptable.   \n",
       "\n",
       "<span style=\"font-weight: bold\">Reasoning Models:</span>                                                                                                  \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>They are a new kind of LLM that use test-time compute to improve their performance on tasks requiring careful   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>reasoning, especially in math and coding. They solve problems step-by-step and are able to self-correct and     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>backtrack when detecting errors.                                                                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>These models embody System 2 thinking - deliberate and methodical thinking.                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>They are different from traditional LLMs, which are more like System 1 thinking - quick, intuitive, and based on\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>pattern recognition.                                                                                            \n",
       "\n",
       "<span style=\"font-weight: bold\">Meta-RL and TTC:</span>                                                                                                   \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>Optimizing test-time compute can be viewed as a meta-RL problem, where the model learns an algorithm to solve   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>queries at test time.                                                                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>This view allows for the development of training objectives that can efficiently use test-time compute.         \n",
       "\n",
       "<span style=\"font-weight: bold\">Limitations of Test-Time Compute:</span>                                                                                  \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>It is not a universal solution. While it improves performance on problems within a model's capabilities,        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>pre-training with more data is more effective for harder problems.                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>TTC is not a replacement for training on diverse data or increasing the model size, but a way to use            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>computational resources more effectively during inference.                                                      \n",
       "\n",
       "In summary, test-time compute and training are emerging techniques that are changing how AI models are used and    \n",
       "optimized. They move away from simply training larger models and focus more on optimizing how models 'think' during\n",
       "usage. While test-time compute is useful, it has limitations, and combining it with other techniques, like training\n",
       "on more data, is needed to improve model performance.                                                              \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Here's a summary of the information found on test-time compute and training:                                       \n",
       "\n",
       "\u001b[1mTest-Time Compute (TTC)\u001b[0m:                                                                                           \n",
       "\n",
       "\u001b[1;33m • \u001b[0mIt is a method that uses additional computational resources during the inference phase to improve model         \n",
       "\u001b[1;33m   \u001b[0mperformance. It's like giving the model more time to \"think\" about its answers, allowing it to engage in more   \n",
       "\u001b[1;33m   \u001b[0mcomplex reasoning.                                                                                              \n",
       "\u001b[1;33m • \u001b[0mIt involves dynamic retrieval of learned patterns, focusing on adapting to specific inputs during application,  \n",
       "\u001b[1;33m   \u001b[0mrather than relying solely on pre-trained parameters.                                                           \n",
       "\u001b[1;33m • \u001b[0mTTC can involve strategies like iterative self-refinement or verifier-guided search.                            \n",
       "\u001b[1;33m • \u001b[0mIt is often used with Chain of Thought (CoT) to make models reason step-by-step, which is similar to system 2   \n",
       "\u001b[1;33m   \u001b[0mthinking in humans.                                                                                             \n",
       "\u001b[1;33m • \u001b[0mIt has been shown that scaling test-time compute can be more effective than scaling model size for certain      \n",
       "\u001b[1;33m   \u001b[0mproblems.                                                                                                       \n",
       "\n",
       "\u001b[1mTest-Time Training (TTT):\u001b[0m                                                                                          \n",
       "\n",
       "\u001b[1;33m • \u001b[0mIt is a technique where the model continues to learn during inference by updating its parameters based on new   \n",
       "\u001b[1;33m   \u001b[0minputs. It's like a musician learning a new instrument during a concert.                                        \n",
       "\u001b[1;33m • \u001b[0mTTT is more computationally intensive than TTC because the model actively updates its parameters during         \n",
       "\u001b[1;33m   \u001b[0minference.                                                                                                      \n",
       "\u001b[1;33m • \u001b[0mIt is particularly useful when models need to adapt to new situations or data during usage.                     \n",
       "\n",
       "\u001b[1mRelation between TTC and TTT:\u001b[0m                                                                                      \n",
       "\n",
       "\u001b[1;33m • \u001b[0mTTC is like a musician playing their practiced notes while TTT is like a musician learning a new instrument in  \n",
       "\u001b[1;33m   \u001b[0mthe middle of a concert. The former is fast and efficient, whereas the latter is more flexible and adaptable.   \n",
       "\n",
       "\u001b[1mReasoning Models:\u001b[0m                                                                                                  \n",
       "\n",
       "\u001b[1;33m • \u001b[0mThey are a new kind of LLM that use test-time compute to improve their performance on tasks requiring careful   \n",
       "\u001b[1;33m   \u001b[0mreasoning, especially in math and coding. They solve problems step-by-step and are able to self-correct and     \n",
       "\u001b[1;33m   \u001b[0mbacktrack when detecting errors.                                                                                \n",
       "\u001b[1;33m • \u001b[0mThese models embody System 2 thinking - deliberate and methodical thinking.                                     \n",
       "\u001b[1;33m • \u001b[0mThey are different from traditional LLMs, which are more like System 1 thinking - quick, intuitive, and based on\n",
       "\u001b[1;33m   \u001b[0mpattern recognition.                                                                                            \n",
       "\n",
       "\u001b[1mMeta-RL and TTC:\u001b[0m                                                                                                   \n",
       "\n",
       "\u001b[1;33m • \u001b[0mOptimizing test-time compute can be viewed as a meta-RL problem, where the model learns an algorithm to solve   \n",
       "\u001b[1;33m   \u001b[0mqueries at test time.                                                                                           \n",
       "\u001b[1;33m • \u001b[0mThis view allows for the development of training objectives that can efficiently use test-time compute.         \n",
       "\n",
       "\u001b[1mLimitations of Test-Time Compute:\u001b[0m                                                                                  \n",
       "\n",
       "\u001b[1;33m • \u001b[0mIt is not a universal solution. While it improves performance on problems within a model's capabilities,        \n",
       "\u001b[1;33m   \u001b[0mpre-training with more data is more effective for harder problems.                                              \n",
       "\u001b[1;33m • \u001b[0mTTC is not a replacement for training on diverse data or increasing the model size, but a way to use            \n",
       "\u001b[1;33m   \u001b[0mcomputational resources more effectively during inference.                                                      \n",
       "\n",
       "In summary, test-time compute and training are emerging techniques that are changing how AI models are used and    \n",
       "optimized. They move away from simply training larger models and focus more on optimizing how models 'think' during\n",
       "usage. While test-time compute is useful, it has limitations, and combining it with other techniques, like training\n",
       "on more data, is needed to improve model performance.                                                              \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'google_scholar_search'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'papers_with_code_search'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'google_general_search'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'google_news_search'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'crawl_website_async'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'google_scholar_search'\u001b[0m,\n",
       "    \u001b[32m'papers_with_code_search'\u001b[0m,\n",
       "    \u001b[32m'google_general_search'\u001b[0m,\n",
       "    \u001b[32m'google_news_search'\u001b[0m,\n",
       "    \u001b[32m'crawl_website_async'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "# Create a console instance\n",
    "console = Console()\n",
    "\n",
    "md = Markdown(result.data.text_response)\n",
    "console.print(md)\n",
    "\n",
    "rprint(result.data.tools_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepseek_R1_call(user_input: str) -> dict:\n",
    "    # Initialize DeepSeek client\n",
    "    deepseek_client = OpenAI(\n",
    "        api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "        base_url=\"https://api.deepseek.com\"\n",
    "    )\n",
    "    deepseek_messages = []\n",
    "    deepseek_messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    DEEPSEEK_MODEL = \"deepseek-reasoner\"\n",
    "\n",
    "    response = deepseek_client.chat.completions.create(\n",
    "                model=DEEPSEEK_MODEL,\n",
    "                #max_tokens=1,\n",
    "                messages=deepseek_messages,\n",
    "                stream=True\n",
    "            )\n",
    "\n",
    "    reasoning_content = \"\"\n",
    "    final_content = \"\"\n",
    "\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.reasoning_content:\n",
    "            reasoning_piece = chunk.choices[0].delta.reasoning_content\n",
    "            reasoning_content += reasoning_piece\n",
    "        elif chunk.choices[0].delta.content:\n",
    "            final_content += chunk.choices[0].delta.content\n",
    "\n",
    "    response = {\n",
    "        \"reasoning_content\": reasoning_content,\n",
    "        \"final_content\": final_content\n",
    "    }\n",
    "    return response        \n",
    "\n",
    "class ReasoningModelQuery(BaseModel):\n",
    "    instruction: str = Field(description=\"The instruction you want give the reasoning model.\")\n",
    "    motivation: str = Field(description=\"The main reason why you ask the reasoning model.\")\n",
    "    additional_context: str = Field(description=\"Add the full context here (e.g., crawled website/markdown text).\")\n",
    "\n",
    "def ask_reasoning_agent(query: ReasoningModelQuery) -> dict:\n",
    "    \"\"\"\n",
    "    Ask a reasoning agent to help you solve problems.\n",
    "\n",
    "    Args:\n",
    "        user_input (ReasoningModelQuery): Contains information about the motivation for the request, an instruction to follow and the additional context (e.g., document, article, PDF, crawled website etc.) \n",
    "\n",
    "    Returns:\n",
    "        dict: The answer of the AI model as a string. Containing the reasoning thoughts and the final response.\n",
    "    \"\"\"\n",
    "\n",
    "    str_query = f\"\"\"\n",
    "    You are working together with other LLM-based agents to answer user questions.\n",
    "    Agents will ask you questions to help them making decisions.\n",
    "    You are a reasoning model and know how to resolve problems.\n",
    "    Take your time thinking about the query of the agent.\n",
    "\n",
    "    Motivation of the agent:\n",
    "    {query.motivation}\n",
    "\n",
    "    Agent Instruction:\n",
    "    {query.instruction}\n",
    "\n",
    "    Additional Context:\n",
    "    {query.additional_context}\n",
    "\n",
    "    \"\"\"\n",
    "    return deepseek_R1_call(str_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenQuestion(BaseModel):\n",
    "    question_number: int = Field(description=\"The question number. Each number has a unique number starting from zero.\", ge=0)\n",
    "    question: str = Field(description=\"A question that needs to be answered\")\n",
    "    notes: list[str] = Field(description=\"A list of notes/information related to the question that help to answer the question as good as possible.\")\n",
    "    rating: int = Field(description=\"The quality of the temporary response (0 to 10)\", ge=0, le=10)\n",
    "\n",
    "class QuestionQueue(BaseModel):\n",
    "    list_of_questions: list[OpenQuestion] = Field(description=\"A list of questions that need to be answered\")\n",
    "\n",
    "question_queue = QuestionQueue(queue=[])\n",
    "\n",
    "def get_open_questions() -> QuestionQueue:\n",
    "    \"\"\"\n",
    "    Returns the current question queue.\n",
    "    \n",
    "    The question queue contains a list of OpenQuestion objects that need to be answered.\n",
    "    Each OpenQuestion object contains a question string, a list of notes that may help to answer the question and a rating.\n",
    "    The rating is the quality of the temporary response (0 to 10) to the question.\n",
    "    \n",
    "    The question queue is a shared object between all agents and is used to keep track of the progress of the question answering process.\n",
    "    \"\"\"\n",
    "    global question_queue\n",
    "\n",
    "    try: \n",
    "        # Make sure that each question has correct unique question number\n",
    "        for k in range(len(question_queue.list_of_questions)):\n",
    "            question_queue.list_of_questions[k].question_number = k\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return question_queue\n",
    "\n",
    "class UpdateRequestQueue(BaseModel):\n",
    "    question_number: int = Field(description=\"The question number. Each number has a unique number starting from zero.\", ge=0)\n",
    "    notes: list[str] = Field(description=\"A list of notes/information related to the question that help to answer the question as good as possible.\")\n",
    "    rating: int = Field(description=\"The quality of the temporary response (0 to 10)\", ge=0, le=10)\n",
    "\n",
    "def update_open_question_in_queue(update_request: UpdateRequestQueue):\n",
    "    \n",
    "    global question_queue\n",
    "    question_number = getattr(update_request, 'question_number', None)\n",
    "\n",
    "    if question_number & question_queue:\n",
    "        if (question_number < len(question_queue.list_of_questions)):\n",
    "\n",
    "            if getattr(update_request, 'notes', None)\n",
    "                question_queue.list_of_questions[question_number].notes = update_request.notes\n",
    "            \n",
    "            if getattr(update_request, 'rating', None)\n",
    "                question_queue.list_of_questions[question_number].rating = update_request.rating\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from WebSearchAgent import *\n",
    "system_prompt = \n",
    "    \"\"\"\n",
    "    You are a web search expert and your goal is to answer a list of user question using external search tools.\n",
    "    You don't have to solve all questions, just try to find the best answer using only the tools available to you.\n",
    "\n",
    "    You belong to a group of experts that try to answer all questions in a question queue. \n",
    "    At the beginning, there might be many questions that are too complex for you to handle.\n",
    "    Collect information usesing the available search tools an save important notes using the save_notes function/tool.\n",
    "    You will then also give a rating on how complete the answer for each question is, so that the next agent knows which questions to answer next.\n",
    "\n",
    "    Our goal is to do deep research. This means, we don't need quick responses. It's okay, if it takes more than 15 min. to answer all questions in the queue. \n",
    "    You can ask the a reasoning agent to give you feedback about what to search/which tools to use etc., just add all information needed to help you. \n",
    "\n",
    "    How to answer the question in the question queue:\n",
    "        - Pick a question that is currently not answered.\n",
    "        - Focus only on the most important question and only one question.\n",
    "        - Don't asker more than one question.\n",
    "        - Don't answer the question directly, even if you think you know the answer.\n",
    "        - Use the search tools (at least one) to help you collect information.\n",
    "        - If the questions seems to be resolved, there is no need to use more search tools.\n",
    "\n",
    "    Information about the search tools:\n",
    "        - Sometimes many tools are available to you, sometimes maybe only one.\n",
    "        - They only provide an overview of search result (except perplexity search, if tool is available).\n",
    "        - The search usually contain web URLs.\n",
    "        - The crawl_website_async tool be used to convert a website or a PDF in markdown format.\n",
    "        - You can use the reason model to help you decide which url should be processed next.\n",
    "        - You should use each search tool max. once (not more) one question.\n",
    "        - The search tools are cheap to use (don't worry using them).\n",
    "    \n",
    "    Inforrmation about the web crawler:\n",
    "        - The tool needs a web url that leads to a website or a PDF document (e.g., ArXiv paper).\n",
    "        - The tool will return a string representing the website or PDF in markdown format.\n",
    "        - Use it to get more information for a give search result.\n",
    "\n",
    "    Information about the reasoning agent:\n",
    "        - This agent is available to you in form as a tool/function call.\n",
    "        - Use this agent to help you make decisions\n",
    "        - The context window of this agent is large (arround 64k).\n",
    "        - Therefore, you can append the crawled markdown text, when asking questions.\n",
    "        - The agent is cheap to use: \n",
    "            $0.14 / million input tokens (cache hit)\n",
    "            $0.55 / million input tokens (cache miss)\n",
    "            $2.19 / million output tokens\n",
    "        - Don't hesitate using this model to help you with everything.\n",
    "        - You can ask the reasoning agent multiple times in a row.\n",
    "        - The reasoning model returns the reasoning thoughts  and the final response to your query.\n",
    "        - Don't dismiss the reasoning thoughts, they can be very useful.\n",
    "    \n",
    "    Information about the question queue:\n",
    "        - Every agent in the loop or chain has access to the question queue (data structure/database).\n",
    "        - You can get all information about the question queue by using the tool 'get_open_questions'.\n",
    "        - You should use this function only once and decide which question to answer next (most relevant question first).\n",
    "\n",
    "    Here is an example how you can process the questions:\n",
    "        - Start by calling the 'get_open_questions' function to get all open questions.\n",
    "        - Selected one question that seems to be most relevant (use reasoning model if you're not sure).\n",
    "        - Based on the nature of the question select the appropriate search tool (can can use multiple in a row) to get an overview.\n",
    "        - After getting an overview of the search results, consult the reasoning agent, which search results seems most promissing to get good results.\n",
    "        - Use the crawl tool function to get a markdown response from the URL link.\n",
    "        - Use the reasoning model again with the additional context to collect notes/information or get a summary of the given context.\n",
    "        - If you have collected all information/notes for a question, update the question queue by calling the function: 'update_open_question_in_queue'\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "tools_list = [ask_reasoning_agent, get_open_questions, update_open_question_in_queue, google_general_search, google_scholar_search, papers_with_code_search, crawl_website_async] #perplexity_search,\n",
    "\n",
    "# Create agent with selected tools\n",
    "agent = Agent(\n",
    "    model,\n",
    "    result_type=Response,\n",
    "    system_prompt=system_prompt,\n",
    "    tools=tools_list\n",
    ")\n",
    "\n",
    "#result = await agent.run('abc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
