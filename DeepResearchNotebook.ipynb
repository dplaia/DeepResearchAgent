{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_utils import *\n",
    "from agent_tools import *\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Log to a file with custom timestamp format\n",
    "logger.add(\"logs/chain_of_thougth_agent_system.log\", format=\"{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}\")\n",
    "model = GeminiModel(config.FLASH2_MODEL)\n",
    "\n",
    "logfire.configure(scrubbing=logfire.ScrubbingOptions(callback=scrubbing_callback))\n",
    "\n",
    "logfire.instrument_httpx()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 10, 'b': 20}\n"
     ]
    }
   ],
   "source": [
    "def save_data(data, name):\n",
    "    dump_folder = \"temp_data/\"\n",
    "    file_path = join(dump_folder, name + \".pkl\")\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "data = {'a': 10, 'b': 20}\n",
    "save_data(data, \"myData\")\n",
    "\n",
    "def load_data(name):\n",
    "    dump_folder = \"temp_data/\"\n",
    "    file_path = join(dump_folder, name + \".pkl\")\n",
    "\n",
    "    if exists(file_path):\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        return data\n",
    "    \n",
    "    return None\n",
    "\n",
    "if mydata := load_data(\"myData\"):\n",
    "    print(mydata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chat handler with your credentials and desired model.\n",
    "reasoningAgent = ChatHandler()\n",
    "    \n",
    "class SearchQueryAgentResponse(BaseModel):\n",
    "    google_search_queries: list[str] = Field(description=\"The extracted google search queries.\")\n",
    "    google_scholar_queries: list[str] = Field(description=\"The extracted google scholar queries.\")\n",
    "    text_summary: str = Field(description=\"Extract the text summary here (if it's present in the input text).\")\n",
    "\n",
    "searchQueryAgent = BaseAgent(SearchQueryAgentResponse, \n",
    "        system_prompt=\"\"\"Your goal is to extract search queries (e.g., google search, google scholar, etc.) \n",
    "        that are mention in a text.\"\"\")\n",
    "\n",
    "class URLRating(BaseModel):\n",
    "    url_number: int = Field(description=\"The URL number\", ge=0)\n",
    "    rating: int = Field(description=\"The rating (value between 0 and 100) for the URL\", ge=0, le=100)\n",
    "\n",
    "class URLRatingAgentResponse(BaseModel):\n",
    "    url_info: list[URLRating] = Field(description=\"A list with URLs with corresponding ratings.\")\n",
    "\n",
    "urlRatingAgent = BaseAgent(URLRatingAgentResponse, \n",
    "        system_prompt=\"\"\"Your goal is to extract URL number and the corresponding rating. \n",
    "        The input text has the following format: {1, 60}, {2,75}, {3, 35} .... \n",
    "        with  \n",
    "        {URL number, Rating}\"\"\")\n",
    "\n",
    "def get_document():\n",
    "    documents = {}\n",
    "    folder_name = 'input_files/'\n",
    "\n",
    "    # Create directory if it doesn't exist\n",
    "    if not exists(folder_name):\n",
    "        makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    main_input_file = join(folder_name, \"Vorhabenbeschreibung_NeuroTrust.txt\")\n",
    "\n",
    "    if not exists(main_input_file):\n",
    "        print(\"file does not exists\")\n",
    "        # Process each file in the input directory\n",
    "        for filename in listdir(folder_name):\n",
    "            filepath = join(folder_name, filename)\n",
    "            \n",
    "            if not os.path.isfile(filepath):\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                md = MarkItDown()\n",
    "                result = md.convert(filepath)\n",
    "                filename = os.path.basename(filepath)\n",
    "                documents[filename] = result.text_content\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filepath}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        doc = \"\"\n",
    "        for filename in documents:\n",
    "            print(f\"Filename: {filename}\")\n",
    "            doc = documents[filename]\n",
    "            count = word_count(doc)\n",
    "            print(f\"Number of Words in the document: {count}\")\n",
    "\n",
    "            break\n",
    "\n",
    "        with open(main_input_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(doc)\n",
    "\n",
    "    else:\n",
    "        with open(main_input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            doc = f.read()\n",
    "\n",
    "    return doc\n",
    "\n",
    "async def get_search_queries_for_document(doc: str):\n",
    "    query = get_system_prompt(\"search_query_recommendation\")\n",
    "    query += f\"\"\"\n",
    "    # Input Document: \n",
    "\n",
    "    {doc}\n",
    "    \"\"\"\n",
    "    \n",
    "    text_response = await reasoningAgent(query)\n",
    "    queries = await searchQueryAgent(f\"Please extract all search queries from this text: {text_response}\")\n",
    "\n",
    "    return queries.data\n",
    "\n",
    "\n",
    "async def rate_search_results(content_text: str):\n",
    "    query = f\"\"\"\n",
    "    Please rate each search result based on relevance (value between 0 and 100).\n",
    "    Each search result has an URL with an URL number, \n",
    "    for example: \"Link [143]: https...\", where 143 is the URL number.\n",
    "\n",
    "    Your generated output format should look like this:\n",
    "\n",
    "    (1, 60), (2,75), (3, 35) ....\n",
    "    \n",
    "    with \n",
    "    \n",
    "    (URL number, Rating)  \n",
    "    \n",
    "    Here are the search results based on your search query suggestions:\n",
    "    \n",
    "    {content_text}\n",
    "    \"\"\"\n",
    "    text_response = await reasoningAgent(query)\n",
    "\n",
    "    url_info = await urlRatingAgent(f\"Please extract the url info in this text: {text_response}\")\n",
    "\n",
    "    return url_info.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    doc = get_document()\n",
    "    if not (queries := load_data(\"search_query_suggestions\")):\n",
    "        queries = await get_search_queries_for_document(doc)\n",
    "        save_data(queries, \"search_query_suggestions\")\n",
    "    \n",
    "    google_search_queries = queries.google_search_queries\n",
    "    google_scholar_queries = queries.google_scholar_queries\n",
    "    document_summary = queries.text_summary\n",
    "    console_print(document_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (search_results := load_data(\"google_search_results\"))\n",
    "    search_results = {}\n",
    "\n",
    "    for search_query in google_search_queries:\n",
    "        print(search_query)\n",
    "        results = await google_general_search_async(search_query)\n",
    "        search_results[search_query] = results\n",
    "    \n",
    "    save_data(search_results, \"google_search_results\")\n",
    "\n",
    "if not (scholar_results := load_data(\"google_scholar_results\"))\n",
    "    scholar_results = {}\n",
    "\n",
    "    for search_query in google_scholar_queries:\n",
    "        print(search_query)\n",
    "        results = await google_scholar_search_async(search_query)\n",
    "        scholar_results[search_query] = results\n",
    "    \n",
    "    save_data(scholar_results, \"google_scholar_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "content.append(\"Google Search Results:\")\n",
    "link_counter = 1\n",
    "\n",
    "links = {}\n",
    "\n",
    "for query in search_results:\n",
    "    result = search_results[query]\n",
    "\n",
    "    content.append(f\"\\nFor search query: '{result['searchParameters']['q']}'.\")\n",
    "\n",
    "    if 'answerBox' in result:\n",
    "        content.append(f\"\\nAnswerBox Text: {result['answerBox']['snippet']}\")\n",
    "    content.append(\"\\nOrganic results:\")\n",
    "\n",
    "    for organic in result['organic']:\n",
    "        #rprint(organic)\n",
    "        content.append(f\"Title: {organic['title']}\")\n",
    "        content.append(f\"Link [{link_counter}]: {organic['link']}\")\n",
    "        links[link_counter] = organic['link']\n",
    "        content.append(f\"Snippet: {organic['snippet']}\")\n",
    "\n",
    "        if 'date' in organic:\n",
    "            content.append(f\"Date: {organic['date']}\")\n",
    "\n",
    "        if 'attributes' in organic:\n",
    "            content.append(f\"Attributes: {organic['attributes']}\")\n",
    "        \n",
    "        content.append(\"\\n\")\n",
    "        link_counter+=1\n",
    "\n",
    "\n",
    "content_text = \"\\n\".join(content)\n",
    "\n",
    "\n",
    "content2 = []\n",
    "content2.append(\"\\nGoogle Scholar Results:\")\n",
    "\n",
    "for query in scholar_results:\n",
    "    result = scholar_results[query] # list\n",
    "    content2.append(f\"\\nFor search query: '{query}'.\\n\")\n",
    "    for entry in result:\n",
    "\n",
    "        content2.append(f\"Title: {entry['title']}\")\n",
    "        content2.append(f\"Link [{link_counter}]: {entry['link']}\")\n",
    "        links[link_counter] = entry['link']\n",
    "        content2.append(f\"Snippet: {entry['snippet']}\")\n",
    "        if 'date' in entry:\n",
    "            content2.append(f\"Date: {entry['date']}\")\n",
    "        if 'attributes' in entry:\n",
    "            content2.append(f\"Attributes: {entry['attributes']}\")\n",
    "        #rprint(entry)\n",
    "        content2.append(\"\\n\")\n",
    "        link_counter+=1\n",
    "\n",
    "content_text = \"\\n\".join(content2)\n",
    "#rprint(content_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
