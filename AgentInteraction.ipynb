{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pprint import pprint\n",
    "from rich import print\n",
    "from os.path import join, exists\n",
    "from os import listdir, makedirs\n",
    "from datetime import datetime\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "import asyncio\n",
    "import nest_asyncio \n",
    "from crawl4ai import *\n",
    "from pydantic_ai import Agent, RunContext, ModelRetry, UnexpectedModelBehavior, capture_run_messages\n",
    "from pydantic_ai.models.gemini import GeminiModel\n",
    "from dataclasses import dataclass\n",
    "# Add this line to allow nested event loops\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "flash_thinking_model = \"gemini-2.0-flash-thinking-exp-01-21\"\n",
    "flash2_model = \"gemini-2.0-flash-exp\"\n",
    "flash1_model = \"gemini-1.5-flash\"\n",
    "deepseek_r1_model = \"deepseek/deepseek-r1\"\n",
    "model = GeminiModel(flash2_model)\n",
    "\n",
    "# @dataclass\n",
    "# class MainDependencies(BaseModel):\n",
    "#     document: str\n",
    "#     response: str\n",
    "\n",
    "class ModelRating(BaseModel):\n",
    "    rating: int = Field(description=\"The rating of the model (0 to 10).\", ge=0, le=10)\n",
    "    model_name: str = Field(description=\"The name of the model (if accessable).\")\n",
    "    evaluation_result: str = Field(description=\"Your assessment for this model. How good is the model? Describe it in detail.\")\n",
    "    chellege_questions: list[str] = Field(description=\"A list of the chellenge questions asked to test models.\")\n",
    "    model_responses: list[str] = Field(description=\"A list of responses of the model.\")   \n",
    "    \n",
    "class Response(BaseModel):\n",
    "    model_rating: list[ModelRating] = Field(description=\"A list of model ratings.\")\n",
    "    additional_notes: str = Field(description=\"Additional notes or observations.\")\n",
    "     \n",
    "\n",
    "system_prompt=\"\"\"You goal is to test reasoning models (available as tools). \n",
    "Come up with a chellenging task that is relatively easy to verify to you but difficult to solve for other models if you provide only the task.\n",
    "You can try different tests, if you're not sure how good the models are. \n",
    "Provide a system instruction to the model that seems optimial for reasoning models. \n",
    "Avoid chain-of-though instructions. \n",
    "The models tend to perform better if they have more time to think.\n",
    "Come up with a system prompt that makes them think longer.\n",
    "\"\"\"\n",
    "\n",
    "agent = Agent(\n",
    "    model,\n",
    "    result_type=Response,\n",
    "    system_prompt=system_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@agent.tool_plain\n",
    "async def run_model_deepseek_r1_agent(query: str, system_instruction_test: str =\"You are a helpful assistent\") -> str:\n",
    "    \"\"\"Uses the DeepSeek API to retrieve information based on a string query.\n",
    "    This model is a reasoner model and uses thinking tokens to generate better results.\n",
    "    Reasoning models tend to be better accross the board but especially in math, reasoning and coding.\n",
    "    This model is very powerful, open-source and cheap to use (2$ per 1M output tokens).\n",
    "\n",
    "    Args:\n",
    "        query (str): The LLM query string.\n",
    "        system_instruction_test (str, optional): The system instruction to use for the query. Defaults to \"You are a helpful assistent\". Adjust it, if it necessary.\n",
    "\n",
    "    Returns:\n",
    "        str: The output string of the DeepSeek R1 model. The returned string is the final answer of the AI model.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    api_key = os.environ.get(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "    if not api_key:\n",
    "        print(\"OPENROUTER_API_KEY not found in environment variables.\")\n",
    "        return \"OPENROUTER_API_KEY not found in environment variables.\"\n",
    "    \n",
    "    client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=api_key,\n",
    "    )\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "    model=deepseek_r1_model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": system_instruction_test\n",
    "                }\n",
    "            ]\n",
    "        },        \n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": query\n",
    "        }\n",
    "    ]\n",
    "    )\n",
    "    return completion.choices[0].message.content      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@agent.tool_plain\n",
    "async def run_model_genai_agent(query: str, system_instruction: str = \"You are a helpful assistent\") -> str:\n",
    "    \"\"\"Uses the Gemini API to use the Gemini-2.0 Flash Thinking model.\n",
    "    This model is a reasoner model and uses thinking tokens to generate better results.\n",
    "    Reasoning models tend to be better accross the board but especially in math, reasoning and coding.\n",
    "    This model is very powerful and free (1500 Req./day), has a context window of 1M tokens and can generate up to 65k tokens in one go.\n",
    "\n",
    "    Args:\n",
    "        query (str): The LLM query string.\n",
    "        system_instruction (str, optional): The system instruction to use for the query. Defaults to \"You are a helpful assistent\". Adjust it, if it necessary.\n",
    "\n",
    "    Returns:\n",
    "        str: The output string of the Gemini-2.0 Flash Thinking model. The returned string is the final answer of the AI model.\n",
    "    \"\"\"\n",
    "    # Only run this block for Gemini Developer API\n",
    "    client = genai.Client()    \n",
    "    flash_thinking_model = \"gemini-2.0-flash-thinking-exp-01-21\"\n",
    "    response = client.models.generate_content(\n",
    "        model=flash_thinking_model,\n",
    "        contents=query,\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=system_instruction,\n",
    "            temperature=0.3,\n",
    "        ),\n",
    "    )\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_agent():\n",
    "    result = await agent.run('Test the model on chellenging tasks and give it a rating.')\n",
    "    return result\n",
    " \n",
    "result = asyncio.run(run_agent())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with capture_run_messages() as messages:  \n",
    "    try:\n",
    "        result = agent.run_sync('Test the model(s) on chellenging tasks and give a rating.')\n",
    "        print(result.all_messages())\n",
    "    except UnexpectedModelBehavior as e:\n",
    "        print('An error occurred:', e)\n",
    "        #> An error occurred: Tool exceeded max retries count of 1\n",
    "        print('Cause:', repr(e.__cause__))\n",
    "        #> cause: ModelRetry('Please try again.')\n",
    "        #print('Messages:', messages)\n",
    "        pprint(vars(messages[0]))\n",
    "\n",
    "    else:\n",
    "        print(f\"Result: {result.data}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
