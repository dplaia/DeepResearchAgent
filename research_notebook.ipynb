{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example DeepResearchAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from os.path import join, exists\n",
    "from os import listdir, makedirs\n",
    "from datetime import datetime\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import requests\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "import asyncio\n",
    "import nest_asyncio \n",
    "from crawl4ai import *\n",
    "\n",
    "# Add this line to allow nested event loops\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Google Search API Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API key is stored in the environment variable `SERPER_API_KEY`.  \n",
    "An account can be created [here](https://serper.dev/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_search_results(query, num_results=10):\n",
    "    api_key = os.environ.get(\"SERPER_API_KEY\")\n",
    "\n",
    "    if not api_key:\n",
    "        print(\"SERPER_API_KEY not found in environment variables.\")\n",
    "        return\n",
    "\n",
    "    url = \"https://google.serper.dev/search\"\n",
    "    payload = json.dumps({\n",
    "    \"q\": query,\n",
    "    \"num\": num_results\n",
    "    })\n",
    "    headers = {\n",
    "    'X-API-KEY': api_key,\n",
    "    'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"Test-time compute and test-time training for Large Language Models.\"\n",
    "response = get_google_search_results(topic)\n",
    "# Convert to JSON\n",
    "json_response = response.json()\n",
    "print(json_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl Webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def crawl_website_async(url_webpage):\n",
    "    async with AsyncWebCrawler() as crawler:\n",
    "        result = await crawler.arun(\n",
    "            url=url_webpage,\n",
    "        )\n",
    "        return result.markdown\n",
    "\n",
    "def crawl_website(url_webpage):\n",
    "    return asyncio.run(crawl_website_async(url_webpage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_folder = \"test-time-compute\"\n",
    "# Create folder if not exists\n",
    "if not exists(query_folder):\n",
    "    makedirs(query_folder)\n",
    "\n",
    "document_data = {}\n",
    "\n",
    "for result in json_response['organic']:\n",
    "    title = result['title']\n",
    "    markdown = crawl_website(result['link'])\n",
    "    filename = result['title'] + \".md\"\n",
    "\n",
    "    document_data[title] = {\n",
    "        'topic': topic,\n",
    "        'link': result['link'],\n",
    "        'snippet': result['snippet'],\n",
    "        'date': result['date'],\n",
    "        'position': result['position'],\n",
    "        'markdown': markdown,\n",
    "        'filename': filename\n",
    "    }\n",
    "\n",
    "    with open(join(query_folder, filename), \"w\") as f:\n",
    "        f.write(markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crate an Agent that rates the quality of generated content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this block for Gemini Developer API\n",
    "client = genai.Client()\n",
    "\n",
    "model_name_thinking = \"gemini-2.0-flash-thinking-exp-01-21\"\n",
    "model_name = \"gemini-2.0-flash-exp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentQuality(BaseModel):\n",
    "    filename: str = Field(description=\"The name of the file\")   \n",
    "    document_length: int = Field(description=\"The length of the document (0 to 10). Is the document short or long.\", ge=0, le=10)\n",
    "    relevance: int = Field(description=\"How relevant is the document with the main topic (0 to 10).\", ge=0, le=10)\n",
    "    document_quality: int = Field(description=\"Guess the quality of the document (0 to 10).\", ge=0, le=10)\n",
    "    document_age: int = Field(description=\"The age of the document relative to the current data (0 to 10).\", ge=0, le=10)\n",
    "    additional_observations: str = Field(description=\"If you noticed something strange about the document. Write it in form of an instruction for another LLM agents.\")\n",
    "\n",
    "def rate_document(document):\n",
    "\n",
    "    system_instruction = f\"\"\"\n",
    "    You are an professional scientific journalist. \n",
    "\n",
    "    You will receive research related documents (markdown format). \n",
    "    Your goal is to estimate the relevance and quality of this document (based on a given topic).\n",
    "    The document will later be used for writing a reasearch report/document.\n",
    "    If the quality of the text isn't good, this will lead to an overall bad outcome of the report. \n",
    "\n",
    "    Topic: {document['topic']}    \n",
    "    Current Date: {datetime.now().date()}\n",
    "    Document Date: {document['date']}\n",
    "    Link: {document['link']}\n",
    "    \"\"\"\n",
    "\n",
    "    markdown_content = markdown\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=model_name,\n",
    "        contents=markdown_content,\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=DocumentQuality,\n",
    "            system_instruction=system_instruction,\n",
    "            temperature=0.3,\n",
    "        ),\n",
    "    )\n",
    "    document['response'] = response\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Files (Quality Assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: Optimizing LLM Test-Time Compute Involves Solving a Meta-RL ....md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reponse Text: {\n",
      "\"additional_observations\": \"The document is a blog post from CMU's Machine Learning blog, discussing a novel approach to optimizing LLM test-time compute using meta-RL. It provides a detailed explanation of the problem, the proposed solution, and the challenges involved. The post is well-structured, includes figures, and provides links to relevant resources. It also includes a citation section and social media sharing options.\",\n",
      "  \"document_length\": 8,\n",
      "  \"document_quality\": 9,\n",
      "  \"filename\": \"test-time-compute/Optimizing LLM Test-Time Compute Involves Solving a Meta-RL ....md\",\n",
      "  \"relevance\": 10\n",
      "}\n",
      "File: Understanding Test-Time Compute: A New Mechanism Allowing AI ....md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reponse Text: {\n",
      "  \"additional_observations\": \"The document provides a good overview of test-time compute, explaining its importance and how it works. However, it lacks specific technical details and could benefit from more concrete examples or research citations. The writing style is accessible to a general audience, but might need more depth for a scientific report.\",\n",
      "  \"document_length\": 7,\n",
      "  \"document_quality\": 7,\n",
      "  \"filename\": \"test-time-compute/Understanding Test-Time Compute: A New Mechanism Allowing AI ....md\",\n",
      "    \"relevance\": 9\n",
      "}\n",
      "File: Train Less, Think More: Advancing LLMs Through Test-Time Compute.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reponse Text: {\n",
      "\"additional_observations\": \"The document is a well-written article explaining test-time compute for LLMs. It provides a good overview of the topic, including relevant research and examples. The use of analogies and clear explanations makes it accessible to a broad audience. The article also includes links to external resources and references, which is good for further exploration. The structure is logical, starting with an introduction to LLMs, moving to the problem of scaling, and then introducing test-time compute as a solution. The author also provides a good summary of the main ideas. The article is well-formatted and easy to read.\",\n",
      "\"document_length\": 8,\n",
      "\"document_quality\": 9,\n",
      "\"filename\": \"test-time-compute/Train Less, Think More: Advancing LLMs Through Test-Time Compute.md\",\n",
      "\"relevance\": 9\n",
      "}\n",
      "File: Scaling LLM Test Time Compute.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reponse Text: {\n",
      "\"additional_observations\": \"The document is a well-structured blog post that provides a comprehensive overview of test-time compute for Large Language Models (LLMs). It includes clear explanations, relevant examples, and references to key research papers. The author also provides a good summary of the current state of the field and future directions. The document is well-written and easy to understand, making it a valuable resource for anyone interested in this topic. The document is very well structured and easy to follow. The author also provides links to the original papers, which is very helpful for further reading. The document is also very well formatted and easy to read. The author also provides links to the original papers, which is very helpful for further reading. The document is also very well formatted and easy to read.\",\n",
      "  \"document_length\": 9,\n",
      "  \"document_quality\": 9,\n",
      "  \"filename\": \"test-time-compute/Scaling LLM Test Time Compute.md\",\n",
      "  \"relevance\": 10\n",
      "}\n",
      "File: Test Time Training Will Take LLM AI to the Next Level.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reponse Text: {\n",
      "  \"additional_observations\": \"The document is a blog post discussing test-time training for large language models. It includes technical details, performance metrics, and links to external sources. The writing style is journalistic and aims to inform a general audience about a complex topic.\",\n",
      "  \"document_length\": 7,\n",
      "  \"document_quality\": 7,\n",
      "  \"filename\": \"test-time-compute/Test Time Training Will Take LLM AI to the Next Level.md\",\n",
      "  \"relevance\": 9\n",
      "}\n",
      "File: Test-Time Compute: The Next Frontier in AI Scaling - IKANGAI.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reponse Text: {\n",
      "  \"additional_observations\": \"The document is a well-structured article discussing test-time compute in AI, including its mechanisms, benefits, and implications. It includes an FAQ section and references, making it a comprehensive resource. The writing is clear and concise, suitable for a scientific audience.\",\n",
      "  \"document_length\": 8,\n",
      "  \"document_quality\": 9,\n",
      "  \"filename\": \"test-time-compute/Test-Time Compute: The Next Frontier in AI Scaling - IKANGAI.md\",\n",
      "  \"relevance\": 10\n",
      "}\n",
      "File: Scaling LLM Test-Time Compute Optimally can be More Effective ....md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reponse Text: {\n",
      "\"additional_observations\": \"The document is a research paper on scaling test-time compute for Large Language Models (LLMs), focusing on improving performance on math reasoning tasks. It introduces a 'compute-optimal' scaling strategy based on question difficulty, comparing different approaches for scaling test-time compute, and analyzing the trade-offs between test-time compute and pre-training compute. The paper is well-structured, with clear sections, figures, and tables. It includes detailed experimental setups, analysis, and results. The authors also discuss limitations and future work.\",\n",
      "\"document_length\": 8,\n",
      "\"document_quality\": 9,\n",
      "\"filename\": \"test-time-compute/Scaling LLM Test-Time Compute Optimally can be More Effective ....md\",\n",
      "\"relevance\": 10\n",
      "}\n",
      "File: Scaling test-time compute - a Hugging Face Space by ....md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reponse Text: {\n",
      "\"additional_observations\": \"The document appears to be a Hugging Face Space page, not a research document. It contains metadata and interactions, not research content. It is not suitable for a research report.\",\n",
      "\"document_length\": 1,\n",
      "\"document_quality\": 1,\n",
      "\"filename\": \"test-time-compute/Scaling test-time compute - a Hugging Face Space by ....md\",\n",
      "\"relevance\": 1\n",
      "}\n",
      "File: What is Test Time Compute? | CSA.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reponse Text: {\n",
      "  \"additional_observations\": \"The document is a blog post from the Cloud Security Alliance (CSA) website. It provides a detailed explanation of Test Time Compute (TTC) for Large Language Models (LLMs). The content is well-structured, covering key aspects, advanced strategies, implications, and future directions of TTC. It also includes a code example and references. The document is informative and relevant to the topic of test-time compute and test-time training for Large Language Models.\",\n",
      "  \"document_length\": 7,\n",
      "  \"document_quality\": 9,\n",
      "  \"filename\": \"test-time-compute/What is Test Time Compute? | CSA.md\",\n",
      "  \"relevance\": 10\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create folder if not exists\n",
    "if exists(query_folder):\n",
    "    # Get all files in folder\n",
    "    files = listdir(query_folder)\n",
    "\n",
    "for title, document in document_data.items():\n",
    "    print(f\"File: {document.filename}\")\n",
    "    document = rate_document(document)\n",
    "    print(f\"Reponse Text: {response.text}\")\n",
    "    document_data[title] = document\n",
    "    time.sleep(5) # sleep for 5 seconds (rate limit is 10 RPM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=model_name_thinking, contents=\"Is it difficult to find large prime numbers? If yes, why?\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"You are an AI assistant designed to produce output that is visually appealing and easily readable in a terminal. When formatting your responses, utilize the syntax of the Python `rich` library. This involves using square brackets to enclose formatting tags.\n",
    "        Here are some examples of how to apply formatting:\n",
    "\n",
    "        * **Emphasis:** Instead of \"This is important\", output \"[bold]This is important[/]\".\n",
    "        * **Headers/Titles:** Instead of \"Section Title:\", output \"[bold blue]Section Title:[/]\".\n",
    "        * **Warnings:** Instead of \"Warning!\", output \"[bold red]Warning![/]\".\n",
    "        * **Success Messages:** Instead of \"Operation successful.\", output \"[green]Operation successful.[/]\".\n",
    "        * **Lists:** You can use colors for list items like \"[cyan]*[/] Item 1\".\n",
    "\n",
    "        Always use the `rich` library's syntax for formatting terminal output to enhance readability.\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=model_name_thinking,\n",
    "    contents=\"Show me the proof for the euler identity?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        temperature=0.3,\n",
    "    ),\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = client.files.upload(path=\"a11.text\")\n",
    "response = client.models.generate_content(\n",
    "    model=model_name, contents=[\"Summarize this file\", file]\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dplaia/Projekte/DeepResearchAgent/.venv/lib/python3.13/site-packages/pydantic/main.py:426: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `enum` but got `str` with value `'STRING'` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in Boston is sunny.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_current_weather(location: str) -> str:\n",
    "    \"\"\"Returns the current weather.\n",
    "\n",
    "    Args:\n",
    "      location: The city and state, e.g. San Francisco, CA\n",
    "    \"\"\"\n",
    "    return \"sunny\"\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    contents=\"What is the weather like in Boston?\",\n",
    "    config=types.GenerateContentConfig(tools=[get_current_weather]),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"capital\": \"Washington, D.C.\",\n",
      "\"continent\": \"North America\",\n",
      "\"gdp\": 25460000000000,\n",
      "\"name\": \"United States\",\n",
      "\"official_language\": \"English\",\n",
      "\"population\": 331002651,\n",
      "\"total_area_sq_mi\": 3796742\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "class CountryInfo(BaseModel):\n",
    "    name: str\n",
    "    population: int\n",
    "    capital: str\n",
    "    continent: str\n",
    "    gdp: int\n",
    "    official_language: str\n",
    "    total_area_sq_mi: int\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    contents=\"Give me information for the United States.\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=CountryInfo,\n",
    "    ),\n",
    ")\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
