{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from os.path import join, exists\n",
    "from os import listdir, makedirs\n",
    "from datetime import datetime\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from openai import OpenAI\n",
    "from openai import AsyncOpenAI\n",
    "import requests\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "from crawl4ai import *\n",
    "from pydantic_ai import Agent, RunContext\n",
    "from pydantic_ai.models.gemini import GeminiModel\n",
    "from dataclasses import dataclass\n",
    "from rich import print\n",
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "import asyncio\n",
    "import nest_asyncio \n",
    "# Add this line to allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from WebSearchAgent import *\n",
    "from agent_tools import *\n",
    "from agent_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response(BaseModel):\n",
    "    text_response: str = Field(description=\"The text response of the agent.\")\n",
    "    additional_notes: str = Field(description=\"Additional notes or observations (optional).\")\n",
    "    tools_used: list[str] = Field(description=\"List all tools that you have used (e.g. Google Search, Papers With Code, etc.)\")\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a web search agent. Choose the search and crawling tools that available to you.\n",
    "Write a report of the search results. Write it down as a form of a news article/blog post.\n",
    "\n",
    "Use the webcrawler for links that you find in the search results but only if it seems useful.\n",
    "\n",
    "Number of function calling are limited:\n",
    "\n",
    "For news you can use:\n",
    "- google_general_search (up to 3 calls)\n",
    "- google_news_search  (up to 3 calls)\n",
    "\n",
    "For scientific papers:\n",
    "- google_scholar_search (up to 3 calls)\n",
    "- papers_with_code_search  (up to 3 calls)\n",
    "\n",
    "Web crawling:\n",
    "- crawl_website_async (up to 3 calls in total).\n",
    "\n",
    "If possible add citations. e.g. [1].\n",
    "\n",
    "At the end of the document:\n",
    "[1] www.example.com\n",
    "[2] etc.\n",
    "\n",
    "The output/formatting should be MarkDown. \n",
    "\"\"\"\n",
    "tools_list = [google_general_search, google_scholar_search, papers_with_code_search, crawl_website_async, google_news_search] #perplexity_search,\n",
    "\n",
    "# Create agent with selected tools\n",
    "agent = Agent(\n",
    "    model,\n",
    "    result_type=Response,\n",
    "    system_prompt=system_prompt,\n",
    "    tools=tools_list\n",
    ")\n",
    "\n",
    "query = \"Find me papers about test-time compute and training.\"\n",
    "result = await agent.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "# Create a console instance\n",
    "console = Console()\n",
    "\n",
    "md = Markdown(result.data.text_response)\n",
    "console.print(md)\n",
    "\n",
    "rprint(result.data.tools_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepseek_R1_call(user_input: str) -> dict:\n",
    "    # Initialize DeepSeek client\n",
    "    deepseek_client = OpenAI(\n",
    "        api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "        base_url=\"https://api.deepseek.com\"\n",
    "    )\n",
    "    deepseek_messages = []\n",
    "    deepseek_messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    DEEPSEEK_MODEL = \"deepseek-reasoner\"\n",
    "\n",
    "    response = deepseek_client.chat.completions.create(\n",
    "                model=DEEPSEEK_MODEL,\n",
    "                #max_tokens=1,\n",
    "                messages=deepseek_messages,\n",
    "                stream=True\n",
    "            )\n",
    "\n",
    "    reasoning_content = \"\"\n",
    "    final_content = \"\"\n",
    "\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.reasoning_content:\n",
    "            reasoning_piece = chunk.choices[0].delta.reasoning_content\n",
    "            reasoning_content += reasoning_piece\n",
    "        elif chunk.choices[0].delta.content:\n",
    "            final_content += chunk.choices[0].delta.content\n",
    "\n",
    "    response = {\n",
    "        \"reasoning_content\": reasoning_content,\n",
    "        \"final_content\": final_content\n",
    "    }\n",
    "    return response        \n",
    "\n",
    "class ReasoningModelQuery(BaseModel):\n",
    "    instruction: str = Field(description=\"The instruction you want give the reasoning model.\")\n",
    "    motivation: str = Field(description=\"The main reason why you ask the reasoning model.\")\n",
    "    additional_context: str = Field(description=\"Add the full context here (e.g., crawled website/markdown text).\")\n",
    "\n",
    "def ask_reasoning_agent(query: ReasoningModelQuery) -> dict:\n",
    "    \"\"\"\n",
    "    Ask a reasoning agent to help you solve problems.\n",
    "\n",
    "    Args:\n",
    "        user_input (ReasoningModelQuery): Contains information about the motivation for the request, an instruction to follow and the additional context (e.g., document, article, PDF, crawled website etc.) \n",
    "\n",
    "    Returns:\n",
    "        dict: The answer of the AI model as a string. Containing the reasoning thoughts and the final response.\n",
    "    \"\"\"\n",
    "\n",
    "    str_query = f\"\"\"\n",
    "    You are working together with other LLM-based agents to answer user questions.\n",
    "    Agents will ask you questions to help them making decisions.\n",
    "    You are a reasoning model and know how to resolve problems.\n",
    "    Take your time thinking about the query of the agent.\n",
    "\n",
    "    Motivation of the agent:\n",
    "    {query.motivation}\n",
    "\n",
    "    Agent Instruction:\n",
    "    {query.instruction}\n",
    "\n",
    "    Additional Context:\n",
    "    {query.additional_context}\n",
    "\n",
    "    \"\"\"\n",
    "    return deepseek_R1_call(str_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenQuestion(BaseModel):\n",
    "    question_number: int = Field(description=\"The question number. Each number has a unique number starting from zero.\", ge=0)\n",
    "    question: str = Field(description=\"A question that needs to be answered\")\n",
    "    notes: list[str] = Field(description=\"A list of notes/information related to the question that help to answer the question as good as possible.\")\n",
    "    rating: int = Field(description=\"The quality of the temporary response (0 to 10)\", ge=0, le=10)\n",
    "\n",
    "class QuestionQueue(BaseModel):\n",
    "    list_of_questions: list[OpenQuestion] = Field(description=\"A list of questions that need to be answered\")\n",
    "\n",
    "question_queue = QuestionQueue(queue=[])\n",
    "\n",
    "def get_open_questions() -> QuestionQueue:\n",
    "    \"\"\"\n",
    "    Returns the current question queue.\n",
    "    \n",
    "    The question queue contains a list of OpenQuestion objects that need to be answered.\n",
    "    Each OpenQuestion object contains a question string, a list of notes that may help to answer the question and a rating.\n",
    "    The rating is the quality of the temporary response (0 to 10) to the question.\n",
    "    \n",
    "    The question queue is a shared object between all agents and is used to keep track of the progress of the question answering process.\n",
    "    \"\"\"\n",
    "    global question_queue\n",
    "\n",
    "    try: \n",
    "        # Make sure that each question has correct unique question number\n",
    "        for k in range(len(question_queue.list_of_questions)):\n",
    "            question_queue.list_of_questions[k].question_number = k\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return question_queue\n",
    "\n",
    "class UpdateRequestQueue(BaseModel):\n",
    "    question_number: int = Field(description=\"The question number. Each number has a unique number starting from zero.\", ge=0)\n",
    "    notes: list[str] = Field(description=\"A list of notes/information related to the question that help to answer the question as good as possible.\")\n",
    "    rating: int = Field(description=\"The quality of the temporary response (0 to 10)\", ge=0, le=10)\n",
    "\n",
    "def update_open_question_in_queue(update_request: UpdateRequestQueue):\n",
    "    \n",
    "    global question_queue\n",
    "    question_number = getattr(update_request, 'question_number', None)\n",
    "\n",
    "    if question_number & question_queue:\n",
    "        if (question_number < len(question_queue.list_of_questions)):\n",
    "\n",
    "            if getattr(update_request, 'notes', None)\n",
    "                question_queue.list_of_questions[question_number].notes = update_request.notes\n",
    "            \n",
    "            if getattr(update_request, 'rating', None)\n",
    "                question_queue.list_of_questions[question_number].rating = update_request.rating\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from WebSearchAgent import *\n",
    "system_prompt = \n",
    "    \"\"\"\n",
    "    You are a web search expert and your goal is to answer a list of user question using external search tools.\n",
    "    You don't have to solve all questions, just try to find the best answer using only the tools available to you.\n",
    "\n",
    "    You belong to a group of experts that try to answer all questions in a question queue. \n",
    "    At the beginning, there might be many questions that are too complex for you to handle.\n",
    "    Collect information usesing the available search tools an save important notes using the save_notes function/tool.\n",
    "    You will then also give a rating on how complete the answer for each question is, so that the next agent knows which questions to answer next.\n",
    "\n",
    "    Our goal is to do deep research. This means, we don't need quick responses. It's okay, if it takes more than 15 min. to answer all questions in the queue. \n",
    "    You can ask the a reasoning agent to give you feedback about what to search/which tools to use etc., just add all information needed to help you. \n",
    "\n",
    "    How to answer the question in the question queue:\n",
    "        - Pick a question that is currently not answered.\n",
    "        - Focus only on the most important question and only one question.\n",
    "        - Don't asker more than one question.\n",
    "        - Don't answer the question directly, even if you think you know the answer.\n",
    "        - Use the search tools (at least one) to help you collect information.\n",
    "        - If the questions seems to be resolved, there is no need to use more search tools.\n",
    "\n",
    "    Information about the search tools:\n",
    "        - Sometimes many tools are available to you, sometimes maybe only one.\n",
    "        - They only provide an overview of search result (except perplexity search, if tool is available).\n",
    "        - The search usually contain web URLs.\n",
    "        - The crawl_website_async tool be used to convert a website or a PDF in markdown format.\n",
    "        - You can use the reason model to help you decide which url should be processed next.\n",
    "        - You should use each search tool max. once (not more) one question.\n",
    "        - The search tools are cheap to use (don't worry using them).\n",
    "    \n",
    "    Inforrmation about the web crawler:\n",
    "        - The tool needs a web url that leads to a website or a PDF document (e.g., ArXiv paper).\n",
    "        - The tool will return a string representing the website or PDF in markdown format.\n",
    "        - Use it to get more information for a give search result.\n",
    "\n",
    "    Information about the reasoning agent:\n",
    "        - This agent is available to you in form as a tool/function call.\n",
    "        - Use this agent to help you make decisions\n",
    "        - The context window of this agent is large (arround 64k).\n",
    "        - Therefore, you can append the crawled markdown text, when asking questions.\n",
    "        - The agent is cheap to use: \n",
    "            $0.14 / million input tokens (cache hit)\n",
    "            $0.55 / million input tokens (cache miss)\n",
    "            $2.19 / million output tokens\n",
    "        - Don't hesitate using this model to help you with everything.\n",
    "        - You can ask the reasoning agent multiple times in a row.\n",
    "        - The reasoning model returns the reasoning thoughts  and the final response to your query.\n",
    "        - Don't dismiss the reasoning thoughts, they can be very useful.\n",
    "    \n",
    "    Information about the question queue:\n",
    "        - Every agent in the loop or chain has access to the question queue (data structure/database).\n",
    "        - You can get all information about the question queue by using the tool 'get_open_questions'.\n",
    "        - You should use this function only once and decide which question to answer next (most relevant question first).\n",
    "\n",
    "    Here is an example how you can process the questions:\n",
    "        - Start by calling the 'get_open_questions' function to get all open questions.\n",
    "        - Selected one question that seems to be most relevant (use reasoning model if you're not sure).\n",
    "        - Based on the nature of the question select the appropriate search tool (can can use multiple in a row) to get an overview.\n",
    "        - After getting an overview of the search results, consult the reasoning agent, which search results seems most promissing to get good results.\n",
    "        - Use the crawl tool function to get a markdown response from the URL link.\n",
    "        - Use the reasoning model again with the additional context to collect notes/information or get a summary of the given context.\n",
    "        - If you have collected all information/notes for a question, update the question queue by calling the function: 'update_open_question_in_queue'\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "tools_list = [ask_reasoning_agent, get_open_questions, update_open_question_in_queue, google_general_search, google_scholar_search, papers_with_code_search, crawl_website_async] #perplexity_search,\n",
    "\n",
    "# Create agent with selected tools\n",
    "agent = Agent(\n",
    "    model,\n",
    "    result_type=Response,\n",
    "    system_prompt=system_prompt,\n",
    "    tools=tools_list\n",
    ")\n",
    "\n",
    "#result = await agent.run('abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m markdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m crawl4ai_website_async(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://brainchip.com/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Code Projects\\deepresearchagent\\agent_tools.py:267\u001b[0m, in \u001b[0;36mcrawl4ai_website_async\u001b[1;34m(url_webpage)\u001b[0m\n\u001b[0;32m    263\u001b[0m     result \u001b[38;5;241m=\u001b[39m md\u001b[38;5;241m.\u001b[39mconvert(url_webpage)\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mtext_content\n\u001b[1;32m--> 267\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m AsyncWebCrawler() \u001b[38;5;28;01mas\u001b[39;00m crawler:\n\u001b[0;32m    268\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m crawler\u001b[38;5;241m.\u001b[39marun(\n\u001b[0;32m    269\u001b[0m         url\u001b[38;5;241m=\u001b[39murl_webpage,\n\u001b[0;32m    270\u001b[0m     )\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mmarkdown\n",
      "File \u001b[1;32mc:\\Code Projects\\deepresearchagent\\.venv\\Lib\\site-packages\\crawl4ai\\async_webcrawler.py:218\u001b[0m, in \u001b[0;36mAsyncWebCrawler.__aenter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[1;32mc:\\Code Projects\\deepresearchagent\\.venv\\Lib\\site-packages\\crawl4ai\\async_webcrawler.py:202\u001b[0m, in \u001b[0;36mAsyncWebCrawler.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    190\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03m    Start the crawler explicitly without using context manager.\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m    This is equivalent to using 'async with' but gives more control over the lifecycle.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m        AsyncWebCrawler: The initialized crawler instance\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrawler_strategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__aenter__\u001b[39m()\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mawarmup()\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Code Projects\\deepresearchagent\\.venv\\Lib\\site-packages\\crawl4ai\\async_crawler_strategy.py:750\u001b[0m, in \u001b[0;36mAsyncPlaywrightCrawlerStrategy.__aenter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 750\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Code Projects\\deepresearchagent\\.venv\\Lib\\site-packages\\crawl4ai\\async_crawler_strategy.py:760\u001b[0m, in \u001b[0;36mAsyncPlaywrightCrawlerStrategy.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    757\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;124;03m    Start the browser and initialize the browser manager.\u001b[39;00m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 760\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbrowser_manager\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m    761\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_hook(\n\u001b[0;32m    762\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_browser_created\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    763\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbrowser_manager\u001b[38;5;241m.\u001b[39mbrowser,\n\u001b[0;32m    764\u001b[0m         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbrowser_manager\u001b[38;5;241m.\u001b[39mdefault_context,\n\u001b[0;32m    765\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Code Projects\\deepresearchagent\\.venv\\Lib\\site-packages\\crawl4ai\\async_crawler_strategy.py:349\u001b[0m, in \u001b[0;36mBrowserManager.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplaywright \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplaywright\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masync_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m async_playwright\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplaywright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m async_playwright()\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_managed_browser:\n\u001b[0;32m    352\u001b[0m     cdp_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanaged_browser\u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[1;32mc:\\Code Projects\\deepresearchagent\\.venv\\Lib\\site-packages\\playwright\\async_api\\_context_manager.py:51\u001b[0m, in \u001b[0;36mPlaywrightContextManager.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncPlaywright:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__aenter__\u001b[39m()\n",
      "File \u001b[1;32mc:\\Code Projects\\deepresearchagent\\.venv\\Lib\\site-packages\\playwright\\async_api\\_context_manager.py:46\u001b[0m, in \u001b[0;36mPlaywrightContextManager.__aenter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m playwright_future\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     45\u001b[0m     playwright_future\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m---> 46\u001b[0m playwright \u001b[38;5;241m=\u001b[39m AsyncPlaywright(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     47\u001b[0m playwright\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__aexit__\u001b[39m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m playwright\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\uv\\python\\cpython-3.13.1-windows-x86_64-none\\Lib\\asyncio\\futures.py:199\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Code Projects\\deepresearchagent\\.venv\\Lib\\site-packages\\playwright\\_impl\\_transport.py:120\u001b[0m, in \u001b[0;36mPipeTransport.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m         startupinfo\u001b[38;5;241m.\u001b[39mwShowWindow \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mSW_HIDE\n\u001b[0;32m    119\u001b[0m     executable_path, entrypoint_path \u001b[38;5;241m=\u001b[39m compute_driver_executable()\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_subprocess_exec(\n\u001b[0;32m    121\u001b[0m         executable_path,\n\u001b[0;32m    122\u001b[0m         entrypoint_path,\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun-driver\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    124\u001b[0m         stdin\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    125\u001b[0m         stdout\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    126\u001b[0m         stderr\u001b[38;5;241m=\u001b[39m_get_stderr_fileno(),\n\u001b[0;32m    127\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32768\u001b[39m,\n\u001b[0;32m    128\u001b[0m         env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m    129\u001b[0m         startupinfo\u001b[38;5;241m=\u001b[39mstartupinfo,\n\u001b[0;32m    130\u001b[0m     )\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_error_future\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\uv\\python\\cpython-3.13.1-windows-x86_64-none\\Lib\\asyncio\\subprocess.py:224\u001b[0m, in \u001b[0;36mcreate_subprocess_exec\u001b[1;34m(program, stdin, stdout, stderr, limit, *args, **kwds)\u001b[0m\n\u001b[0;32m    221\u001b[0m loop \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39mget_running_loop()\n\u001b[0;32m    222\u001b[0m protocol_factory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: SubprocessStreamProtocol(limit\u001b[38;5;241m=\u001b[39mlimit,\n\u001b[0;32m    223\u001b[0m                                                     loop\u001b[38;5;241m=\u001b[39mloop)\n\u001b[1;32m--> 224\u001b[0m transport, protocol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m loop\u001b[38;5;241m.\u001b[39msubprocess_exec(\n\u001b[0;32m    225\u001b[0m     protocol_factory,\n\u001b[0;32m    226\u001b[0m     program, \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m    227\u001b[0m     stdin\u001b[38;5;241m=\u001b[39mstdin, stdout\u001b[38;5;241m=\u001b[39mstdout,\n\u001b[0;32m    228\u001b[0m     stderr\u001b[38;5;241m=\u001b[39mstderr, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Process(transport, protocol, loop)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\uv\\python\\cpython-3.13.1-windows-x86_64-none\\Lib\\asyncio\\base_events.py:1787\u001b[0m, in \u001b[0;36mBaseEventLoop.subprocess_exec\u001b[1;34m(self, protocol_factory, program, stdin, stdout, stderr, universal_newlines, shell, bufsize, encoding, errors, text, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1785\u001b[0m     debug_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexecute program \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogram\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1786\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_subprocess(debug_log, stdin, stdout, stderr)\n\u001b[1;32m-> 1787\u001b[0m transport \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_subprocess_transport(\n\u001b[0;32m   1788\u001b[0m     protocol, popen_args, \u001b[38;5;28;01mFalse\u001b[39;00m, stdin, stdout, stderr,\n\u001b[0;32m   1789\u001b[0m     bufsize, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debug \u001b[38;5;129;01mand\u001b[39;00m debug_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1791\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, debug_log, transport)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\uv\\python\\cpython-3.13.1-windows-x86_64-none\\Lib\\asyncio\\base_events.py:534\u001b[0m, in \u001b[0;36mBaseEventLoop._make_subprocess_transport\u001b[1;34m(self, protocol, args, shell, stdin, stdout, stderr, bufsize, extra, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_make_subprocess_transport\u001b[39m(\u001b[38;5;28mself\u001b[39m, protocol, args, shell,\n\u001b[0;32m    531\u001b[0m                                      stdin, stdout, stderr, bufsize,\n\u001b[0;32m    532\u001b[0m                                      extra\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    533\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create subprocess transport.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 534\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "markdown = await crawl4ai_website_async(\"https://brainchip.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a console instance\n",
    "console = Console()\n",
    "\n",
    "md = Markdown(markdown)\n",
    "console.print(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a console instance\n",
    "console = Console()\n",
    "\n",
    "md = Markdown(markdown)\n",
    "console.print(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://google.serper.dev/search \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "results = await google_general_search(\"site:brainchip.com training models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ai.pydantic.dev\"\n",
    "result = await crawl_website(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"result.json\"\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(result, json_file)\n",
    "with open(file_path, \"r\") as json_file:\n",
    "    result = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_folder = \"pydantic_ai_docs/\"\n",
    "if not exists(destination_folder):\n",
    "    makedirs(destination_folder)\n",
    "\n",
    "for k in range(len(result['data'])):\n",
    "    md = result['data'][k]['markdown']\n",
    "    title = result['data'][k]['metadata']['title']\n",
    "    with open(join(destination_folder, f\"{title}.md\"), \"w\") as file:\n",
    "        file.write(md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
