{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from os.path import join, exists\n",
    "from os import listdir, makedirs\n",
    "from datetime import datetime\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from openai import OpenAI\n",
    "from openai import AsyncOpenAI\n",
    "import requests\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "from crawl4ai import *\n",
    "from pydantic_ai import Agent, RunContext\n",
    "from pydantic_ai.models.gemini import GeminiModel\n",
    "from dataclasses import dataclass\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "from queue import Queue, Empty\n",
    "import asyncio\n",
    "import nest_asyncio \n",
    "# Add this line to allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from agent_tools import *\n",
    "from agent_utils import *\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "config = Config()\n",
    "\n",
    "console = Console()\n",
    "# Log to a file with custom timestamp format\n",
    "logger.add(\"agent_output.log\", format=\"{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}\")\n",
    "model = GeminiModel(config.FLASH2_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Answer(BaseModel):\n",
    "   \n",
    "    answer: str = Field(description=\"The answer for the question (if available).\")\n",
    "    notes: list[str] = Field(description=\"Add study notes or pieces of text that helps other assistants to answer the question in the future. The better the quality of the notes, the more likely it is to get better responses in the future.\")\n",
    "    index: int = Field(description=\"Equals question number or index.\")\n",
    "    rating: int = Field(description=\"Rate the quality of your response between 0 and 10\", ge=0, le=10)\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question: str = Field(description=\"The question that needs to be answered.\")\n",
    "    index: int = Field(description=\"The question number or index.\")\n",
    "\n",
    "class Deps(BaseModel):\n",
    "    questions: dict[int, Question] = Field(description=\"A dict with questions that need to be answered (keys: question number, values: question).\")\n",
    "\n",
    "    def get_all_questions(self) -> dict:\n",
    "        return self.questions\n",
    "\n",
    "    def remove_question(self, idx: int) -> bool:\n",
    "        if idx not in self.questions.keys():\n",
    "            return False\n",
    "\n",
    "        del self.questions[idx]\n",
    "        \n",
    "        return True\n",
    "\n",
    "questions_list = [\n",
    "    \"Explain the concept of 'artificial intelligence' in a way that a 10-year-old could understand.\", \n",
    "    \"Compare and contrast the philosophies of Plato and Aristotle, highlighting their key differences and similarities in their views on ethics and knowledge.\", \n",
    "    \"Write a short poem about the feeling of walking through a forest in autumn.\",\n",
    "    \"If someone is planning a trip to Italy and enjoys art and history, what are three cities you would recommend they visit and why?\",\n",
    "    \"Summarize the main arguments for and against universal basic income.\"]\n",
    "\n",
    "question_dict = {}\n",
    "for (k,q) in enumerate(questions_list):\n",
    "    question = Question(question=q, index=k)\n",
    "    question_dict[k] = question\n",
    "\n",
    "deps = Deps(questions=question_dict)\n",
    "\n",
    "system_prompt=\"\"\"\n",
    "You are a helpful assistant.\n",
    "\n",
    "- First get all questions (use tools)\n",
    "- Then, choose only one question\n",
    "- Finally answer the one question\n",
    "\"\"\"\n",
    "\n",
    "agent = Agent(\n",
    "    model,\n",
    "    deps_type=deps,  \n",
    "    result_type=Answer,\n",
    "    system_prompt=system_prompt)\n",
    "\n",
    "@agent.tool\n",
    "def get_all_questions(ctx: RunContext[Deps]) -> dict:\n",
    "    return ctx.deps.get_all_questions()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_answered = []\n",
    "\n",
    "for k in range(5):\n",
    "    \n",
    "    result = await agent.run('Get all questions and give an answer to only one question.', deps=deps)\n",
    "    console.print(Markdown(result.data.answer))\n",
    "    logger.info(f\"LLM Output: {result.data.answer}\")\n",
    "\n",
    "    saved_answered.append(result.data)\n",
    "\n",
    "    if result.data.rating >= 8:\n",
    "        # remove question from deps\n",
    "        deps.remove_question(result.data.index)\n",
    "\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Search Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use google search to extract some useful links first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a search expert that has access to a search tool/function.\n",
    "Use the search tool multiple times (if necessary) to find relevant links that might be useful for a given user prompt or search query.\n",
    "You can add as many links to the output list as you like (but not more than 10).\n",
    "\"\"\"\n",
    "\n",
    "class SearchResponse(BaseModel):\n",
    "    links: list[str] = Field(description=\"A list with relevant links (collection of links).\")\n",
    "\n",
    "search_agent = Agent(\n",
    "    model,\n",
    "    result_type=SearchResponse,\n",
    "    system_prompt=system_prompt)\n",
    "\n",
    "\n",
    "@search_agent.tool_plain\n",
    "async def google_search(search_query: str) -> dict:\n",
    "    \"\"\"Use the Google Search API to find results given a search query.\"\"\"\n",
    "    return await google_general_search_async(search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = 'How suited is the H100 GPU from NVIDEA for mixture of experts LLMs/Transformer models?'\n",
    "result = await search_agent.run(search_query)\n",
    "rprint(result.data.links)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use web crawling to get the content of the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content_markdown = {}\n",
    "for link in result.data.links:\n",
    "    print(f\"Link: {link}\")\n",
    "    markdown = await crawl4ai_website_async(link)\n",
    "    page_content_markdown[link] = markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_markdown = \"\"\n",
    "\n",
    "for (k, link) in enumerate(page_content_markdown.keys()):\n",
    "    markdown = page_content_markdown[link]\n",
    "    combined_markdown += f\"From link ([{k+1}] {link}):\\n\\n{markdown}\\n\\n\"\n",
    "\n",
    "\n",
    "combined_markdown = f\"\"\" Here is the search query of the user: \n",
    "{search_query}\n",
    "\n",
    "Here is some content that might be useful to answer the user query:\n",
    "\n",
    "{combined_markdown}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response Agent or Summary Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an expert at writing professional technical writer (articles, blogs, books, etc.).\n",
    "\n",
    "After receiving a user query and some files, your goal is to write an report about the user query.\n",
    "This writen report should be technically detailed but comprehensive for normal readers.\n",
    "\n",
    "Please use references in the report (e.g. [1]). You can find the link of a given input text above the text with \"From link ([1] http ...)\".\n",
    "\n",
    "Always use References at the end of the report.\n",
    "  \n",
    "Write the output strictly in Markdown format. \n",
    "\"\"\"\n",
    "\n",
    "summary_agent = Agent(\n",
    "    model,\n",
    "    result_type=str,\n",
    "    system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "result = await summary_agent.run(combined_markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃                           <span style=\"font-weight: bold\">Suitability of NVIDIA H100 GPU for Mixture of Experts LLMs</span>                            ┃\n",
       "┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n",
       "\n",
       "This report assesses the suitability of the NVIDIA H100 GPU for running Mixture of Experts (MoE) Large Language    \n",
       "Models (LLMs), which are gaining popularity for their ability to scale model capacity, reduce training costs, and  \n",
       "decrease latency [1].                                                                                              \n",
       "\n",
       "\n",
       "                                  <span style=\"font-weight: bold; text-decoration: underline\">Understanding Mixture of Experts (MoE) in LLMs</span>                                   \n",
       "\n",
       "MoE models split the computational workload of a neural network layer into multiple \"expert\" subnetworks [1]. These\n",
       "subnetworks, which can be dense or sparse, operate independently, and their results are combined to produce the    \n",
       "output of the MoE layer. In sparse MoEs, only a subset of experts are activated for each input, reducing the       \n",
       "compute required per token [1]. This leads to greater efficiency, enabling larger models to be trained and deployed\n",
       "with the same compute and memory constraints as dense models.                                                      \n",
       "\n",
       "MoE models are particularly attractive because they can [1]:                                                       \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Increase Model Capacity:</span> By adding more parameters through multiple expert subnetworks without a proportional   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>increase in computation.                                                                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Reduce Training Costs:</span> Sparse MoEs use fewer parameters per token, reducing the computational cost of training. \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Decrease Latency:</span> By only activating a subset of experts, latency can be significantly decreased which is       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>crucial for real-time applications.                                                                             \n",
       "\n",
       "\n",
       "                                   <span style=\"font-weight: bold; text-decoration: underline\">NVIDIA H100 GPU: A High-Performance Solution</span>                                    \n",
       "\n",
       "The NVIDIA H100 Tensor Core GPU is designed for high-performance computing, AI, and deep learning workloads [2].   \n",
       "Key features that make it well-suited for MoE LLMs include:                                                        \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Hopper Architecture:</span> The H100 is based on the NVIDIA Hopper architecture, which includes fourth-generation      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Tensor Cores and a Transformer Engine [2, 3]. These cores are designed to efficiently handle the matrix         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>multiplications and other operations central to LLMs.                                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">FP8 Precision:</span> The H100 supports FP8 precision, which doubles the computational throughput compared to FP16 or  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>BF16 [8, 2]. This is crucial for accelerating both training and inference in MoE models. FP8 also reduces memory\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>footprint, enabling larger batches to be processed [8].                                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">High Memory Bandwidth:</span> The H100 has a very high memory bandwidth (up to 3.9 TB/s), which is essential for       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>loading model parameters and processing large datasets efficiently [2, 8]. This is crucial for handling the     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>large number of parameters in MoE models.                                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">NVLink and NVSwitch:</span> The H100 supports NVLink and NVSwitch, enabling fast GPU-to-GPU communication for large    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>models across multiple GPUs in a node [2]. This is essential when training large models.                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Multi-Instance GPU (MIG):</span> The H100 supports MIG, allowing partitioning of the GPU resources for better          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>utilization [2]. This is particularly useful for handling smaller workloads or for optimizing resource          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>allocation in multi-user environments.                                                                          \n",
       "\n",
       "\n",
       "                                           <span style=\"font-weight: bold; text-decoration: underline\">How H100 Accelerates MoE LLMs</span>                                           \n",
       "\n",
       "The H100's architecture is particularly suited to accelerating MoE LLMs in several ways:                           \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Transformer Engine:</span> This dedicated engine significantly speeds up transformer calculations which are the        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>building blocks of LLMs and MoE models [3].                                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">FP8 Support:</span> The support for FP8 computations drastically increases processing throughput without compromising  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>model accuracy [6, 8, 2].                                                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Expert Parallelism:</span> The architecture facilitates distributing experts across multiple GPUs and servers via      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>NVLink allowing for efficient scaling of MoE models [8, 4]. This helps in balancing the computational workload  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>and reducing inference latency.                                                                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">High Throughput:</span> H100 GPUs, paired with TensorRT-LLM can reach very high throughput with low latency [8].       \n",
       "\n",
       "\n",
       "                                              <span style=\"font-weight: bold; text-decoration: underline\">Performance Advantages</span>                                               \n",
       "\n",
       "Recent studies and benchmarks illustrate the H100's capabilities with MoE models like Mixtral 8x7B [8]:            \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Increased Throughput:</span>  H100 GPUs, combined with NVIDIA TensorRT-LLM software, significantly boosts throughput,  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>processing more requests per second, especially when employing FP8 precision [8].                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Reduced Latency:</span> The architecture significantly reduces both end-to-end and per-token latencies. This is crucial\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>for interactive applications and real-time inference [1].                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Efficient Batch Processing:</span> The H100 GPU can process larger batches with decreased latency, which is essential  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>to optimize throughput [8].                                                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Improved Scalability</span> : With NVLink and NVSwitch, H100 GPUs can be scaled in multi-GPU and multi-node setups [2].\n",
       "\n",
       "\n",
       "                                            <span style=\"font-weight: bold; text-decoration: underline\">TensorRT-LLM Optimizations</span>                                             \n",
       "\n",
       "NVIDIA's TensorRT-LLM plays a critical role in optimizing the performance of LLMs, particularly MoE models [8]. It \n",
       "offers a range of optimizations including [8]:                                                                     \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Optimized Attention Kernels:</span> Faster and more memory efficient operations.                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">KV Caching:</span> Reduces redundant computations by efficiently managing key-value pairs.                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">FP8 Quantization:</span> Further reduces memory requirements and increases computational throughput.                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Expert Parallelism (EP):</span> Allows the distribution of experts across GPUs, improving parallelism [8].             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Hybrid Parallelism</span>: Optimized hybrid of expert and tensor parallelism [8].                                      \n",
       "\n",
       "\n",
       "                                      <span style=\"font-weight: bold; text-decoration: underline\">Experimental Results with Mixtral 8x7B</span>                                       \n",
       "\n",
       "Experimental results with the Mixtral 8x7B model on H100 GPUs and TensorRT-LLM show significant performance gains  \n",
       "[8]:                                                                                                               \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Throughput vs. Latency:</span> A pair of H100 GPUs using FP8 precision achieves nearly 50% higher throughput than FP16 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>for the same response time.                                                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Throughput vs. Output Token Time:</span>  At 0.016 seconds per output token, a two H100 setup can achieve 38.4 requests\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>per second using FP8 precision                                                                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Offline Throughput</span>: When not constrained by latency, a two H100 GPU setup can process 21,000 tokens per second  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>with FP8 when batch sizes are 1024.                                                                             \n",
       "\n",
       "These results showcase how effectively the H100 can process MoE models while maintaining low latency.              \n",
       "\n",
       "\n",
       "                                        <span style=\"font-weight: bold; text-decoration: underline\">Considerations for H100 Deployment</span>                                         \n",
       "\n",
       "While the H100 is highly suitable for MoE LLMs, a few points must be considered:                                   \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Cost:</span> H100 GPUs are more expensive than previous generation GPUs [2]. However, the performance gains can offset \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>this by providing a lower TCO (Total Cost of Ownership) through higher throughput and reduced inference latency.\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Scalability:</span> The H100 can be deployed in single- or multi-GPU configurations.                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Software Stack:</span> The software stack, including TensorRT-LLM, must be properly configured to take full advantage  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>of the H100's capabilities [8].                                                                                 \n",
       "\n",
       "\n",
       "                                                    <span style=\"font-weight: bold; text-decoration: underline\">Conclusion</span>                                                     \n",
       "\n",
       "The NVIDIA H100 Tensor Core GPU is exceptionally well-suited for handling the demanding workloads of MoE LLMs,     \n",
       "offering significant benefits in terms of throughput, latency, and scalability [2, 3, 8]. Its support for FP8      \n",
       "precision, along with its specialized hardware and highly optimized software like TensorRT-LLM, enables these      \n",
       "models to perform exceptionally well while maintaining accuracy [8]. For organizations looking to deploy MoE LLMs, \n",
       "the H100 is currently a compelling choice. Further advancements to the Blackwell architecture will only improve    \n",
       "these metrics.                                                                                                     \n",
       "\n",
       "\n",
       "                                                    <span style=\"font-weight: bold; text-decoration: underline\">References</span>                                                     \n",
       "\n",
       "[1]  K. Kranen and V. Nguyen, “Applying Mixture of Experts in LLM Architectures,” NVIDIA Technical Blog, 2024.     \n",
       "[Online]. Available: https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/           \n",
       "\n",
       "[2]  NVIDIA, “NVIDIA H100 Tensor Core GPU,” [Online]. Available: https://www.nvidia.com/en-us/data-center/h100/    \n",
       "\n",
       "[3]  D. Salvator, “H100 Transformer Engine Supercharges AI Training, Delivering Up to 6x Higher Performance Without\n",
       "Losing Accuracy,” NVIDIA Technical Blog, 2022. [Online]. Available:                                                \n",
       "https://blogs.nvidia.com/blog/h100-transformer-engine/                                                             \n",
       "\n",
       "[4] NVIDIA, “Megatron-LM”, [Online]. Available: https://github.com/NVIDIA/Megatron-LM                              \n",
       "\n",
       "[5]  Reddit, “Why choose an H100 over an A100 for LLM inference?”, [Online]. Available:                            \n",
       "https://www.reddit.com/r/MachineLearning/comments/17hsjdt/d_why_choose_an_h100_over_an_a100_for_llm/               \n",
       "\n",
       "[6]  M. Vidrih, “Why Are Transformer Models Trending?,” 2024. [Online]. Available:                                 \n",
       "https://vidrihmarko.medium.com/why-are-transformer-models-trending-189bfee5e3f1                                    \n",
       "\n",
       "[7] S. Mukherjee, “Accelerating Large Language Models: The H100 GPU’s Role in Advanced AI Development”, Paperspace \n",
       "Blog, 2024. [Online]. Available: https://blog.paperspace.com/h100-deep-learning-frameworks-compatibility/          \n",
       "\n",
       "[8] A. Eassa, N. Comly, W. Hill and G. Ho, “Achieving High Mixtral 8x7B Performance with NVIDIA H100 Tensor Core   \n",
       "GPUs and NVIDIA TensorRT-LLM”, NVIDIA Technical Blog, 2024. [Online]. Available:                                   \n",
       "https://developer.nvidia.com/blog/achieving-high-mixtral-8x7b-performance-with-nvidia-h100-tensor-core-gpus-and-ten\n",
       "sorrt-llm/                                                                                                         \n",
       "\n",
       "[9] S. Yun, K. Kyung, J. Cho, J. Kim, B. Kim, S. Lee, K. Sohn, and J. H. Ahn, “Duplex: A Device for Large Language \n",
       "Models with Mixture of Experts, Grouped Query Attention, and Continuous Batching.” arXiv preprint arXiv:2409.01141,\n",
       "2024. [Online]. Available: https://arxiv.org/html/2409.01141v1                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃                           \u001b[1mSuitability of NVIDIA H100 GPU for Mixture of Experts LLMs\u001b[0m                            ┃\n",
       "┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n",
       "\n",
       "This report assesses the suitability of the NVIDIA H100 GPU for running Mixture of Experts (MoE) Large Language    \n",
       "Models (LLMs), which are gaining popularity for their ability to scale model capacity, reduce training costs, and  \n",
       "decrease latency [1].                                                                                              \n",
       "\n",
       "\n",
       "                                  \u001b[1;4mUnderstanding Mixture of Experts (MoE) in LLMs\u001b[0m                                   \n",
       "\n",
       "MoE models split the computational workload of a neural network layer into multiple \"expert\" subnetworks [1]. These\n",
       "subnetworks, which can be dense or sparse, operate independently, and their results are combined to produce the    \n",
       "output of the MoE layer. In sparse MoEs, only a subset of experts are activated for each input, reducing the       \n",
       "compute required per token [1]. This leads to greater efficiency, enabling larger models to be trained and deployed\n",
       "with the same compute and memory constraints as dense models.                                                      \n",
       "\n",
       "MoE models are particularly attractive because they can [1]:                                                       \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mIncrease Model Capacity:\u001b[0m By adding more parameters through multiple expert subnetworks without a proportional   \n",
       "\u001b[1;33m   \u001b[0mincrease in computation.                                                                                        \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mReduce Training Costs:\u001b[0m Sparse MoEs use fewer parameters per token, reducing the computational cost of training. \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mDecrease Latency:\u001b[0m By only activating a subset of experts, latency can be significantly decreased which is       \n",
       "\u001b[1;33m   \u001b[0mcrucial for real-time applications.                                                                             \n",
       "\n",
       "\n",
       "                                   \u001b[1;4mNVIDIA H100 GPU: A High-Performance Solution\u001b[0m                                    \n",
       "\n",
       "The NVIDIA H100 Tensor Core GPU is designed for high-performance computing, AI, and deep learning workloads [2].   \n",
       "Key features that make it well-suited for MoE LLMs include:                                                        \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mHopper Architecture:\u001b[0m The H100 is based on the NVIDIA Hopper architecture, which includes fourth-generation      \n",
       "\u001b[1;33m   \u001b[0mTensor Cores and a Transformer Engine [2, 3]. These cores are designed to efficiently handle the matrix         \n",
       "\u001b[1;33m   \u001b[0mmultiplications and other operations central to LLMs.                                                           \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mFP8 Precision:\u001b[0m The H100 supports FP8 precision, which doubles the computational throughput compared to FP16 or  \n",
       "\u001b[1;33m   \u001b[0mBF16 [8, 2]. This is crucial for accelerating both training and inference in MoE models. FP8 also reduces memory\n",
       "\u001b[1;33m   \u001b[0mfootprint, enabling larger batches to be processed [8].                                                         \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mHigh Memory Bandwidth:\u001b[0m The H100 has a very high memory bandwidth (up to 3.9 TB/s), which is essential for       \n",
       "\u001b[1;33m   \u001b[0mloading model parameters and processing large datasets efficiently [2, 8]. This is crucial for handling the     \n",
       "\u001b[1;33m   \u001b[0mlarge number of parameters in MoE models.                                                                       \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mNVLink and NVSwitch:\u001b[0m The H100 supports NVLink and NVSwitch, enabling fast GPU-to-GPU communication for large    \n",
       "\u001b[1;33m   \u001b[0mmodels across multiple GPUs in a node [2]. This is essential when training large models.                        \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mMulti-Instance GPU (MIG):\u001b[0m The H100 supports MIG, allowing partitioning of the GPU resources for better          \n",
       "\u001b[1;33m   \u001b[0mutilization [2]. This is particularly useful for handling smaller workloads or for optimizing resource          \n",
       "\u001b[1;33m   \u001b[0mallocation in multi-user environments.                                                                          \n",
       "\n",
       "\n",
       "                                           \u001b[1;4mHow H100 Accelerates MoE LLMs\u001b[0m                                           \n",
       "\n",
       "The H100's architecture is particularly suited to accelerating MoE LLMs in several ways:                           \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mTransformer Engine:\u001b[0m This dedicated engine significantly speeds up transformer calculations which are the        \n",
       "\u001b[1;33m   \u001b[0mbuilding blocks of LLMs and MoE models [3].                                                                     \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mFP8 Support:\u001b[0m The support for FP8 computations drastically increases processing throughput without compromising  \n",
       "\u001b[1;33m   \u001b[0mmodel accuracy [6, 8, 2].                                                                                       \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mExpert Parallelism:\u001b[0m The architecture facilitates distributing experts across multiple GPUs and servers via      \n",
       "\u001b[1;33m   \u001b[0mNVLink allowing for efficient scaling of MoE models [8, 4]. This helps in balancing the computational workload  \n",
       "\u001b[1;33m   \u001b[0mand reducing inference latency.                                                                                 \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mHigh Throughput:\u001b[0m H100 GPUs, paired with TensorRT-LLM can reach very high throughput with low latency [8].       \n",
       "\n",
       "\n",
       "                                              \u001b[1;4mPerformance Advantages\u001b[0m                                               \n",
       "\n",
       "Recent studies and benchmarks illustrate the H100's capabilities with MoE models like Mixtral 8x7B [8]:            \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mIncreased Throughput:\u001b[0m  H100 GPUs, combined with NVIDIA TensorRT-LLM software, significantly boosts throughput,  \n",
       "\u001b[1;33m   \u001b[0mprocessing more requests per second, especially when employing FP8 precision [8].                               \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mReduced Latency:\u001b[0m The architecture significantly reduces both end-to-end and per-token latencies. This is crucial\n",
       "\u001b[1;33m   \u001b[0mfor interactive applications and real-time inference [1].                                                       \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mEfficient Batch Processing:\u001b[0m The H100 GPU can process larger batches with decreased latency, which is essential  \n",
       "\u001b[1;33m   \u001b[0mto optimize throughput [8].                                                                                     \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mImproved Scalability\u001b[0m : With NVLink and NVSwitch, H100 GPUs can be scaled in multi-GPU and multi-node setups [2].\n",
       "\n",
       "\n",
       "                                            \u001b[1;4mTensorRT-LLM Optimizations\u001b[0m                                             \n",
       "\n",
       "NVIDIA's TensorRT-LLM plays a critical role in optimizing the performance of LLMs, particularly MoE models [8]. It \n",
       "offers a range of optimizations including [8]:                                                                     \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mOptimized Attention Kernels:\u001b[0m Faster and more memory efficient operations.                                       \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mKV Caching:\u001b[0m Reduces redundant computations by efficiently managing key-value pairs.                             \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mFP8 Quantization:\u001b[0m Further reduces memory requirements and increases computational throughput.                   \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mExpert Parallelism (EP):\u001b[0m Allows the distribution of experts across GPUs, improving parallelism [8].             \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mHybrid Parallelism\u001b[0m: Optimized hybrid of expert and tensor parallelism [8].                                      \n",
       "\n",
       "\n",
       "                                      \u001b[1;4mExperimental Results with Mixtral 8x7B\u001b[0m                                       \n",
       "\n",
       "Experimental results with the Mixtral 8x7B model on H100 GPUs and TensorRT-LLM show significant performance gains  \n",
       "[8]:                                                                                                               \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mThroughput vs. Latency:\u001b[0m A pair of H100 GPUs using FP8 precision achieves nearly 50% higher throughput than FP16 \n",
       "\u001b[1;33m   \u001b[0mfor the same response time.                                                                                     \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mThroughput vs. Output Token Time:\u001b[0m  At 0.016 seconds per output token, a two H100 setup can achieve 38.4 requests\n",
       "\u001b[1;33m   \u001b[0mper second using FP8 precision                                                                                  \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mOffline Throughput\u001b[0m: When not constrained by latency, a two H100 GPU setup can process 21,000 tokens per second  \n",
       "\u001b[1;33m   \u001b[0mwith FP8 when batch sizes are 1024.                                                                             \n",
       "\n",
       "These results showcase how effectively the H100 can process MoE models while maintaining low latency.              \n",
       "\n",
       "\n",
       "                                        \u001b[1;4mConsiderations for H100 Deployment\u001b[0m                                         \n",
       "\n",
       "While the H100 is highly suitable for MoE LLMs, a few points must be considered:                                   \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mCost:\u001b[0m H100 GPUs are more expensive than previous generation GPUs [2]. However, the performance gains can offset \n",
       "\u001b[1;33m   \u001b[0mthis by providing a lower TCO (Total Cost of Ownership) through higher throughput and reduced inference latency.\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mScalability:\u001b[0m The H100 can be deployed in single- or multi-GPU configurations.                                   \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mSoftware Stack:\u001b[0m The software stack, including TensorRT-LLM, must be properly configured to take full advantage  \n",
       "\u001b[1;33m   \u001b[0mof the H100's capabilities [8].                                                                                 \n",
       "\n",
       "\n",
       "                                                    \u001b[1;4mConclusion\u001b[0m                                                     \n",
       "\n",
       "The NVIDIA H100 Tensor Core GPU is exceptionally well-suited for handling the demanding workloads of MoE LLMs,     \n",
       "offering significant benefits in terms of throughput, latency, and scalability [2, 3, 8]. Its support for FP8      \n",
       "precision, along with its specialized hardware and highly optimized software like TensorRT-LLM, enables these      \n",
       "models to perform exceptionally well while maintaining accuracy [8]. For organizations looking to deploy MoE LLMs, \n",
       "the H100 is currently a compelling choice. Further advancements to the Blackwell architecture will only improve    \n",
       "these metrics.                                                                                                     \n",
       "\n",
       "\n",
       "                                                    \u001b[1;4mReferences\u001b[0m                                                     \n",
       "\n",
       "[1]  K. Kranen and V. Nguyen, “Applying Mixture of Experts in LLM Architectures,” NVIDIA Technical Blog, 2024.     \n",
       "[Online]. Available: https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/           \n",
       "\n",
       "[2]  NVIDIA, “NVIDIA H100 Tensor Core GPU,” [Online]. Available: https://www.nvidia.com/en-us/data-center/h100/    \n",
       "\n",
       "[3]  D. Salvator, “H100 Transformer Engine Supercharges AI Training, Delivering Up to 6x Higher Performance Without\n",
       "Losing Accuracy,” NVIDIA Technical Blog, 2022. [Online]. Available:                                                \n",
       "https://blogs.nvidia.com/blog/h100-transformer-engine/                                                             \n",
       "\n",
       "[4] NVIDIA, “Megatron-LM”, [Online]. Available: https://github.com/NVIDIA/Megatron-LM                              \n",
       "\n",
       "[5]  Reddit, “Why choose an H100 over an A100 for LLM inference?”, [Online]. Available:                            \n",
       "https://www.reddit.com/r/MachineLearning/comments/17hsjdt/d_why_choose_an_h100_over_an_a100_for_llm/               \n",
       "\n",
       "[6]  M. Vidrih, “Why Are Transformer Models Trending?,” 2024. [Online]. Available:                                 \n",
       "https://vidrihmarko.medium.com/why-are-transformer-models-trending-189bfee5e3f1                                    \n",
       "\n",
       "[7] S. Mukherjee, “Accelerating Large Language Models: The H100 GPU’s Role in Advanced AI Development”, Paperspace \n",
       "Blog, 2024. [Online]. Available: https://blog.paperspace.com/h100-deep-learning-frameworks-compatibility/          \n",
       "\n",
       "[8] A. Eassa, N. Comly, W. Hill and G. Ho, “Achieving High Mixtral 8x7B Performance with NVIDIA H100 Tensor Core   \n",
       "GPUs and NVIDIA TensorRT-LLM”, NVIDIA Technical Blog, 2024. [Online]. Available:                                   \n",
       "https://developer.nvidia.com/blog/achieving-high-mixtral-8x7b-performance-with-nvidia-h100-tensor-core-gpus-and-ten\n",
       "sorrt-llm/                                                                                                         \n",
       "\n",
       "[9] S. Yun, K. Kyung, J. Cho, J. Kim, B. Kim, S. Lee, K. Sohn, and J. H. Ahn, “Duplex: A Device for Large Language \n",
       "Models with Mixture of Experts, Grouped Query Attention, and Continuous Batching.” arXiv preprint arXiv:2409.01141,\n",
       "2024. [Online]. Available: https://arxiv.org/html/2409.01141v1                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(Markdown(result.data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
