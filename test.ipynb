{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import controlflow as cf\n",
    "\n",
    "from agent_utils import *\n",
    "from agent_tools import *\n",
    "\n",
    "from extensive_search import run_research\n",
    "from rich import print as pprint\n",
    "FLASH2_MODEL = \"google/\" + config.FLASH2_MODEL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "We need an detailed and extensive overview of existing systems for running pretrained model (e.g., transformer models) up to 100 million parameters (inference only).\n",
    "To accomplish this, we plan to do a deep research by processing many webpages, PDFs, acticles etc. using LLM APIs (e.g., Google Gemini, GPT4o etc.).\n",
    "\n",
    "We use an existing Python agent framework (e.g., PydanticAI, LangChain, Controlflow) to build the system.\n",
    "\n",
    "We have access to different types of search tools:\n",
    "- Google Search API (general search with time range search)\n",
    "- Google Schoolar API (search papers)\n",
    "- Google News API (search news articles)\n",
    "- Could be expanded in the future.\n",
    "\n",
    "For this task, we plan to build our own deep research agent system that uses LLMs to process a workflow.\n",
    "\n",
    "The workflow has these basic working steps or tasks:\n",
    "- Based on a search query (user input) generate a research plan/instruction and suitable search queries for the search tools.\n",
    "- Use search APIs for generated search queries to collect information.\n",
    "- Use a reasoning model (LLM + thinking) to select suitable URLs to extract information from.\n",
    "- Use web crawlers to comvert HTML or PDF content to markdown text for the LLMs.\n",
    "- For the first markdown document use an LLM to rate the relevance and quality of the document.\n",
    "- For the first document also extract a summary of all relevant and important information.\n",
    "- Save the document with rating and contectualized summary.\n",
    "- For the second markdown document use the contectualized summary from the previous document, to genrate a rating and a new contextualized summary.\n",
    "- Repeat the earlier steps for each downloaded document.\n",
    "- Stop when a cerain document budget is reached.\n",
    "- Use the original user query and the compressed/accumulated summary to create a report based on the generated research plan/instruction.\n",
    "\n",
    "What do you think about this approach? How would you improve it?\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Here are suitable search queries for Google Search, Google Scholar API, and Google News API to find information on \n",
       "systems for running pretrained models (up to 100 million parameters, inference only).                              \n",
       "\n",
       "<span style=\"font-weight: bold\">Google Search:</span>                                                                                                     \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 1: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"inference systems\" \"pretrained models\" \"transformer models\" \"up to 100 million parameters\"</span>            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query directly targets systems designed for inference of pretrained transformer models within\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>the specified parameter range. It uses exact phrases for better precision.                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 2: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"model serving frameworks\" \"transformer inference\" \"small language models\"</span>                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span>  This query focuses on frameworks specifically for serving models (inference). It uses            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>\"transformer inference\" and \"small language models\" as a proxy for models up to 100 million parameters, as   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>these models are often considered relatively small in the current LLM landscape.                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 3: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"open source inference libraries\" \"transformer models\" \"performance benchmark\"</span>                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query seeks open-source libraries that are commonly used for transformer inference and looks \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>for performance benchmarks, which often indicate suitability for different model sizes and resource          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>constraints.                                                                                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 4: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"cloud inference platforms\" \"pretrained transformer deployment\"</span>                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query targets cloud-based platforms designed for deploying and running pretrained            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>transformers for inference. It's relevant as cloud platforms are a common solution for model serving.        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 5: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"edge inference solutions\" \"transformer models\" \"resource constrained devices\"</span>                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query explores solutions for running transformer inference on edge devices or                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>resource-constrained environments, which is often necessary for smaller models and real-time applications.   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 6: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"hardware acceleration\" \"transformer inference\" \"GPU\" \"CPU\" \"optimized\"</span>                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query focuses on hardware acceleration techniques (using GPUs, CPUs, or other specialized    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>hardware) to optimize transformer inference performance, a crucial aspect for efficient systems.             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 7: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"real-time inference\" \"transformer models\" \"low latency\"</span>                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query targets systems designed for real-time inference with low latency, a critical          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>requirement for many applications using pretrained models.                                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 8: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"lightweight transformer inference\" \"mobile devices\" \"embedded systems\"</span>                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query specifically aims for lightweight solutions suitable for mobile devices and embedded   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>systems, which often involve smaller models and require efficient inference.                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 9: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"serverless inference\" \"transformer models\" \"auto-scaling\"</span>                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query explores serverless inference options, which offer scalability and cost-efficiency for \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>model serving, and are often used with pretrained models.                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 10: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"commercial inference servers\" \"transformer models\" \"enterprise solutions\"</span>                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query investigates commercial products and enterprise-grade solutions for transformer        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>inference, highlighting systems used in professional settings.                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 11: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"best practices\" \"transformer model inference\" \"deployment guide\"</span>                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query aims to find best practices and deployment guides related to transformer model         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>inference, which can provide insights into existing and recommended systems.                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 12: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"state-of-the-art\" \"transformer inference systems\" \"performance comparison\"</span>                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query seeks information on state-of-the-art systems and performance comparisons, helping to  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>identify leading and efficient solutions.                                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 13: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"inference as a service\" \"transformer models\" \"API\"</span>                                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query targets \"Inference as a Service\" offerings, where inference capabilities are provided  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>as an API, a popular approach for using pretrained models without managing infrastructure directly.          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 14: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"model optimization techniques\" \"inference speed\" \"transformer models\"</span>                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> While not directly a system, understanding model optimization techniques (quantization, pruning,  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>distillation) is crucial for making models run efficiently on various systems.                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 15: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"benchmark results\" \"transformer inference frameworks\" \"latency throughput\"</span>                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query specifically looks for benchmark results of different inference frameworks, which can  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>provide data-driven comparisons and insights into system performance.                                        \n",
       "\n",
       "<span style=\"font-weight: bold\">Google Scholar API:</span>                                                                                                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 1: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"survey\" \"inference systems\" \"pretrained transformer models\"</span>                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span>  Searching for survey papers provides comprehensive overviews of existing systems and research    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>trends in transformer inference.                                                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 2: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"efficient inference\" \"transformer models\" \"system design\"</span>                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query targets research papers focused on designing efficient inference systems for           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>transformer models, often discussing novel architectures and optimization techniques.                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 3: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"model serving framework\" \"performance evaluation\" \"transformer inference\"</span>                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query seeks papers that evaluate the performance of different model serving frameworks       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>specifically for transformer inference, providing comparative analysis.                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 4: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"hardware acceleration\" \"transformer inference\" \"FPGA\" \"ASIC\" \"GPU\"</span>                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query focuses on academic research into hardware acceleration methods for transformer        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>inference, including specialized hardware like FPGAs and ASICs, as well as GPUs.                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 5: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"low latency inference\" \"transformer models\" \"optimization techniques\"</span>                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query targets papers exploring optimization techniques to achieve low latency inference for  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>transformer models, a key performance metric.                                                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 6: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"edge computing\" \"transformer inference\" \"resource constraints\"</span>                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query seeks academic work on running transformer inference in edge computing environments    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>with limited resources.                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 7: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"quantization\" \"pruning\" \"distillation\" \"transformer inference\"</span>                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query targets research papers on model optimization techniques like quantization, pruning,   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>and distillation, which are vital for efficient inference.                                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 8: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"system architecture\" \"transformer inference\" \"scalable\"</span>                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query focuses on papers discussing the system architecture of scalable transformer inference \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>systems, addressing aspects like distribution and concurrency.                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 9: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"benchmark dataset\" \"transformer inference performance\"</span>                                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query seeks papers that introduce benchmark datasets or evaluate transformer inference       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>performance using standard datasets, providing a basis for comparison.                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 10: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"comparative study\" \"inference frameworks\" \"transformer models\"</span>                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query aims to find comparative studies that analyze and compare different inference          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>frameworks for transformer models, offering insights into their strengths and weaknesses.                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 11: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"distributed inference\" \"transformer models\" \"parallelism\"</span>                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query explores research on distributed inference techniques for transformer models, focusing \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>on parallelism to improve performance.                                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 12: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"real-time systems\" \"transformer inference\" \"latency analysis\"</span>                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query targets academic work related to real-time systems for transformer inference, including\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>latency analysis and performance guarantees.                                                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 13: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"adaptive inference\" \"transformer models\" \"resource management\"</span>                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query seeks papers discussing adaptive inference systems that can dynamically adjust resource\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>allocation based on demand and model characteristics.                                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 14: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"transformer inference engine\" \"design and implementation\"</span>                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query aims to find papers detailing the design and implementation of specific transformer    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>inference engines or systems.                                                                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 15: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"case study\" \"transformer inference deployment\" \"industry application\"</span>                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query seeks case studies of real-world deployments of transformer inference systems in       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>various industry applications, providing practical examples.                                                 \n",
       "\n",
       "<span style=\"font-weight: bold\">Google News API:</span>                                                                                                   \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 1: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"new inference platform\" \"transformer models\" \"released\"</span>                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query aims to find news articles announcing the release of new inference platforms           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>specifically designed for transformer models.                                                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 2: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"transformer inference service\" \"launched\" \"cloud provider\"</span>                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query targets news about cloud providers launching new inference services for transformer    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>models.                                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 3: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"edge inference solution\" \"transformer models\" \"announced\"</span>                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query seeks news articles announcing new edge inference solutions for transformer models.    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 4: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"faster transformer inference\" \"technology breakthrough\"</span>                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query looks for news about technological breakthroughs or advancements that lead to faster   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>transformer inference.                                                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 5: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"company announces\" \"transformer inference hardware\"</span>                                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query targets news about companies announcing new hardware specifically designed for         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>transformer inference acceleration.                                                                          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 6: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"open source inference library\" \"transformer models\" \"update\"</span>                                          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query seeks news about updates or new releases of open-source inference libraries for        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>transformer models.                                                                                          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 7: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"benchmark results\" \"transformer inference platforms\" \"comparison\"</span>                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query aims to find news articles reporting on benchmark results and comparisons of different \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>transformer inference platforms.                                                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 8: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"AI inference chip\" \"transformer models\" \"market release\"</span>                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query targets news about the market release of new AI inference chips that are suitable for  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>running transformer models.                                                                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 9: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"industry trends\" \"transformer inference\" \"adoption\"</span>                                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query seeks news articles discussing industry trends and the adoption of transformer         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>inference technologies.                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 10: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"startup raises funding\" \"transformer inference technology\"</span>                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query looks for news about startups in the transformer inference technology space that have  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>raised funding, indicating areas of active development and investment.                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 11: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"partnership\" \"cloud company\" \"inference platform\" \"transformer models\"</span>                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query targets news about partnerships between cloud companies and inference platform         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>providers for transformer models.                                                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 12: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"security vulnerability\" \"inference system\" \"transformer models\"</span>                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> While less positive, news about security vulnerabilities in inference systems is also relevant to \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>understand the landscape and challenges.                                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 13: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"acquisition\" \"inference technology company\" \"transformer models\"</span>                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query seeks news about acquisitions in the inference technology space related to transformer \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>models, indicating market consolidation and interest.                                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 14: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"conference announcement\" \"transformer inference research\"</span>                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query targets news from conferences or industry events announcing new research or            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>developments in transformer inference.                                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>query 15: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">\"product review\" \"transformer inference server\" \"performance tested\"</span>                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-style: italic\">Rationale:</span> This query seeks product reviews of transformer inference servers or platforms, often including   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>performance testing and real-world usage reports.                                                            \n",
       "\n",
       "These queries are designed to be comprehensive and cover various aspects of systems for running pretrained         \n",
       "transformer models for inference, utilizing the strengths of each search tool. Remember to adjust time ranges      \n",
       "within each API based on your desired recency of information.                                                      \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Here are suitable search queries for Google Search, Google Scholar API, and Google News API to find information on \n",
       "systems for running pretrained models (up to 100 million parameters, inference only).                              \n",
       "\n",
       "\u001b[1mGoogle Search:\u001b[0m                                                                                                     \n",
       "\n",
       "\u001b[1;33m • \u001b[0mquery 1: \u001b[1;36;40m\"inference systems\" \"pretrained models\" \"transformer models\" \"up to 100 million parameters\"\u001b[0m            \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query directly targets systems designed for inference of pretrained transformer models within\n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mthe specified parameter range. It uses exact phrases for better precision.                                   \n",
       "\u001b[1;33m • \u001b[0mquery 2: \u001b[1;36;40m\"model serving frameworks\" \"transformer inference\" \"small language models\"\u001b[0m                             \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m  This query focuses on frameworks specifically for serving models (inference). It uses            \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\"transformer inference\" and \"small language models\" as a proxy for models up to 100 million parameters, as   \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mthese models are often considered relatively small in the current LLM landscape.                             \n",
       "\u001b[1;33m • \u001b[0mquery 3: \u001b[1;36;40m\"open source inference libraries\" \"transformer models\" \"performance benchmark\"\u001b[0m                         \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query seeks open-source libraries that are commonly used for transformer inference and looks \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mfor performance benchmarks, which often indicate suitability for different model sizes and resource          \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mconstraints.                                                                                                 \n",
       "\u001b[1;33m • \u001b[0mquery 4: \u001b[1;36;40m\"cloud inference platforms\" \"pretrained transformer deployment\"\u001b[0m                                        \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query targets cloud-based platforms designed for deploying and running pretrained            \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mtransformers for inference. It's relevant as cloud platforms are a common solution for model serving.        \n",
       "\u001b[1;33m • \u001b[0mquery 5: \u001b[1;36;40m\"edge inference solutions\" \"transformer models\" \"resource constrained devices\"\u001b[0m                         \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query explores solutions for running transformer inference on edge devices or                \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mresource-constrained environments, which is often necessary for smaller models and real-time applications.   \n",
       "\u001b[1;33m • \u001b[0mquery 6: \u001b[1;36;40m\"hardware acceleration\" \"transformer inference\" \"GPU\" \"CPU\" \"optimized\"\u001b[0m                                \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query focuses on hardware acceleration techniques (using GPUs, CPUs, or other specialized    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mhardware) to optimize transformer inference performance, a crucial aspect for efficient systems.             \n",
       "\u001b[1;33m • \u001b[0mquery 7: \u001b[1;36;40m\"real-time inference\" \"transformer models\" \"low latency\"\u001b[0m                                               \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query targets systems designed for real-time inference with low latency, a critical          \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mrequirement for many applications using pretrained models.                                                   \n",
       "\u001b[1;33m • \u001b[0mquery 8: \u001b[1;36;40m\"lightweight transformer inference\" \"mobile devices\" \"embedded systems\"\u001b[0m                                \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query specifically aims for lightweight solutions suitable for mobile devices and embedded   \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0msystems, which often involve smaller models and require efficient inference.                                 \n",
       "\u001b[1;33m • \u001b[0mquery 9: \u001b[1;36;40m\"serverless inference\" \"transformer models\" \"auto-scaling\"\u001b[0m                                             \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query explores serverless inference options, which offer scalability and cost-efficiency for \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mmodel serving, and are often used with pretrained models.                                                    \n",
       "\u001b[1;33m • \u001b[0mquery 10: \u001b[1;36;40m\"commercial inference servers\" \"transformer models\" \"enterprise solutions\"\u001b[0m                            \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query investigates commercial products and enterprise-grade solutions for transformer        \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0minference, highlighting systems used in professional settings.                                               \n",
       "\u001b[1;33m • \u001b[0mquery 11: \u001b[1;36;40m\"best practices\" \"transformer model inference\" \"deployment guide\"\u001b[0m                                     \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query aims to find best practices and deployment guides related to transformer model         \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0minference, which can provide insights into existing and recommended systems.                                 \n",
       "\u001b[1;33m • \u001b[0mquery 12: \u001b[1;36;40m\"state-of-the-art\" \"transformer inference systems\" \"performance comparison\"\u001b[0m                           \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query seeks information on state-of-the-art systems and performance comparisons, helping to  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0midentify leading and efficient solutions.                                                                    \n",
       "\u001b[1;33m • \u001b[0mquery 13: \u001b[1;36;40m\"inference as a service\" \"transformer models\" \"API\"\u001b[0m                                                   \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query targets \"Inference as a Service\" offerings, where inference capabilities are provided  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mas an API, a popular approach for using pretrained models without managing infrastructure directly.          \n",
       "\u001b[1;33m • \u001b[0mquery 14: \u001b[1;36;40m\"model optimization techniques\" \"inference speed\" \"transformer models\"\u001b[0m                                \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m While not directly a system, understanding model optimization techniques (quantization, pruning,  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mdistillation) is crucial for making models run efficiently on various systems.                               \n",
       "\u001b[1;33m • \u001b[0mquery 15: \u001b[1;36;40m\"benchmark results\" \"transformer inference frameworks\" \"latency throughput\"\u001b[0m                           \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query specifically looks for benchmark results of different inference frameworks, which can  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mprovide data-driven comparisons and insights into system performance.                                        \n",
       "\n",
       "\u001b[1mGoogle Scholar API:\u001b[0m                                                                                                \n",
       "\n",
       "\u001b[1;33m • \u001b[0mquery 1: \u001b[1;36;40m\"survey\" \"inference systems\" \"pretrained transformer models\"\u001b[0m                                           \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m  Searching for survey papers provides comprehensive overviews of existing systems and research    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mtrends in transformer inference.                                                                             \n",
       "\u001b[1;33m • \u001b[0mquery 2: \u001b[1;36;40m\"efficient inference\" \"transformer models\" \"system design\"\u001b[0m                                             \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query targets research papers focused on designing efficient inference systems for           \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mtransformer models, often discussing novel architectures and optimization techniques.                        \n",
       "\u001b[1;33m • \u001b[0mquery 3: \u001b[1;36;40m\"model serving framework\" \"performance evaluation\" \"transformer inference\"\u001b[0m                             \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query seeks papers that evaluate the performance of different model serving frameworks       \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mspecifically for transformer inference, providing comparative analysis.                                      \n",
       "\u001b[1;33m • \u001b[0mquery 4: \u001b[1;36;40m\"hardware acceleration\" \"transformer inference\" \"FPGA\" \"ASIC\" \"GPU\"\u001b[0m                                    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query focuses on academic research into hardware acceleration methods for transformer        \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0minference, including specialized hardware like FPGAs and ASICs, as well as GPUs.                             \n",
       "\u001b[1;33m • \u001b[0mquery 5: \u001b[1;36;40m\"low latency inference\" \"transformer models\" \"optimization techniques\"\u001b[0m                                 \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query targets papers exploring optimization techniques to achieve low latency inference for  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mtransformer models, a key performance metric.                                                                \n",
       "\u001b[1;33m • \u001b[0mquery 6: \u001b[1;36;40m\"edge computing\" \"transformer inference\" \"resource constraints\"\u001b[0m                                        \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query seeks academic work on running transformer inference in edge computing environments    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mwith limited resources.                                                                                      \n",
       "\u001b[1;33m • \u001b[0mquery 7: \u001b[1;36;40m\"quantization\" \"pruning\" \"distillation\" \"transformer inference\"\u001b[0m                                        \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query targets research papers on model optimization techniques like quantization, pruning,   \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mand distillation, which are vital for efficient inference.                                                   \n",
       "\u001b[1;33m • \u001b[0mquery 8: \u001b[1;36;40m\"system architecture\" \"transformer inference\" \"scalable\"\u001b[0m                                               \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query focuses on papers discussing the system architecture of scalable transformer inference \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0msystems, addressing aspects like distribution and concurrency.                                               \n",
       "\u001b[1;33m • \u001b[0mquery 9: \u001b[1;36;40m\"benchmark dataset\" \"transformer inference performance\"\u001b[0m                                                \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query seeks papers that introduce benchmark datasets or evaluate transformer inference       \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mperformance using standard datasets, providing a basis for comparison.                                       \n",
       "\u001b[1;33m • \u001b[0mquery 10: \u001b[1;36;40m\"comparative study\" \"inference frameworks\" \"transformer models\"\u001b[0m                                       \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query aims to find comparative studies that analyze and compare different inference          \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mframeworks for transformer models, offering insights into their strengths and weaknesses.                    \n",
       "\u001b[1;33m • \u001b[0mquery 11: \u001b[1;36;40m\"distributed inference\" \"transformer models\" \"parallelism\"\u001b[0m                                            \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query explores research on distributed inference techniques for transformer models, focusing \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mon parallelism to improve performance.                                                                       \n",
       "\u001b[1;33m • \u001b[0mquery 12: \u001b[1;36;40m\"real-time systems\" \"transformer inference\" \"latency analysis\"\u001b[0m                                        \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query targets academic work related to real-time systems for transformer inference, including\n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mlatency analysis and performance guarantees.                                                                 \n",
       "\u001b[1;33m • \u001b[0mquery 13: \u001b[1;36;40m\"adaptive inference\" \"transformer models\" \"resource management\"\u001b[0m                                       \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query seeks papers discussing adaptive inference systems that can dynamically adjust resource\n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mallocation based on demand and model characteristics.                                                        \n",
       "\u001b[1;33m • \u001b[0mquery 14: \u001b[1;36;40m\"transformer inference engine\" \"design and implementation\"\u001b[0m                                            \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query aims to find papers detailing the design and implementation of specific transformer    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0minference engines or systems.                                                                                \n",
       "\u001b[1;33m • \u001b[0mquery 15: \u001b[1;36;40m\"case study\" \"transformer inference deployment\" \"industry application\"\u001b[0m                                \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query seeks case studies of real-world deployments of transformer inference systems in       \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mvarious industry applications, providing practical examples.                                                 \n",
       "\n",
       "\u001b[1mGoogle News API:\u001b[0m                                                                                                   \n",
       "\n",
       "\u001b[1;33m • \u001b[0mquery 1: \u001b[1;36;40m\"new inference platform\" \"transformer models\" \"released\"\u001b[0m                                               \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query aims to find news articles announcing the release of new inference platforms           \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mspecifically designed for transformer models.                                                                \n",
       "\u001b[1;33m • \u001b[0mquery 2: \u001b[1;36;40m\"transformer inference service\" \"launched\" \"cloud provider\"\u001b[0m                                            \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query targets news about cloud providers launching new inference services for transformer    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mmodels.                                                                                                      \n",
       "\u001b[1;33m • \u001b[0mquery 3: \u001b[1;36;40m\"edge inference solution\" \"transformer models\" \"announced\"\u001b[0m                                             \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query seeks news articles announcing new edge inference solutions for transformer models.    \n",
       "\u001b[1;33m • \u001b[0mquery 4: \u001b[1;36;40m\"faster transformer inference\" \"technology breakthrough\"\u001b[0m                                               \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query looks for news about technological breakthroughs or advancements that lead to faster   \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mtransformer inference.                                                                                       \n",
       "\u001b[1;33m • \u001b[0mquery 5: \u001b[1;36;40m\"company announces\" \"transformer inference hardware\"\u001b[0m                                                   \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query targets news about companies announcing new hardware specifically designed for         \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mtransformer inference acceleration.                                                                          \n",
       "\u001b[1;33m • \u001b[0mquery 6: \u001b[1;36;40m\"open source inference library\" \"transformer models\" \"update\"\u001b[0m                                          \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query seeks news about updates or new releases of open-source inference libraries for        \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mtransformer models.                                                                                          \n",
       "\u001b[1;33m • \u001b[0mquery 7: \u001b[1;36;40m\"benchmark results\" \"transformer inference platforms\" \"comparison\"\u001b[0m                                     \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query aims to find news articles reporting on benchmark results and comparisons of different \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mtransformer inference platforms.                                                                             \n",
       "\u001b[1;33m • \u001b[0mquery 8: \u001b[1;36;40m\"AI inference chip\" \"transformer models\" \"market release\"\u001b[0m                                              \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query targets news about the market release of new AI inference chips that are suitable for  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mrunning transformer models.                                                                                  \n",
       "\u001b[1;33m • \u001b[0mquery 9: \u001b[1;36;40m\"industry trends\" \"transformer inference\" \"adoption\"\u001b[0m                                                   \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query seeks news articles discussing industry trends and the adoption of transformer         \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0minference technologies.                                                                                      \n",
       "\u001b[1;33m • \u001b[0mquery 10: \u001b[1;36;40m\"startup raises funding\" \"transformer inference technology\"\u001b[0m                                           \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query looks for news about startups in the transformer inference technology space that have  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mraised funding, indicating areas of active development and investment.                                       \n",
       "\u001b[1;33m • \u001b[0mquery 11: \u001b[1;36;40m\"partnership\" \"cloud company\" \"inference platform\" \"transformer models\"\u001b[0m                               \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query targets news about partnerships between cloud companies and inference platform         \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mproviders for transformer models.                                                                            \n",
       "\u001b[1;33m • \u001b[0mquery 12: \u001b[1;36;40m\"security vulnerability\" \"inference system\" \"transformer models\"\u001b[0m                                      \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m While less positive, news about security vulnerabilities in inference systems is also relevant to \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0munderstand the landscape and challenges.                                                                     \n",
       "\u001b[1;33m • \u001b[0mquery 13: \u001b[1;36;40m\"acquisition\" \"inference technology company\" \"transformer models\"\u001b[0m                                     \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query seeks news about acquisitions in the inference technology space related to transformer \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mmodels, indicating market consolidation and interest.                                                        \n",
       "\u001b[1;33m • \u001b[0mquery 14: \u001b[1;36;40m\"conference announcement\" \"transformer inference research\"\u001b[0m                                            \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query targets news from conferences or industry events announcing new research or            \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mdevelopments in transformer inference.                                                                       \n",
       "\u001b[1;33m • \u001b[0mquery 15: \u001b[1;36;40m\"product review\" \"transformer inference server\" \"performance tested\"\u001b[0m                                  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[3mRationale:\u001b[0m This query seeks product reviews of transformer inference servers or platforms, often including   \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mperformance testing and real-world usage reports.                                                            \n",
       "\n",
       "These queries are designed to be comprehensive and cover various aspects of systems for running pretrained         \n",
       "transformer models for inference, utilizing the strengths of each search tool. Remember to adjust time ranges      \n",
       "within each API based on your desired recency of information.                                                      \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reasoningModel = ReasoningModel()\n",
    "basicSearchModel = BasicSearchModel()\n",
    "\n",
    "\n",
    "user_query = \"We need a detailed and extensive overview of existing systems (e.g., ARM, FPGA, NPU, GPUs, etc.) for running pretrained model (e.g., transformer models) up to 100 million parameters (inference only).\"\n",
    "\n",
    "query = f\"\"\"\n",
    "{user_query}\n",
    "\n",
    "We can use different types of search tools:\n",
    "- Google Search API (general search with time range search)\n",
    "- Google Schoolar API (search papers)\n",
    "- Google News API (search news articles)\n",
    "\n",
    "Come up with suitable search queries for these tools.\n",
    "\n",
    "The format should following this structure:\n",
    "\n",
    "Google Search:\n",
    "- query 1\n",
    "- query 2\n",
    "- etc.\n",
    "\n",
    "Google Schoolar API:\n",
    "- etc.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if not (response := load_data(\"embedded_devices_search\")):\n",
    "    response = reasoningModel(query)\n",
    "    save_data(response, \"embedded_devices_search\")\n",
    "    cprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dplaia/Projekte/DeepResearch/agent_tools.py:39: LogfireNotConfiguredWarning: No logs or spans will be created until `logfire.configure()` has been called. Set the environment variable LOGFIRE_IGNORE_NO_CONFIG=1 or add ignore_no_config=true in pyproject.toml to suppress this warning.\n",
      "  return await self.agent.run(user_input)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SearchQueries(BaseModel):\n",
    "    google_search: Optional[list[str]] = Field(description=\"The search queries from Google Search.\")\n",
    "    scholar_search: Optional[list[str]] = Field(description=\"The search queries from Google Scholar.\")\n",
    "    news_search: Optional[list[str]] = Field(description=\"The search queries from Google News.\")\n",
    "\n",
    "query_extraction = BasicAgent(result_type=SearchQueries, system_prompt=\"Extract the search queries from a given text (only the queries, nothing else).\")\n",
    "\n",
    "\n",
    "result = await query_extraction(f\"{response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_search_queries = result.data.google_search\n",
    "scholar_search_queries = result.data.scholar_search\n",
    "news_search_queries = result.data.news_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for google_search_query in google_search_queries:\n",
    "    search_results = await google_general_search_async(google_search_query.replace(\"\\\"\", \"\"), time_span = TimeSpan.YEAR)\n",
    "\n",
    "    results.append(search_results)\n",
    "\n",
    "for result in results:\n",
    "    organic = result['organic']\n",
    "\n",
    "    urls = [t['link'] for t in organic]\n",
    "\n",
    "    for url in urls:\n",
    "        if not url in url_list:\n",
    "             url_list.append(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scholar Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for scholar_query in scholar_search_queries:\n",
    "    search_results = await google_general_search_async(scholar_query.replace(\"\\\"\", \"\"))\n",
    "\n",
    "    results.append(search_results)\n",
    "\n",
    "for result in results:\n",
    "    organic = result['organic']\n",
    "\n",
    "    urls = [t['link'] for t in organic]\n",
    "\n",
    "    for url in urls:\n",
    "        if not url in url_list:\n",
    "             url_list.append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/html/2404.02852v1\n",
      "https://www.vde.com/resource/blob/2361636/bc0c7b6d8464dc8e285618b35b11caa7/paper---large-language-models-data.pdf\n",
      "https://developer.nvidia.com/blog/demystifying-ai-inference-deployments-for-trillion-parameter-large-language-models/\n",
      "https://huggingface.co/google-t5/t5-small\n",
      "https://jbs.live/top-10-pre-trained-models-shaping-the-future-of-nlp/\n",
      "https://arxiv.org/html/2405.14159v1\n",
      "https://blog.openvino.ai/blog-posts/accelerate-inference-of-hugging-face-transformer-models-with-optimum-intel-and-openvino\n",
      "https://neurips.cc/virtual/2024/poster/96498\n",
      "https://en.wikipedia.org/wiki/Large_language_model\n",
      "https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models\n",
      "https://www.datacamp.com/blog/small-language-models\n",
      "https://medium.com/@williamwarley/unlocking-the-power-of-small-language-models-understanding-applications-and-advantages-9f65b7ba0c2c\n",
      "https://blog.futuresmart.ai/small-language-models-the-next-big-thing-in-ai\n",
      "https://www.analyticsvidhya.com/blog/2024/12/top-small-language-models/\n",
      "https://medium.com/@kombib/21-small-language-models-that-are-revolutionizing-ai-375e6e28122e\n",
      "https://arxiv.org/html/2411.03350v1\n",
      "https://www.splunk.com/en_us/blog/learn/small-language-models-slms.html\n",
      "https://www.leewayhertz.com/small-language-models/\n",
      "https://deepsense.ai/blog/implementing-small-language-models-slms-with-rag-on-embedded-devices-leading-to-cost-reduction-data-privacy-and-offline-use/\n",
      "https://arxiv.org/html/2501.05465v1\n",
      "https://www.inferless.com/learn/ctranslate2-or-tensorrt-llm-comparing-top-libraries-for-large-language-model-deployment\n",
      "https://www.reddit.com/r/LocalLLaMA/comments/1fwde4t/why_is_transformers_library_w_huggingface_so_slow/\n",
      "https://rocm.blogs.amd.com/artificial-intelligence/ctranslate2/README.html\n",
      "https://qed.usc.edu/paolieri/papers/2024_edgesys_mobile_inference_benchmark.pdf\n",
      "https://arxiv.org/html/2411.00136v1\n",
      "https://blog.spheron.network/comparing-community-support-for-open-source-llms-like-hugging-face-transformers-with-closed-platforms\n",
      "https://www.bentoml.com/blog/benchmarking-llm-inference-backends\n",
      "https://www.blog.aiport.tech/p/top-9-libraries-to-accelerate-llm\n",
      "https://www.inferless.com/learn/ctranslate2-vs-triton-inference-server-the-best-choice-for-efficient-llm-deployment\n",
      "https://www.qualcomm.com/developer/blog/2024/05/qualcomm-cloud-ai-introduces-efficient-transformers-one-api\n",
      "https://www.helicone.ai/blog/llm-api-providers\n",
      "https://stackoverflow.com/questions/78655250/how-to-deploy-a-hugging-face-transformers-model-for-inference-using-kserve-with\n",
      "https://huggingface.co/docs/google-cloud/en/examples/vertex-ai-notebooks-deploy-bert-on-vertex-ai\n",
      "https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-models-from-huggingface?view=azureml-api-2\n",
      "https://www.beam.cloud/blog/serverless-gpu\n",
      "https://www.labellerr.com/blog/comparing-top-10-model-serving-platforms-pros-and-co/\n",
      "https://docs.databricks.com/ja/archive/machine-learning/train-model/model-inference-nlp.html\n",
      "https://huggingface.co/blog/alvarobartt/deploy-from-hub-to-vertex-ai\n",
      "https://iqua.ece.toronto.edu/papers/chenghao-icdcs24.pdf\n",
      "https://arxiv.org/html/2403.02619v3\n",
      "https://newsroom.arm.com/blog/enabling-next-gen-edge-ai-applications-with-transformer-networks\n",
      "https://medium.com/accredian/edge-ai-deploying-large-language-models-for-smarter-devices-cdee25023673\n",
      "https://www.wevolver.com/article/challenges-and-opportunities-in-edge-based-generative-ai\n",
      "https://arxiv.org/html/2403.02619v2\n",
      "https://medium.com/@hassaanidrees7/edge-ai-bringing-intelligence-to-the-edge-7386cfdad88a\n",
      "https://www.mdpi.com/2076-3417/15/3/1097\n",
      "https://liacs.leidenuniv.nl/~stefanovtp/pdf/TCAD_24.pdf\n",
      "https://www.zyphra.com/post/edge-llms-benefits-challenges-and-solutions\n",
      "https://www.aiacceleratorinstitute.com/improving-ai-inference-performance-with-hardware-accelerators/\n",
      "https://astralord.github.io/posts/transformer-inference-optimization-toolset/\n",
      "https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/tutorials/Conceptual_Guide/Part_4-inference_acceleration/README.html\n",
      "https://www.ridgerun.com/post/ai-deep-learning-inference-acceleration\n",
      "https://sbert.net/docs/sentence_transformer/usage/efficiency.html\n",
      "https://discuss.huggingface.co/t/seeking-advice-on-optimizing-hardware-resources-for-model-training/100709\n",
      "https://arxiv.org/html/2410.04466v2\n",
      "https://www.reddit.com/r/learnmachinelearning/comments/1aubc4u/gpu_vs_cpu_for_inference/\n",
      "https://www.hyperstack.cloud/blog/case-study/optimising-ai-inference-for-performance-and-efficiency\n",
      "https://community.juniper.net/blogs/sharada-yeluri/2024/02/20/llm-inference-hw-sw-optimizations\n",
      "https://arxiv.org/abs/2409.05207\n",
      "https://blog.premai.io/transformer-inference-techniques-for-faster-ai-models/\n",
      "https://arxiv.org/html/2409.05207v1\n",
      "https://medium.com/@hassaanidrees7/real-time-machine-learning-harnessing-ai-for-instant-decision-making-ccbb71b76cd9\n",
      "https://developer.nvidia.com/blog/low-latency-inference-chapter-1-up-to-1-9x-higher-llama-3-1-performance-with-medusa-on-nvidia-hgx-h200-with-nvlink-switch/\n",
      "https://dominguezdaniel.medium.com/understanding-inference-time-compute-c6c6ca2e17a8\n",
      "https://huggingface.co/docs/transformers/main/en/llm_optims\n",
      "https://github.com/alibaba/MNN\n",
      "https://arxiv.org/abs/2403.08368\n",
      "https://medium.com/@Swarnika.yadav/exploring-mini-llms-lightweight-language-models-for-practical-use-7520ab765cbd\n",
      "https://www.sciencedirect.com/science/article/abs/pii/S0893608023006676\n",
      "https://viso.ai/computer-vision/best-lightweight-computer-vision-models/\n",
      "https://www.nature.com/articles/s41598-024-72682-8\n",
      "https://www.sciencedirect.com/science/article/abs/pii/S0262885624001574\n",
      "https://link.springer.com/article/10.1007/s10462-024-10877-1\n",
      "https://www.iterate.ai/ai-glossary/mobilenet-for-mobile-embedded-vision-applications\n",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints-autoscale.html\n",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-autoscale.html\n",
      "https://modal.com/blog/serverless-inference-article\n",
      "https://www.usenix.org/system/files/osdi24-fu.pdf\n",
      "https://www.inferless.com/blog/effortless-autoscaling-for-your-hugging-face-application\n",
      "https://huggingface.co/blog/alvarobartt/serve-any-model-inference-endpoints-custom\n",
      "https://wandb.ai/byyoung3/ML-NEWS2/reports/A-guide-to-using-the-Azure-AI-model-inference-API--Vmlldzo4OTY1MjEy\n",
      "https://www.reddit.com/r/MachineLearning/comments/1b43ndx/d_best_way_to_deploy_transformer_models/\n",
      "https://medium.com/@ketangangal98/deploy-asynchronous-inference-endpoint-of-transformer-model-on-amazon-sagemaker-4231880db642\n",
      "https://resources.nvidia.com/en-us-ai-inference-content/watch-110\n",
      "https://www.qualcomm.com/products/technology/processors/cloud-artificial-intelligence\n",
      "https://resources.nvidia.com/en-us-ai-inference-content/watch-111\n",
      "https://gautam75.medium.com/ten-ways-to-serve-large-language-models-a-comprehensive-guide-292250b02c11\n",
      "https://www.cisco.com/c/en/us/products/collateral/servers-unified-computing/ucs-x-series-modular-system/ai-ucs-m7-blades-5th-gen-xeon-cpu-wp.html\n",
      "https://cerebras.ai/\n",
      "https://sima.ai/mlsoc/\n",
      "https://www.databasemart.com/ai-solution?srsltid=AfmBOormtQ8JZM1zJDlhBrYVtrpjU58aHxPn7uhXBWD7E-98BFyp2zcJ\n",
      "https://www.hyperstack.cloud/blog/case-study/what-is-vllm-a-guide-to-quick-inference\n",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-best-practices.html\n",
      "https://rejolut.com/blog/best-practices-for-transformer-model-development/\n",
      "https://huggingface.co/learn/computer-vision-course/unit9/model_deployment\n",
      "https://medium.com/towards-data-science/a-complete-guide-to-write-your-own-transformers-29e23f371ddd\n",
      "http://fastbots.ai/blog/guide-to-choosing-the-best-transformer-model\n",
      "https://arxiv.org/html/2412.13490v1\n",
      "https://www.sciencedirect.com/science/article/pii/S0957417424004032\n",
      "https://www.nature.com/articles/s41598-024-72254-w\n",
      "https://www.reddit.com/r/MachineLearning/comments/1hhhcu7/d_are_lstms_faster_than_transformers_during/\n",
      "https://medium.com/@bijit211987/the-transformer-architecture-with-hybrid-models-eca885e12056\n",
      "https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2022WR032602\n",
      "https://arxiv.org/html/2403.18276v2\n",
      "https://www.sciencedirect.com/science/article/abs/pii/S0957417423031688\n",
      "https://www.datologyai.com/post/technical-deep-dive-curating-our-way-to-a-state-of-the-art-text-dataset\n",
      "https://openreview.net/forum?id=G7QS68ICPJ&referrer=%5Bthe%20profile%20of%20Yu%20Yu%5D(%2Fprofile%3Fid%3D~Yu_Yu1)\n",
      "https://huggingface.co/learn/cookbook/en/enterprise_dedicated_endpoints\n",
      "https://github.com/huggingface/huggingface-inference-toolkit\n",
      "https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/inference/1_inference_ver3_HF_transformers.html\n",
      "https://www.ais.com/transformer-based-ai-models-overview-inference-the-impact-on-knowledge-work/\n",
      "https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/tutorials/Quick_Deploy/HuggingFaceTransformers/README.html\n",
      "https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference\n",
      "https://huggingface.co/learn/ml-games-course/en/unit1/local-vs-api\n",
      "https://yaofu.notion.site/Towards-100x-Speedup-Full-Stack-Transformer-Inference-Optimization-43124c3688e14cffaf2f1d6cbdf26c6c\n",
      "https://www.linkedin.com/pulse/optimizing-inference-large-language-models-strategies-ravi-naarla-d6ppc\n",
      "https://medium.com/@yugank.aman/llm-model-optimization-techniques-and-frameworks-e21d57744ca1\n",
      "https://arxiv.org/html/2408.03130v1\n",
      "https://www.run.ai/guides/ai-open-source-projects/fastertransformer\n",
      "https://arxiv.org/html/2407.09111v1\n",
      "https://medium.com/@manindersingh120996/how-to-optimize-llm-inference-0076d9f3dd1b\n",
      "https://www.lamini.ai/blog/evaluate-performance-llm-inference-frameworks\n",
      "https://www.cs.cmu.edu/~csd-phd-blog/2024/low-latency-llm-serving/\n",
      "https://blog.spheron.network/best-practices-for-llm-inference-performance-monitoring\n",
      "https://medium.com/@martiniglesiasgo/anatomy-of-tgi-text-generation-inference-ii-6aace06c5efb\n",
      "https://www.analyticsvidhya.com/blog/2025/01/deep-learning-cpu-benchmarks/\n",
      "https://www.eeworldonline.com/what-are-the-different-mlperf-benchmarks-from-mlcommons/\n",
      "https://www.run.ai/blog/run-ai-model-streamer-performance-benchmarks\n",
      "https://arxiv.org/abs/2307.07982\n",
      "https://www.sciencedirect.com/science/article/abs/pii/S1383762123001698\n",
      "https://arxiv.org/html/2412.08145\n",
      "https://medium.com/data-science-at-microsoft/efficient-transformers-survey-of-recent-work-75022cddc86a\n",
      "https://ojs.aaai.org/index.php/AAAI/article/view/26255\n",
      "https://dl.acm.org/doi/10.1145/3530811\n",
      "https://www.researchgate.net/publication/353863371_AMMUS_A_Survey_of_Transformer-based_Pretrained_Models_in_Natural_Language_Processing\n",
      "https://journalofbigdata.springeropen.com/articles/10.1186/s40537-023-00842-0\n",
      "https://ieeexplore.ieee.org/document/9585316/\n",
      "https://www.youtube.com/watch?v=mzMiKydilZQ\n",
      "https://ieeexplore.ieee.org/document/10046087\n",
      "https://arxiv.org/abs/2207.00032\n",
      "https://www.reddit.com/r/MachineLearning/comments/vueqv8/r_deepspeed_inference_enabling_efficient/\n",
      "https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51088/\n",
      "https://proceedings.mlsys.org/paper_files/paper/2023/file/c4be71ab8d24cdfb45e3d06dbfca2780-Paper-mlsys2023.pdf\n",
      "https://arxiv.org/abs/2302.14017\n",
      "https://myrtle.ai/learn/leo-3-power-efficient-inference/\n",
      "https://tryolabs.com/blog/2022/11/24/transformer-based-model-for-faster-inference\n",
      "https://arxiv.org/html/2411.10337v1\n",
      "https://medium.com/better-programming/frameworks-for-serving-llms-60b7f7b23407\n",
      "https://aws.amazon.com/blogs/machine-learning/deploy-large-models-at-high-performance-using-fastertransformer-on-amazon-sagemaker/\n",
      "https://www.reddit.com/r/MachineLearning/comments/yg1mpz/d_how_to_get_the_fastest_pytorch_inference_and/\n",
      "https://haoran-qiu.com/pdf/atc24-preprint.pdf\n",
      "https://dachengli1.github.io/awards/amazon_funding_secure_serving.pdf\n",
      "https://opensearch.org/docs/2.4/ml-commons-plugin/model-serving-framework/\n",
      "https://blog.vespa.ai/ml-model-serving-at-scale/\n",
      "https://medium.com/@plthiyagu/comparing-llm-serving-frameworks-llmops-f02505864754\n",
      "https://arxiv.org/html/2409.03384v1\n",
      "https://dl.acm.org/doi/full/10.1145/3613963\n",
      "https://www.reddit.com/r/FPGA/comments/1hmmrpn/fpga_based_hardware_accelerator_for_transformers/\n",
      "https://arxiv.org/html/2401.09890v1\n",
      "https://dl.acm.org/doi/10.1016/j.sysarc.2024.103247\n",
      "https://www.mdpi.com/2076-3417/15/2/586\n",
      "https://wangshusen.github.io/papers/ISQED2021.pdf\n",
      "https://www.ibm.com/think/topics/fpga-vs-gpu\n",
      "https://getstream.io/blog/optimize-transformer-inference/\n",
      "https://lilianweng.github.io/posts/2023-01-10-inference-optimization/\n",
      "https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/\n",
      "https://arxiv.org/pdf/2307.07982\n",
      "https://arxiv.org/html/2410.11650v1\n",
      "https://ieeexplore.ieee.org/document/10181988/\n",
      "https://ieeexplore.ieee.org/document/10181174/\n",
      "https://semiengineering.com/generative-ai-transforming-inference-at-the-edge/\n",
      "https://www.sciencedirect.com/science/article/abs/pii/S1568494624001959\n",
      "https://dl.acm.org/doi/fullHtml/10.1145/3650200.3656628\n",
      "https://link.springer.com/article/10.1007/s00607-025-01427-w\n",
      "https://www.computer.org/csdl/journal/tm/2024/12/10591707/1YraFlDdKYo\n",
      "https://medium.com/openvino-toolkit/joint-pruning-quantization-and-distillation-for-efficient-inference-of-transformers-21333481f2ad\n",
      "https://blog.openvino.ai/blog-posts/joint-pruning-quantization-and-distillation-for-efficient-inference-of-transformers\n",
      "https://www.youtube.com/watch?v=UcwDgsMgTu4\n",
      "https://ai.stackexchange.com/questions/43054/when-to-use-pruning-quantization-distillation-and-others-when-optimizing-spee\n",
      "https://phontron.com/class/anlp2024/assets/slides/anlp-11-distillation.pdf\n",
      "https://medium.com/@VK_Venkatkumar/model-optimization-techniques-pruning-quantization-knowledge-distillation-sparsity-2d95aa34ea05\n",
      "https://discuss.huggingface.co/t/how-to-prune-transformer-based-model/32476\n",
      "https://www.youtube.com/watch?v=s9yyH3RPhdM\n",
      "https://www.linkedin.com/pulse/quantization-distillation-pruning-llm-tejas-bankar-k64vf\n",
      "https://jax-ml.github.io/scaling-book/inference/\n",
      "https://arxiv.org/pdf/2211.05102\n",
      "https://medium.com/@kamalmeet/transformer-architecture-db80b07735a9\n",
      "https://www.stat.berkeley.edu/~mmahoney/pubs/7_full_stack_optimization_of_tra.pdf\n",
      "https://www.reddit.com/r/MachineLearning/comments/18apkw6/d_which_architecture_could_substitute_the/\n",
      "https://europe.naverlabs.com/blog/a-scalable-transformer-architecture-for-summarizing-long-documents/\n",
      "https://www.reddit.com/r/MachineLearning/comments/vxqv3l/why_do_transformers_scale_so_well_d/\n",
      "https://medium.com/towards-data-science/what-does-the-transformer-architecture-tell-us-cd3a4fd6a59d\n",
      "https://huggingface.co/docs/transformers/en/benchmarks\n",
      "https://medium.com/huggingface/benchmarking-transformers-pytorch-and-tensorflow-e2917fb891c2\n",
      "https://ieeexplore.ieee.org/document/10595070/\n",
      "https://medium.com/towards-data-science/hugging-face-transformer-inference-under-1-millisecond-latency-e1be0057a51c\n",
      "https://github.com/mli/transformers-benchmarks\n",
      "https://arxiv.org/abs/2102.06621\n",
      "https://www.nvidia.com/en-us/data-center/resources/mlperf-benchmarks/\n",
      "https://www.baseten.co/blog/llm-transformer-inference-guide/\n",
      "https://dl.acm.org/doi/10.1007/978-3-031-15512-3_17\n",
      "https://arxiv.org/abs/2301.11847\n",
      "https://thesai.org/Downloads/Volume15No4/Paper_37-Comparative_Analysis_of_Transformer_Models.pdf\n",
      "https://www.scipublications.com/journal/index.php/jsmhes/article/view/697\n",
      "https://www.mdpi.com/2079-9292/13/24/4877\n",
      "https://arxiv.org/html/2308.09372v2\n",
      "https://www.researchgate.net/publication/381553531_REVOLUTIONISING_TRANSLATION_TECHNOLOGY_A_COMPARATIVE_STUDY_OF_VARIANT_TRANSFORMER_MODELS_-BERT_GPT_AND_T5\n",
      "https://www.techtarget.com/searchenterpriseai/tip/GAN-vs-transformer-models-Comparing-architectures-and-uses\n",
      "https://ceur-ws.org/Vol-3706/Paper8.pdf\n",
      "https://huggingface.co/docs/accelerate/en/usage_guides/distributed_inference\n",
      "https://huggingface.co/docs/diffusers/en/training/distributed_inference\n",
      "https://ieeexplore.ieee.org/document/10812976/\n",
      "http://brandonye.tech/publication/detransformer/detransformer_date24.pdf\n",
      "https://www.deepspeed.ai/tutorials/inference-tutorial/\n",
      "https://www.infracloud.io/blogs/inference-parallelism/\n",
      "https://arxiv.org/html/2408.07802v2\n",
      "https://ieeexplore.ieee.org/document/10546617/\n",
      "https://github.com/huggingface/accelerate/issues/2018\n",
      "https://openaccess.thecvf.com/content/CVPR2023/papers/Girase_Latency_Matters_Real-Time_Action_Forecasting_Transformer_CVPR_2023_paper.pdf\n",
      "https://ijgis.pubpub.org/pub/bgz6nix4\n",
      "https://github.com/cli99/llm-analysis\n",
      "https://research.vu.nl/files/258499994/phdthesis-vinodvufinal%204%20-%2065043c3f62dc9.pdf\n",
      "https://arxiv.org/html/2402.04359v1\n",
      "https://openreview.net/pdf?id=A942VRmfhQ\n",
      "https://arxiv.org/html/2409.14846v1\n",
      "https://pages.cs.wisc.edu/~zxu444/home/paper/LlavaAdaptive_Inf_CVPR.pdf\n",
      "https://aclanthology.org/2023.acl-long.829.pdf\n",
      "https://openreview.net/forum?id=A942VRmfhQ\n",
      "https://www.sciencedirect.com/science/article/abs/pii/S0925231224012955\n",
      "https://www.machinelearningmastery.com/inferencing-the-transformer-model/\n",
      "https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/index.html\n",
      "https://github.com/NVIDIA/TransformerEngine\n",
      "https://www.reddit.com/r/LocalLLaMA/comments/187rfax/gptfast_a_fast_and_hackable_implementation_of/\n",
      "https://transformerlab.ai/docs/concepts/inference-engines/\n",
      "https://dl.acm.org/doi/10.1007/978-3-031-22677-9_29\n",
      "https://datascience.stackexchange.com/questions/114511/inference-process-in-autoregressive-transformer-architecture\n",
      "https://github.com/xdit-project/xDiT\n",
      "https://www.thoughtspot.com/data-trends/ai/what-is-transformer-architecture-chatgpt\n",
      "https://machinelearning.apple.com/research/neural-engine-transformers\n",
      "https://www.linkedin.com/pulse/transformer-inference-techniques-faster-ai-models-premai-d2tkf\n",
      "https://cdrdv2-public.intel.com/792871/hugging-face-case-study-112023.pdf\n",
      "http://arxiv.org/pdf/2302.14017\n",
      "https://bergum.medium.com/serverless-transformer-nlp-inference-34346f6dbf65\n",
      "https://uwspace.uwaterloo.ca/handle/10012/19111\n"
     ]
    }
   ],
   "source": [
    "for url in url_list:\n",
    "    print(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
