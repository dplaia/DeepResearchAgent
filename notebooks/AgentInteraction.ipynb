{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_utils import *\n",
    "from agent_tools import *\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Log to a file with custom timestamp format\n",
    "logger.add(\"logs/chain_of_thougth_agent_system.log\", format=\"{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}\")\n",
    "model = GeminiModel(config.FLASH2_MODEL)\n",
    "\n",
    "logfire.configure(scrubbing=logfire.ScrubbingOptions(callback=scrubbing_callback))\n",
    "\n",
    "logfire.instrument_httpx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chat handler with your credentials and desired model.\n",
    "chat = ChatHandler()\n",
    "    \n",
    "response = chat.send_question(\"What is 2*2?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = {}\n",
    "folder_name = 'input_files/'\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "if not exists(folder_name):\n",
    "    makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "main_input_file = join(folder_name, \"Vorhabenbeschreibung_NeuroTrust.txt\")\n",
    "\n",
    "if not exists(main_input_file):\n",
    "    print(\"file does not exists\")\n",
    "    # Process each file in the input directory\n",
    "    for filename in listdir(folder_name):\n",
    "        filepath = join(folder_name, filename)\n",
    "        \n",
    "        if not os.path.isfile(filepath):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            md = MarkItDown()\n",
    "            result = md.convert(filepath)\n",
    "            filename = os.path.basename(filepath)\n",
    "            documents[filename] = result.text_content\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filepath}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    doc = \"\"\n",
    "    for filename in documents:\n",
    "        print(f\"Filename: {filename}\")\n",
    "        doc = documents[filename]\n",
    "        count = word_count(doc)\n",
    "        print(f\"Number of Words in the document: {count}\")\n",
    "\n",
    "        break\n",
    "\n",
    "    with open(main_input_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(doc)\n",
    "\n",
    "else:\n",
    "    with open(main_input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"Let's assume you have access to the Google Search API and the Google Scholar API. \n",
    "\n",
    "Based on the folling document, which search queries would you do, to improve the quality of the document?\n",
    "\n",
    "# Document: \n",
    "\n",
    "{doc}\"\"\"\n",
    "\n",
    "response = chat.send_question(query)\n",
    "console_print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Your goal is to extract search queries (e.g., google search, google scholar, etc.) that are mention in a text.\"\n",
    "user_input = f\"Please extract all search queries from this text: {response}\"\n",
    "\n",
    "baseAgent = BaseAgent(AgentResponse, system_prompt)\n",
    "result = await baseAgent(user_input)\n",
    "\n",
    "print(result.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AgentResponse(BaseModel):\n",
    "    google_search_queries: list[str] = Field(description=\"The extracted google search queries.\")\n",
    "    google_scholar_queries: list[str] = Field(description=\"The extracted google scholar queries.\")\n",
    "\n",
    "system_prompt = \"Your goal is to extract search queries (e.g., google search, google scholar, etc.) that are mention in a text.\"\n",
    "user_input = f\"Please extract all search queries from this text: {response_thinking.final_answer}\"\n",
    "\n",
    "agent = Agent(\n",
    "    model,\n",
    "    result_type=AgentResponse,\n",
    "    system_prompt=system_prompt)\n",
    "\n",
    "result = await agent.run(user_input)\n",
    "data = result.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Agent that reads the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentStore:\n",
    "    \"\"\"Thread-safe FIFO string storage with non-blocking retrieval\n",
    "    \n",
    "    Usage:\n",
    "        store = DocumentStore()\n",
    "        store.put('data')\n",
    "        item = store.get()  # returns None if empty\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._items = deque()\n",
    "        self._lock = threading.Lock()\n",
    "\n",
    "    def put(self, item: str):\n",
    "        \"\"\"Add string to storage\"\"\"\n",
    "        with self._lock:\n",
    "            self._items.append(item)\n",
    "\n",
    "    def get(self) -> str | None:\n",
    "        \"\"\"Retrieve and remove oldest string, returns None if empty\"\"\"\n",
    "        with self._lock:\n",
    "            return self._items.popleft() if self._items else None\n",
    "\n",
    "    def clear(self):\n",
    "        self._items.clear()\n",
    "\n",
    "    @property\n",
    "    def count(self) -> int:\n",
    "        return len(self._items)\n",
    "        \n",
    "    def __copy__(self):\n",
    "        \"\"\"Create a shallow copy of the DocumentStore instance.\"\"\"\n",
    "        new_store = DocumentStore()\n",
    "        with self._lock:\n",
    "            new_store._items = deque(self._items)\n",
    "        return new_store\n",
    "\n",
    "store = DocumentStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = read_system_prompt('chain_of_agents')\n",
    "\n",
    "class AgentResponse(BaseModel):\n",
    "    main_findings: str = Field(..., description=\"Markdown summary with inline references (e.g., [Source 1]).\")\n",
    "    agent_instruction: str = Field(..., description=\"Specific, actionable steps for the next agent (e.g., 'Analyze X in Document 5').\")\n",
    "    links: list[str] = Field(..., description=\"High-priority links for further research.\")\n",
    "    search_queries_used: list[str] = Field(..., description=\"Queries executed in this step to avoid repetition.\")\n",
    "    processed_docs: list[str] = Field(..., description=\"IDs of documents analyzed (e.g., doc_3, doc_7).\")\n",
    "    confidence_score: float = Field(1.0, description=\"0-1 score indicating confidence in findings (1=high).\")\n",
    "    suggestions_for_improvements: str = Field(\"\", description=\"Feedback on system/prompt issues.\")\n",
    "\n",
    "agent = Agent(\n",
    "    model,\n",
    "    deps_type=store,\n",
    "    result_type=AgentResponse,\n",
    "    system_prompt=system_prompt)\n",
    "\n",
    "# @agent.system_prompt\n",
    "# def add_document_info() -> str:  \n",
    "#     return f'Number of stored documents {store.count}.'\n",
    "\n",
    "# Create a RateLimiter instance allowing 5 requests per 10 seconds\n",
    "#rate_limiter = RateLimiter(rpm=10, window=60.0)\n",
    "\n",
    "async_call_limiter = AsyncFunctionCallLimiter(num=1)  # Restrict the number of function calls\n",
    "\n",
    "@agent.tool_plain\n",
    "@async_call_limiter\n",
    "async def google_search(search_query: str, \n",
    "                        time_span: Optional[TimeSpan] = None, \n",
    "                        web_domain: Optional[str] = None) -> Optional[dict] | str:\n",
    "    \"\"\"\n",
    "    Perform a Google search using the Serper API.\n",
    "    \n",
    "    Args:\n",
    "        search_query (str): The search query string.\n",
    "        time_span (Optional[TimeSpan], optional): The time span. Defaults to None.\n",
    "            - Allowed:\n",
    "                - \"qdr:h\" (for hour)\n",
    "                - \"qdr:d\" (for day)\n",
    "                - \"qdr:w\" (for week)\n",
    "                - \"qdr:m\" (for month)\n",
    "                - \"qdr:y\" (for year)\n",
    "        web_domain (Optional[str], optional): Search inside a web domain (e.g., web_domain=\"brainchip.com\" -> searches only pages with this domain)\n",
    "    Returns:\n",
    "        Optional[dict]: The search results.\n",
    "    \"\"\"\n",
    "    print(f\"google_search with arguments = ({search_query}, {time_span}, {web_domain})\")\n",
    "    response = await google_general_search_async(search_query, time_span, web_domain)\n",
    "\n",
    "    return response\n",
    "\n",
    "@agent.tool_plain\n",
    "@async_call_limiter\n",
    "async def scholar_search(search_query: str, num_pages: int = 1) -> dict  | str:\n",
    "    \"\"\"Google scholar search using an API.\n",
    "\n",
    "        Args:\n",
    "            search_query (str): The search query string.\n",
    "            num_pages (int): The amount of page results that should be returned (more pages=more results).\n",
    "        Returns:\n",
    "            dict: The search results.\n",
    "        \n",
    "    \"\"\"\n",
    "    print(f\"scholar_search with arguments = ({search_query}, {num_pages})\")\n",
    "    response = await google_scholar_search_async(search_query, num_pages)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = f\"\"\"\n",
    "The following document (in German) is a preview of a research proposal that we want to submit. \n",
    "\n",
    "Please help us to improve the content of the document in the following ways:\n",
    "- Do research about the topics mentioned in the document.\n",
    "- Output things that might be relavant and valueble to the report.\n",
    "- Include information that might add value to the report.\n",
    "- Analyse new released papers and see if certain things might have changed.\n",
    "\n",
    "# Research proposal document:\n",
    "\n",
    "{doc}\n",
    "\"\"\"\n",
    "\n",
    "pickle_file = \"chain_of_agent_output.pkl\"\n",
    "os.system(f'rm {pickle_file}')\n",
    "\n",
    "result = await agent.run(user_input, deps=store)\n",
    "data = result.data\n",
    "print(f\"Number of Links: {len(data.links)}\")\n",
    "with open(pickle_file, \"wb\") as f:\n",
    "    pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pickle file (instead of running model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exists(pickle_file):\n",
    "    with open(pickle_file, \"rb\") as f:\n",
    "        result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console_print(result.data.main_findings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_folder = \"temp/\"\n",
    "\n",
    "if not exists(temp_folder):\n",
    "    makedirs(temp_folder)\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "files = {}\n",
    "\n",
    "for k, link in enumerate(data.links):\n",
    "    filename = join(temp_folder, f\"page_{k}.html\")\n",
    "    try:\n",
    "        makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        response = requests.get(link, headers=headers, timeout=10)\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(response.text)\n",
    "\n",
    "        print(f\"Saved: {filename}\")\n",
    "        files[basename(filename)] = link\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout occurred for {link}.\")\n",
    "        continue\n",
    "\n",
    "    except IOError as e:\n",
    "        print(f\"Failed to save {filename}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = MarkItDown()\n",
    "min_word_threshold = 1000\n",
    "max_word_threshold = 250000\n",
    "\n",
    "for filename in files:\n",
    "    link = files[filename]\n",
    "    filepath = join(temp_folder, filename)\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        file_content = f.read()\n",
    "        count = word_count(file_content)\n",
    "\n",
    "        if count < max_word_threshold:\n",
    "            if count < min_word_threshold:\n",
    "                print(f\"Word count: {count} -> Use crawl4ai\")\n",
    "                markdown_output = await crawl4ai_website_async(link)\n",
    "            else:\n",
    "                markdown_output = md.convert(filepath).text_content\n",
    "\n",
    "            store.put(markdown_output)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(store.count)\n",
    "store2 = copy.copy(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(store2.count)\n",
    "console_print(store2.get())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
