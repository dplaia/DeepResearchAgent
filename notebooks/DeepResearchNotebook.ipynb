{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logfire project URL: \u001b]8;id=438078;https://logfire.pydantic.dev/dplaia/deepresearchagent\u001b\\https://logfire.pydantic.dev/dplaia/deepresearchagent\u001b]8;;\u001b\\\n"
     ]
    }
   ],
   "source": [
    "from agent_utils import *\n",
    "from agent_tools import *\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Log to a file with custom timestamp format\n",
    "logger.add(\"logs/chain_of_thougth_agent_system.log\", format=\"{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}\")\n",
    "model = GeminiModel(config.FLASH2_MODEL)\n",
    "\n",
    "logfire.configure(scrubbing=logfire.ScrubbingOptions(callback=scrubbing_callback))\n",
    "logfire.instrument_httpx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chat handler with your credentials and desired model.\n",
    "reasoningAgentChat = ChatHandler()\n",
    "basicSearchAgent = BasicSearchAgent()\n",
    "\n",
    "class SearchQueryAgentResponse(BaseModel):\n",
    "    google_search_queries: list[str] | None = Field(description=\"The extracted google search queries (if available).\")\n",
    "    google_scholar_queries: list[str] | None = Field(description=\"The extracted google scholar queries (if available).\")\n",
    "    text_summary: str  | None = Field(description=\"Extract the text summary here (if available).\")\n",
    "\n",
    "searchQueryAgent = BaseAgent(SearchQueryAgentResponse, \n",
    "        system_prompt=\"\"\"Your goal is to extract search queries (e.g., google search, google scholar, etc.) \n",
    "        that are mention in a text.\"\"\")\n",
    "\n",
    "class URLRating(BaseModel):\n",
    "    url_number: int = Field(description=\"The URL number\", ge=0)\n",
    "    rating: int = Field(description=\"The rating (value between 0 and 100) for the URL\", ge=0, le=100)\n",
    "\n",
    "class URLRatingAgentResponse(BaseModel):\n",
    "    url_info: list[URLRating] = Field(description=\"A list with URLs with corresponding ratings.\")\n",
    "\n",
    "urlRatingAgent = BaseAgent(URLRatingAgentResponse, \n",
    "        system_prompt=\"\"\"Your goal is to extract URL number and the corresponding rating. \n",
    "        The input text has the following format: {1, 60}, {2,75}, {3, 35} .... \n",
    "        with  \n",
    "        {URL number, Rating}\"\"\")\n",
    "\n",
    "def get_document():\n",
    "    documents = {}\n",
    "    folder_name = 'input_files/'\n",
    "\n",
    "    # Create directory if it doesn't exist\n",
    "    if not exists(folder_name):\n",
    "        makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    main_input_file = join(folder_name, \"Vorhabenbeschreibung_NeuroTrust.txt\")\n",
    "\n",
    "    if not exists(main_input_file):\n",
    "        print(\"file does not exists\")\n",
    "        # Process each file in the input directory\n",
    "        for filename in listdir(folder_name):\n",
    "            filepath = join(folder_name, filename)\n",
    "            \n",
    "            if not os.path.isfile(filepath):\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                md = MarkItDown()\n",
    "                result = md.convert(filepath)\n",
    "                filename = os.path.basename(filepath)\n",
    "                documents[filename] = result.text_content\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filepath}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        doc = \"\"\n",
    "        for filename in documents:\n",
    "            print(f\"Filename: {filename}\")\n",
    "            doc = documents[filename]\n",
    "            count = word_count(doc)\n",
    "            print(f\"Number of Words in the document: {count}\")\n",
    "\n",
    "            break\n",
    "\n",
    "        with open(main_input_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(doc)\n",
    "\n",
    "    else:\n",
    "        with open(main_input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            doc = f.read()\n",
    "\n",
    "    return doc\n",
    "\n",
    "async def get_search_queries_for_document(doc: str):\n",
    "    query = get_system_prompt(\"search_query_recommendation\")\n",
    "    query += f\"\"\"\n",
    "    # Input Document: \n",
    "\n",
    "    {doc}\n",
    "    \"\"\"\n",
    "    \n",
    "    text_response = await reasoningAgentChat(query)\n",
    "    queries = await searchQueryAgent(f\"Please extract all search queries from this text: {text_response}\")\n",
    "\n",
    "    return queries.data\n",
    "\n",
    "async def get_search_query_help(query: str):\n",
    "\n",
    "    query += f\"\"\"\n",
    "    We have to improve the search results given a user search query. \n",
    "    Please think of multiple google search queries (plain text) that would increase the quality of the search results, when we combine all the search results.\n",
    "\n",
    "    # Desired output format:\n",
    "\n",
    "    Google Search Queries:\n",
    "    - Search query 1\n",
    "    - Search query 2\n",
    "    - etc.\n",
    "\n",
    "    # User Search Query:\n",
    "    {query}\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    text_response = await reasoningAgentChat(query)\n",
    "    queries = await searchQueryAgent(f\"Please extract all search queries from this text: {text_response}\")\n",
    "\n",
    "    return queries.data\n",
    "\n",
    "async def rate_search_results(content_text: str):\n",
    "    query = f\"\"\"\n",
    "    Please rate each search result based on relevance (value between 0 and 100).\n",
    "    Each search result has an URL with an URL number, \n",
    "    for example: \"Link [143]: https...\", where 143 is the URL number.\n",
    "\n",
    "    Your generated output format should look like this:\n",
    "\n",
    "    (1, 60), (2,75), (3, 35) ....\n",
    "    \n",
    "    with \n",
    "    \n",
    "    (URL number, Rating)  \n",
    "    \n",
    "    Here are the search results based on your search query suggestions:\n",
    "    \n",
    "    {content_text}\n",
    "    \"\"\"\n",
    "    text_response = await reasoningAgent(query)\n",
    "\n",
    "    url_info = await urlRatingAgent(f\"Please extract the url info in this text: {text_response}\")\n",
    "\n",
    "    return url_info.data\n",
    "\n",
    "async def search_queries(search_queries: list[str]) -> list[str]:\n",
    "    results = []\n",
    "    for query in search_queries:\n",
    "        print(f\"Search query: {query}\")\n",
    "        response = await basicSearchAgent(query)\n",
    "        results.append(response)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use generic Google/Perplexity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:AFC is enabled with max remote calls: 10.\n",
      "/home/dplaia/Projekte/deepresearchagent/.venv/lib/python3.13/site-packages/google/genai/types.py:2769: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  for field_name, field_value in part.dict(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:20:05.410 agent run prompt=Please extract all search queries from this text: Google Searc... inference toolchain\n",
      "- FPGA acceleration transformer inference\n",
      "11:20:05.411   preparing model and tools run_step=1\n",
      "11:20:05.413   model request\n",
      "11:20:05.414     POST /v1beta/models/gemini-2.0-flash:generateContent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:20:06.565   handle model response\n"
     ]
    }
   ],
   "source": [
    "user_search_query = \"Find FPGA platforms that can run pretrained transformer models up to 100 million parameters (inference only).\"\n",
    "queries = await get_search_query_help(user_search_query)\n",
    "\n",
    "results = await search_queries(queries.google_search_queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_text = \"\"\n",
    "\n",
    "for k, result in enumerate(results):\n",
    "    #console_print(result)\n",
    "    result_text += f\"\"\"\n",
    "    # Result query {k+1}:\n",
    "\n",
    "    {result}\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "#console_print(result_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:AFC is enabled with max remote calls: 10.\n",
      "/home/dplaia/Projekte/deepresearchagent/.venv/lib/python3.13/site-packages/google/genai/types.py:2769: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  for field_name, field_value in part.dict(\n"
     ]
    }
   ],
   "source": [
    "response = await reasoningAgentChat(f\"\"\"\n",
    "Please generate a high quality report text based on the search results (output in Markdown format).\n",
    "Add a reference section with citations at the end.\n",
    "Format:\n",
    "# References\n",
    "[1] http...\n",
    "[2] etc.\n",
    "\n",
    "Add the correct citations in the writen report. \n",
    "\n",
    "The text should be as relevant to the user search query as possible. \n",
    "\n",
    "# User Search Query:\n",
    "{user_search_query} \n",
    "\n",
    "#Search Results (text):\n",
    "{result_text}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research PUFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_document()\n",
    "if not (queries := load_data(\"search_query_suggestions\")):\n",
    "    queries = await get_search_queries_for_document(doc)\n",
    "    save_data(queries, \"search_query_suggestions\")\n",
    "\n",
    "google_search_queries = queries.google_search_queries\n",
    "google_scholar_queries = queries.google_scholar_queries\n",
    "document_summary = queries.text_summary\n",
    "console_print(document_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (search_results := load_data(\"google_search_results\")):\n",
    "    search_results = {}\n",
    "\n",
    "    for search_query in google_search_queries:\n",
    "        print(search_query)\n",
    "        results = await google_general_search_async(search_query)\n",
    "        search_results[search_query] = results\n",
    "    \n",
    "    save_data(search_results, \"google_search_results\")\n",
    "\n",
    "if not (scholar_results := load_data(\"google_scholar_results\"))\n",
    "    scholar_results = {}\n",
    "\n",
    "    for search_query in google_scholar_queries:\n",
    "        print(search_query)\n",
    "        results = await google_scholar_search_async(search_query)\n",
    "        scholar_results[search_query] = results\n",
    "    \n",
    "    save_data(scholar_results, \"google_scholar_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "content.append(\"Google Search Results:\")\n",
    "link_counter = 1\n",
    "\n",
    "links = {}\n",
    "\n",
    "for query in search_results:\n",
    "    result = search_results[query]\n",
    "\n",
    "    content.append(f\"\\nFor search query: '{result['searchParameters']['q']}'.\")\n",
    "\n",
    "    if 'answerBox' in result:\n",
    "        content.append(f\"\\nAnswerBox Text: {result['answerBox']['snippet']}\")\n",
    "    content.append(\"\\nOrganic results:\")\n",
    "\n",
    "    for organic in result['organic']:\n",
    "        #rprint(organic)\n",
    "        content.append(f\"Title: {organic['title']}\")\n",
    "        content.append(f\"Link [{link_counter}]: {organic['link']}\")\n",
    "        links[link_counter] = organic['link']\n",
    "        content.append(f\"Snippet: {organic['snippet']}\")\n",
    "\n",
    "        if 'date' in organic:\n",
    "            content.append(f\"Date: {organic['date']}\")\n",
    "\n",
    "        if 'attributes' in organic:\n",
    "            content.append(f\"Attributes: {organic['attributes']}\")\n",
    "        \n",
    "        content.append(\"\\n\")\n",
    "        link_counter+=1\n",
    "\n",
    "\n",
    "content_text = \"\\n\".join(content)\n",
    "\n",
    "content2 = []\n",
    "content2.append(\"\\nGoogle Scholar Results:\")\n",
    "\n",
    "for query in scholar_results:\n",
    "    result = scholar_results[query] # list\n",
    "    content2.append(f\"\\nFor search query: '{query}'.\\n\")\n",
    "    for entry in result:\n",
    "\n",
    "        content2.append(f\"Title: {entry['title']}\")\n",
    "        content2.append(f\"Link [{link_counter}]: {entry['link']}\")\n",
    "        links[link_counter] = entry['link']\n",
    "        content2.append(f\"Snippet: {entry['snippet']}\")\n",
    "        if 'date' in entry:\n",
    "            content2.append(f\"Date: {entry['date']}\")\n",
    "        if 'attributes' in entry:\n",
    "            content2.append(f\"Attributes: {entry['attributes']}\")\n",
    "        #rprint(entry)\n",
    "        content2.append(\"\\n\")\n",
    "        link_counter+=1\n",
    "\n",
    "content_text = \"\\n\".join(content2)\n",
    "#rprint(content_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
