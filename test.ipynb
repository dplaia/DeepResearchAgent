{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import controlflow as cf\n",
    "\n",
    "from agent_utils import *\n",
    "from agent_tools import *\n",
    "\n",
    "from extensive_search import run_research\n",
    "from rich import print as pprint\n",
    "FLASH2_MODEL = \"google/\" + config.FLASH2_MODEL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "We need an detailed and extensive overview of existing systems for running pretrained model (e.g., transformer models) up to 100 million parameters (inference only).\n",
    "To accomplish this, we plan to do a deep research by processing many webpages, PDFs, acticles etc. using LLM APIs (e.g., Google Gemini, GPT4o etc.).\n",
    "\n",
    "We use an existing Python agent framework (e.g., PydanticAI, LangChain, Controlflow) to build the system.\n",
    "\n",
    "We have access to different types of search tools:\n",
    "- Google Search API (general search with time range search)\n",
    "- Google Schoolar API (search papers)\n",
    "- Google News API (search news articles)\n",
    "- Could be expanded in the future.\n",
    "\n",
    "For this task, we plan to build our own deep research agent system that uses LLMs to process a workflow.\n",
    "\n",
    "The workflow has these basic working steps or tasks:\n",
    "- Based on a search query (user input) generate a research plan/instruction and suitable search queries for the search tools.\n",
    "- Use search APIs for generated search queries to collect information.\n",
    "- Use a reasoning model (LLM + thinking) to select suitable URLs to extract information from.\n",
    "- Use web crawlers to comvert HTML or PDF content to markdown text for the LLMs.\n",
    "- For the first markdown document use an LLM to rate the relevance and quality of the document.\n",
    "- For the first document also extract a summary of all relevant and important information.\n",
    "- Save the document with rating and contectualized summary.\n",
    "- For the second markdown document use the contectualized summary from the previous document, to genrate a rating and a new contextualized summary.\n",
    "- Repeat the earlier steps for each downloaded document.\n",
    "- Stop when a cerain document budget is reached.\n",
    "- Use the original user query and the compressed/accumulated summary to create a report based on the generated research plan/instruction.\n",
    "\n",
    "What do you think about this approach? How would you improve it?\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoningModel = ReasoningModel()\n",
    "basicSearchModel = BasicSearchModel()\n",
    "\n",
    "\n",
    "user_query = \"We need a detailed and extensive overview of existing systems (e.g., ARM, FPGA, NPU, GPUs, etc.) for running pretrained model (e.g., transformer models) up to 100 million parameters (inference only).\"\n",
    "\n",
    "query = f\"\"\"\n",
    "{user_query}\n",
    "\n",
    "We can use different types of search tools:\n",
    "- Google Search API (general search with time range search)\n",
    "- Google Schoolar API (search papers)\n",
    "- Google News API (search news articles)\n",
    "\n",
    "Come up with suitable search queries for these tools.\n",
    "\n",
    "The format should following this structure:\n",
    "\n",
    "Google Search:\n",
    "- query 1\n",
    "- query 2\n",
    "- etc.\n",
    "\n",
    "Google Schoolar API:\n",
    "- etc.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if not (response := load_data(\"embedded_devices_search\")):\n",
    "    response = reasoningModel(query)\n",
    "    save_data(response, \"embedded_devices_search\")\n",
    "    cprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dplaia/Projekte/DeepResearch/agent_tools.py:39: LogfireNotConfiguredWarning: No logs or spans will be created until `logfire.configure()` has been called. Set the environment variable LOGFIRE_IGNORE_NO_CONFIG=1 or add ignore_no_config=true in pyproject.toml to suppress this warning.\n",
      "  return await self.agent.run(user_input)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SearchQueries(BaseModel):\n",
    "    google_search: Optional[list[str]] = Field(description=\"The search queries from Google Search.\")\n",
    "    scholar_search: Optional[list[str]] = Field(description=\"The search queries from Google Scholar.\")\n",
    "    news_search: Optional[list[str]] = Field(description=\"The search queries from Google News.\")\n",
    "\n",
    "query_extraction = BasicAgent(result_type=SearchQueries, system_prompt=\"Extract the search queries from a given text (only the queries, nothing else).\")\n",
    "\n",
    "\n",
    "result = await query_extraction(f\"{response}\")\n",
    "\n",
    "google_search_queries = result.data.google_search\n",
    "scholar_search_queries = result.data.scholar_search\n",
    "news_search_queries = result.data.news_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not (results := load_data(\"google_search_results\")):\n",
    "    results = []\n",
    "\n",
    "    for google_search_query in google_search_queries:\n",
    "        search_results = await google_general_search_async(google_search_query.replace(\"\\\"\", \"\"), time_span = TimeSpan.YEAR)\n",
    "\n",
    "        results.append(search_results)\n",
    "\n",
    "    for result in results:\n",
    "        organic = result['organic']\n",
    "\n",
    "        urls = [t['link'] for t in organic]\n",
    "\n",
    "        for url in urls:\n",
    "            if not url in url_list:\n",
    "                url_list.append(url)\n",
    "\n",
    "    save_data(results, \"google_search_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scholar Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (results := load_data(\"scholar_search_results\")):\n",
    "    results = []\n",
    "\n",
    "    for scholar_query in scholar_search_queries:\n",
    "        search_results = await google_general_search_async(scholar_query.replace(\"\\\"\", \"\"))\n",
    "\n",
    "        results.append(search_results)\n",
    "\n",
    "    for result in results:\n",
    "        organic = result['organic']\n",
    "\n",
    "        urls = [t['link'] for t in organic]\n",
    "\n",
    "        for url in urls:\n",
    "            if not url in url_list:\n",
    "                url_list.append(url)\n",
    "    save_data(results, \"scholar_search_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (markdown_pages := load_data(\"markdown_pages\")):\n",
    "\n",
    "    markdown_pages = {}\n",
    "\n",
    "    for url in url_list:\n",
    "        print(url)\n",
    "\n",
    "        try:\n",
    "            markdown_text = await crawl4ai_website_async(url)\n",
    "            markdown_pages[url] = markdown_text\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    save_data(markdown_pages, \"markdown_pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory needed: 24.91 MB\n"
     ]
    }
   ],
   "source": [
    "memory = 0\n",
    "for page in markdown_pages.values():\n",
    "    memory += sys.getsizeof(page)\n",
    "\n",
    "print(f\"Memory needed: {memory/1024/1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
